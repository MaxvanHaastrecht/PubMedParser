abstract,keywords
"OBJECTIVE: Natural language processing (NLP) of symptoms from electronic health records (EHRs) could contribute to the advancement of symptom science. We aim to synthesize the literature on the use of NLP to process or analyze symptom information documented in EHR free-text narratives. MATERIALS AND METHODS: Our search of 1964 records from PubMed and EMBASE was narrowed to 27 eligible articles. Data related to the purpose, free-text corpus, patients, symptoms, NLP methodology, evaluation metrics, and quality indicators were extracted for each study. RESULTS: Symptom-related information was presented as a primary outcome in 14 studies. EHR narratives represented various inpatient and outpatient clinical specialties, with general, cardiology, and mental health occurring most frequently. Studies encompassed a wide variety of symptoms, including shortness of breath, pain, nausea, dizziness, disturbed sleep, constipation, and depressed mood. NLP approaches included previously developed NLP tools, classification methods, and manually curated rule-based processing. Only one-third (n = 9) of studies reported patient demographic characteristics. DISCUSSION: NLP is used to extract information from EHR free-text narratives written by a variety of healthcare providers on an expansive range of symptoms across diverse clinical specialties. The current focus of this field is on the development of methods to extract symptom information and the use of symptom information for disease classification tasks rather than the examination of symptoms themselves. CONCLUSION: Future NLP studies should concentrate on the investigation of symptoms and symptom documentation in EHR free-text narratives. Efforts should be undertaken to examine patient characteristics and make symptom-related NLP algorithms or pipelines and vocabularies openly available.",electronic health records | natural language processing | review | signs and symptoms
"BACKGROUND: Novel approaches that complement and go beyond evidence-based medicine are required in the domain of chronic diseases, given the growing incidence of such conditions on the worldwide population. A promising avenue is the secondary use of electronic health records (EHRs), where patient data are analyzed to conduct clinical and translational research. Methods based on machine learning to process EHRs are resulting in improved understanding of patient clinical trajectories and chronic disease risk prediction, creating a unique opportunity to derive previously unknown clinical insights. However, a wealth of clinical histories remains locked behind clinical narratives in free-form text. Consequently, unlocking the full potential of EHR data is contingent on the development of natural language processing (NLP) methods to automatically transform clinical text into structured clinical data that can guide clinical decisions and potentially delay or prevent disease onset. OBJECTIVE: The goal of the research was to provide a comprehensive overview of the development and uptake of NLP methods applied to free-text clinical notes related to chronic diseases, including the investigation of challenges faced by NLP methodologies in understanding clinical narratives. METHODS: Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were followed and searches were conducted in 5 databases using ""clinical notes,"" ""natural language processing,"" and ""chronic disease"" and their variations as keywords to maximize coverage of the articles. RESULTS: Of the 2652 articles considered, 106 met the inclusion criteria. Review of the included papers resulted in identification of 43 chronic diseases, which were then further classified into 10 disease categories using the International Classification of Diseases, 10th Revision. The majority of studies focused on diseases of the circulatory system (n=38) while endocrine and metabolic diseases were fewest (n=14). This was due to the structure of clinical records related to metabolic diseases, which typically contain much more structured data, compared with medical records for diseases of the circulatory system, which focus more on unstructured data and consequently have seen a stronger focus of NLP. The review has shown that there is a significant increase in the use of machine learning methods compared to rule-based approaches; however, deep learning methods remain emergent (n=3). Consequently, the majority of works focus on classification of disease phenotype with only a handful of papers addressing extraction of comorbidities from the free text or integration of clinical notes with structured data. There is a notable use of relatively simple methods, such as shallow classifiers (or combination with rule-based methods), due to the interpretability of predictions, which still represents a significant issue for more complex methods. Finally, scarcity of publicly available data may also have contributed to insufficient development of more advanced methods, such as extraction of word embeddings from clinical notes. CONCLUSIONS: Efforts are still required to improve (1) progression of clinical NLP methods from extraction toward understanding; (2) recognition of relations among entities rather than entities in isolation; (3) temporal extraction to understand past, current, and future clinical events; (4) exploitation of alternative sources of clinical knowledge; and (5) availability of large-scale, de-identified clinical corpora.",cancer | chronic diseases | clinical notes | deep learning | diabetes | electronic health records | heart disease | lung disease | machine learning | natural language processing | stroke
"BACKGROUND: Many factors involved in the onset and clinical course of the ongoing COVID-19 pandemic are still unknown. Although big data analytics and artificial intelligence are widely used in the realms of health and medicine, researchers are only beginning to use these tools to explore the clinical characteristics and predictive factors of patients with COVID-19. OBJECTIVE: Our primary objectives are to describe the clinical characteristics and determine the factors that predict intensive care unit (ICU) admission of patients with COVID-19. Determining these factors using a well-defined population can increase our understanding of the real-world epidemiology of the disease. METHODS: We used a combination of classic epidemiological methods, natural language processing (NLP), and machine learning (for predictive modeling) to analyze the electronic health records (EHRs) of patients with COVID-19. We explored the unstructured free text in the EHRs within the Servicio de Salud de Castilla-La Mancha (SESCAM) Health Care Network (Castilla-La Mancha, Spain) from the entire population with available EHRs (1,364,924 patients) from January 1 to March 29, 2020. We extracted related clinical information regarding diagnosis, progression, and outcome for all COVID-19 cases. RESULTS: A total of 10,504 patients with a clinical or polymerase chain reaction-confirmed diagnosis of COVID-19 were identified; 5519 (52.5%) were male, with a mean age of 58.2 years (SD 19.7). Upon admission, the most common symptoms were cough, fever, and dyspnea; however, all three symptoms occurred in fewer than half of the cases. Overall, 6.1% (83/1353) of hospitalized patients required ICU admission. Using a machine-learning, data-driven algorithm, we identified that a combination of age, fever, and tachypnea was the most parsimonious predictor of ICU admission; patients younger than 56 years, without tachypnea, and temperature <39 degrees Celsius (or >39 ºC without respiratory crackles) were not admitted to the ICU. In contrast, patients with COVID-19 aged 40 to 79 years were likely to be admitted to the ICU if they had tachypnea and delayed their visit to the emergency department after being seen in primary care. CONCLUSIONS: Our results show that a combination of easily obtainable clinical variables (age, fever, and tachypnea with or without respiratory crackles) predicts whether patients with COVID-19 will require ICU admission.",COVID-19 | SARS-CoV-2 | artificial intelligence | big data | electronic health records | predictive model | tachypnea
"BACKGROUND: Clinical narratives represent the main form of communication within health care, providing a personalized account of patient history and assessments, and offering rich information for clinical decision making. Natural language processing (NLP) has repeatedly demonstrated its feasibility to unlock evidence buried in clinical narratives. Machine learning can facilitate rapid development of NLP tools by leveraging large amounts of text data. OBJECTIVE: The main aim of this study was to provide systematic evidence on the properties of text data used to train machine learning approaches to clinical NLP. We also investigated the types of NLP tasks that have been supported by machine learning and how they can be applied in clinical practice. METHODS: Our methodology was based on the guidelines for performing systematic reviews. In August 2018, we used PubMed, a multifaceted interface, to perform a literature search against MEDLINE. We identified 110 relevant studies and extracted information about text data used to support machine learning, NLP tasks supported, and their clinical applications. The data properties considered included their size, provenance, collection methods, annotation, and any relevant statistics. RESULTS: The majority of datasets used to train machine learning models included only hundreds or thousands of documents. Only 10 studies used tens of thousands of documents, with a handful of studies utilizing more. Relatively small datasets were utilized for training even when much larger datasets were available. The main reason for such poor data utilization is the annotation bottleneck faced by supervised machine learning algorithms. Active learning was explored to iteratively sample a subset of data for manual annotation as a strategy for minimizing the annotation effort while maximizing the predictive performance of the model. Supervised learning was successfully used where clinical codes integrated with free-text notes into electronic health records were utilized as class labels. Similarly, distant supervision was used to utilize an existing knowledge base to automatically annotate raw text. Where manual annotation was unavoidable, crowdsourcing was explored, but it remains unsuitable because of the sensitive nature of data considered. Besides the small volume, training data were typically sourced from a small number of institutions, thus offering no hard evidence about the transferability of machine learning models. The majority of studies focused on text classification. Most commonly, the classification results were used to support phenotyping, prognosis, care improvement, resource management, and surveillance. CONCLUSIONS: We identified the data annotation bottleneck as one of the key obstacles to machine learning approaches in clinical NLP. Active learning and distant supervision were explored as a way of saving the annotation efforts. Future research in this field would benefit from alternatives such as data augmentation and transfer learning, or unsupervised learning, which do not require data annotation.",machine learning | medical informatics | medical informatics applications | natural language processing
"With the emergence of electronic health records, the reuse of clinical data offers new perspectives for the diagnosis and management of patients with rare diseases. However, there are many obstacles to the repurposing of clinical data. The development of decision support systems depends on the ability to recruit patients, extract and integrate the patients' data, mine and stratify these data, and integrate the decision support algorithm into patient care. This last step requires an adaptability of the electronic health records to integrate learning health system tools. In this literature review, we examine the research that provides solutions to unlock these barriers and accelerate translational research: structured electronic health records and free-text search engines to find patients, data warehouses and natural language processing to extract phenotypes, machine learning algorithms to classify patients, and similarity metrics to diagnose patients. Medical informatics is experiencing an impellent request to develop decision support systems, and this requires ethical considerations for clinicians and patients to ensure appropriate use of health data.",artificial intelligence | education | electronic health record | pediatric nephrology | rare diseases
"Electronic health records (EHRs), originally designed to facilitate health care delivery, are becoming a valuable data source for health research. EHR systems have two components, both of which have various components, and points of data entry, management, and analysis. The ""front end"" refers to where the data are entered, primarily by healthcare workers (e.g. physicians and nurses). The second component of EHR systems is the electronic data warehouse, or ""back-end,"" where the data are stored in a relational database. EHR data elements can be of many types, which can be categorized as structured, unstructured free-text, and imaging data. The Sunrise Clinical Manager (SCM) EHR is one example of an inpatient EHR system, which covers the city of Calgary (Alberta, Canada). This system, under the management of Alberta Health Services, is now being explored for research use. The purpose of the present paper is to describe the SCM EHR for research purposes, showing how this generalizes to EHRs in general. We further discuss advantages, challenges (e.g. potential bias and data quality issues), analytical capacities, and requirements associated with using EHRs in a health research context.",
"BACKGROUND: Recently, more electronic data sources are becoming available in the healthcare domain. Electronic health records (EHRs), with their vast amounts of potentially available data, can greatly improve healthcare. Although EHR de-identification is necessary to protect personal information, automatic de-identification of Japanese language EHRs has not been studied sufficiently. This study was conducted to raise de-identification performance for Japanese EHRs through classic machine learning, deep learning, and rule-based methods, depending on the dataset. RESULTS: Using three datasets, we implemented de-identification systems for Japanese EHRs and compared the de-identification performances found for rule-based, Conditional Random Fields (CRF), and Long-Short Term Memory (LSTM)-based methods. Gold standard tags for de-identification are annotated manually for age, hospital, person, sex, and time. We used different combinations of our datasets to train and evaluate our three methods. Our best F1-scores were 84.23, 68.19, and 81.67 points, respectively, for evaluations of the MedNLP dataset, a dummy EHR dataset that was virtually written by a medical doctor, and a Pathology Report dataset. Our LSTM-based method was the best performing, except for the MedNLP dataset. The rule-based method was best for the MedNLP dataset. The LSTM-based method achieved a good score of 83.07 points for this MedNLP dataset, which differs by 1.16 points from the best score obtained using the rule-based method. Results suggest that LSTM adapted well to different characteristics of our datasets. Our LSTM-based method performed better than our CRF-based method, yielding a 7.41 point F1-score, when applied to our Pathology Report dataset. This report is the first of study applying this LSTM-based method to any de-identification task of a Japanese EHR. CONCLUSIONS: Our LSTM-based machine learning method was able to extract named entities to be de-identified with better performance, in general, than that of our rule-based methods. However, machine learning methods are inadequate for processing expressions with low occurrence. Our future work will specifically examine the combination of LSTM and rule-based methods to achieve better performance. Our currently achieved level of performance is sufficiently higher than that of publicly available Japanese de-identification tools. Therefore, our system will be applied to actual de-identification tasks in hospitals.",De-identification | Electronic health records | Japanese language
"OBJECTIVES: To develop classification algorithms that accurately identify axial SpA (axSpA) patients in electronic health records, and compare the performance of algorithms incorporating free-text data against approaches using only International Classification of Diseases (ICD) codes. METHODS: An enriched cohort of 7853 eligible patients was created from electronic health records of two large hospitals using automated searches (⩾1 ICD codes combined with simple text searches). Key disease concepts from free-text data were extracted using NLP and combined with ICD codes to develop algorithms. We created both supervised regression-based algorithms-on a training set of 127 axSpA cases and 423 non-cases-and unsupervised algorithms to identify patients with high probability of having axSpA from the enriched cohort. Their performance was compared against classifications using ICD codes only. RESULTS: NLP extracted four disease concepts of high predictive value: ankylosing spondylitis, sacroiliitis, HLA-B27 and spondylitis. The unsupervised algorithm, incorporating both the NLP concept and ICD code for AS, identified the greatest number of patients. By setting the probability threshold to attain 80% positive predictive value, it identified 1509 axSpA patients (mean age 53 years, 71% male). Sensitivity was 0.78, specificity 0.94 and area under the curve 0.93. The two supervised algorithms performed similarly but identified fewer patients. All three outperformed traditional approaches using ICD codes alone (area under the curve 0.80-0.87). CONCLUSION: Algorithms incorporating free-text data can accurately identify axSpA patients in electronic health records. Large cohorts identified using these novel methods offer exciting opportunities for future clinical research.",ICD code | ankylosing spondylitis | axial spondyloarthritis | classification | electronic health records | free-text | machine learning | natural language processing | phenotyping
"BACKGROUND: Much effort has been put into the use of automated approaches, such as natural language processing (NLP), to mine or extract data from free-text medical records in order to construct comprehensive patient profiles for delivering better health care. Reusing NLP models in new settings, however, remains cumbersome, as it requires validation and retraining on new data iteratively to achieve convergent results. OBJECTIVE: The aim of this work is to minimize the effort involved in reusing NLP models on free-text medical records. METHODS: We formally define and analyze the model adaptation problem in phenotype-mention identification tasks. We identify ""duplicate waste"" and ""imbalance waste,"" which collectively impede efficient model reuse. We propose a phenotype embedding-based approach to minimize these sources of waste without the need for labelled data from new settings. RESULTS: We conduct experiments on data from a large mental health registry to reuse NLP models in four phenotype-mention identification tasks. The proposed approach can choose the best model for a new task, identifying up to 76% waste (duplicate waste), that is, phenotype mentions without the need for validation and model retraining and with very good performance (93%-97% accuracy). It can also provide guidance for validating and retraining the selected model for novel language patterns in new tasks, saving around 80% waste (imbalance waste), that is, the effort required in ""blind"" model-adaptation approaches. CONCLUSIONS: Adapting pretrained NLP models for new tasks can be more efficient and effective if the language pattern landscapes of old settings and new settings can be made explicit and comparable. Our experiments show that the phenotype-mention embedding approach is an effective way to model language patterns for phenotype-mention identification tasks and that its use can guide efficient NLP model reuse.",clustering | electronic health records | machine learning | model adaptation | natural language processing | phenotype | phenotype embedding | text mining | word embedding
"IMPORTANCE: Interventions based on behavioral science might reduce inappropriate antibiotic prescribing. OBJECTIVE: To assess effects of behavioral interventions and rates of inappropriate (not guideline-concordant) antibiotic prescribing during ambulatory visits for acute respiratory tract infections. DESIGN, SETTING, AND PARTICIPANTS: Cluster randomized clinical trial conducted among 47 primary care practices in Boston and Los Angeles. Participants were 248 enrolled clinicians randomized to receive 0, 1, 2, or 3 interventions for 18 months. All clinicians received education on antibiotic prescribing guidelines on enrollment. Interventions began between November 1, 2011, and October 1, 2012. Follow-up for the latest-starting sites ended on April 1, 2014. Adult patients with comorbidities and concomitant infections were excluded. INTERVENTIONS: Three behavioral interventions, implemented alone or in combination: suggested alternatives presented electronic order sets suggesting nonantibiotic treatments; accountable justification prompted clinicians to enter free-text justifications for prescribing antibiotics into patients' electronic health records; peer comparison sent emails to clinicians that compared their antibiotic prescribing rates with those of ""top performers"" (those with the lowest inappropriate prescribing rates). MAIN OUTCOMES AND MEASURES: Antibiotic prescribing rates for visits with antibiotic-inappropriate diagnoses (nonspecific upper respiratory tract infections, acute bronchitis, and influenza) from 18 months preintervention to 18 months afterward, adjusting each intervention's effects for co-occurring interventions and preintervention trends, with random effects for practices and clinicians. RESULTS: There were 14,753 visits (mean patient age, 47 years; 69% women) for antibiotic-inappropriate acute respiratory tract infections during the baseline period and 16,959 visits (mean patient age, 48 years; 67% women) during the intervention period. Mean antibiotic prescribing rates decreased from 24.1% at intervention start to 13.1% at intervention month 18 (absolute difference, -11.0%) for control practices; from 22.1% to 6.1% (absolute difference, -16.0%) for suggested alternatives (difference in differences, -5.0% [95% CI, -7.8% to 0.1%]; P = .66 for differences in trajectories); from 23.2% to 5.2% (absolute difference, -18.1%) for accountable justification (difference in differences, -7.0% [95% CI, -9.1% to -2.9%]; P < .001); and from 19.9% to 3.7% (absolute difference, -16.3%) for peer comparison (difference in differences, -5.2% [95% CI, -6.9% to -1.6%]; P < .001). There were no statistically significant interactions (neither synergy nor interference) between interventions. CONCLUSIONS AND RELEVANCE: Among primary care practices, the use of accountable justification and peer comparison as behavioral interventions resulted in lower rates of inappropriate antibiotic prescribing for acute respiratory tract infections. TRIAL REGISTRATION: clinicaltrials.gov Identifier: NCT01454947.",
"The purpose of this study was to evaluate the quality of adverse drug reaction (ADR) documentation in a state-wide electronic health record (EHR), and to assess the impact of the interface design on documentation accuracy and ability to provide decision support. Data were extracted from 43 011 unique records in a state-wide electronic health record in South Australia, Australia. Information obtained included ADR coding as allergy or intolerance, allergen name, reaction, and occupation of those entering data. Categorization into drug allergy or intolerance was assessed for accuracy. Reactions were entered predominantly by nurses (60.1%), also by doctors (31.0%) and pharmacists (6.1%). Of 27 314 reactions, 86.5% were coded as allergy and 13.5% as intolerance. The majority (78.2%) described reactions to drugs (as opposed to food, environmental or contact allergens), predominantly chosen from the drug database (96.4%). Many entries used free text for the reaction description (27.4%). Terms found in the predefined list under the allergy heading were more likely to be categorized as allergy, even when the mechanism was pharmacological intolerance. Only 45.1% (n = 1671/3705) of reactions consistent with intolerance (eg, ""nausea,"" ""diarrhea"") were correctly categorized as such, although categorization by pharmacists was more accurate (P < .0001). These data suggest that ADR categorization as allergy or intolerance is influenced by the EHR design. The obligatory classification of ADRs into allergy or intolerance was not well understood and does not appear to have practical benefit.",adverse drug reactions | drug allergy | electronic health record
"Suicidal ideation is a risk factor for self-harm, completed suicide and can be indicative of mental health issues. Adolescents are a particularly vulnerable group, but few studies have examined suicidal behaviour prevalence in large cohorts. Electronic Health Records (EHRs) are a rich source of secondary health care data that could be used to estimate prevalence. Most EHR documentation related to suicide risk is written in free text, thus requiring Natural Language Processing (NLP) approaches. We adapted and evaluated a simple lexicon- and rule-based NLP approach to identify suicidal adolescents from a large EHR database. We developed a comprehensive manually annotated EHR reference standard and assessed NLP performance at both document and patient level on data from 200 patients ( 5000 documents). We achieved promising results (>80% f1 score at both document and patient level). Simple NLP approaches can be successfully used to identify patients who exhibit suicidal risk behaviour, and our proposed approach could be useful for other populations and settings.",Electronic Health Records | Natural Language Processing | Suicide
"BACKGROUND: Free-text descriptions in electronic health records (EHRs) can be of interest for clinical research and care optimization. However, free text cannot be readily interpreted by a computer and, therefore, has limited value. Natural Language Processing (NLP) algorithms can make free text machine-interpretable by attaching ontology concepts to it. However, implementations of NLP algorithms are not evaluated consistently. Therefore, the objective of this study was to review the current methods used for developing and evaluating NLP algorithms that map clinical text fragments onto ontology concepts. To standardize the evaluation of algorithms and reduce heterogeneity between studies, we propose a list of recommendations. METHODS: Two reviewers examined publications indexed by Scopus, IEEE, MEDLINE, EMBASE, the ACM Digital Library, and the ACL Anthology. Publications reporting on NLP for mapping clinical text from EHRs to ontology concepts were included. Year, country, setting, objective, evaluation and validation methods, NLP algorithms, terminology systems, dataset size and language, performance measures, reference standard, generalizability, operational use, and source code availability were extracted. The studies' objectives were categorized by way of induction. These results were used to define recommendations. RESULTS: Two thousand three hundred fifty five unique studies were identified. Two hundred fifty six studies reported on the development of NLP algorithms for mapping free text to ontology concepts. Seventy-seven described development and evaluation. Twenty-two studies did not perform a validation on unseen data and 68 studies did not perform external validation. Of 23 studies that claimed that their algorithm was generalizable, 5 tested this by external validation. A list of sixteen recommendations regarding the usage of NLP systems and algorithms, usage of data, evaluation and validation, presentation of results, and generalizability of results was developed. CONCLUSION: We found many heterogeneous approaches to the reporting on the development and evaluation of NLP algorithms that map clinical text to ontology concepts. Over one-fourth of the identified publications did not perform an evaluation. In addition, over one-fourth of the included studies did not perform a validation, and 88% did not perform external validation. We believe that our recommendations, alongside an existing reporting standard, will increase the reproducibility and reusability of future studies and NLP algorithms in medicine.",Annotation | Concept mapping | Entity linking | Evaluation studies | Named-entity recognition | Natural language processing | Ontologies | Recommendations for future studies
"BACKGROUND: Social and behavioral determinants of health (SBDH) are environmental and behavioral factors that often impede disease management and result in sexually transmitted infections. Despite their importance, SBDH are inconsistently documented in electronic health records (EHRs) and typically collected only in an unstructured format. Evidence suggests that structured data elements present in EHRs can contribute further to identify SBDH in the patient record. OBJECTIVE: Explore the automated inference of both the presence of SBDH documentation and individual SBDH risk factors in patient records. Compare the relative ability of clinical notes and structured EHR data, such as laboratory measurements and diagnoses, to support inference. METHODS: We attempt to infer the presence of SBDH documentation in patient records, as well as patient status of 11 SBDH, including alcohol abuse, homelessness, and sexual orientation. We compare classification performance when considering clinical notes only, structured data only, and notes and structured data together. We perform an error analysis across several SBDH risk factors. RESULTS: Classification models inferring the presence of SBDH documentation achieved good performance (F1 score: 92.7-78.7; F1 considered as the primary evaluation metric). Performance was variable for models inferring patient SBDH risk status; results ranged from F1 = 82.7 for LGBT (lesbian, gay, bisexual, and transgender) status to F1 = 28.5 for intravenous drug use. Error analysis demonstrated that lexical diversity and documentation of historical SBDH status challenge inference of patient SBDH status. Three of five classifiers inferring topic-specific SBDH documentation and 10 of 11 patient SBDH status classifiers achieved highest performance when trained using both clinical notes and structured data. CONCLUSION: Our findings suggest that combining clinical free-text notes and structured data provide the best approach in classifying patient SBDH status. Inferring patient SBDH status is most challenging among SBDH with low prevalence and high lexical diversity.",
"OBJECTIVE: The aim of this study was to systematically assess the application and potential benefits of natural language processing (NLP) in surgical outcomes research. SUMMARY BACKGROUND DATA: Widespread implementation of electronic health records (EHRs) has generated a massive patient data source. Traditional methods of data capture, such as billing codes and/or manual review of free-text narratives in EHRs, are highly labor-intensive, costly, subjective, and potentially prone to bias. METHODS: A literature search of PubMed, MEDLINE, Web of Science, and Embase identified all articles published starting in 2000 that used NLP models to assess perioperative surgical outcomes. Evaluation metrics of NLP systems were assessed by means of pooled analysis and meta-analysis. Qualitative synthesis was carried out to assess the results and risk of bias on outcomes. RESULTS: The present study included 29 articles, with over half (n = 15) published after 2018. The most common outcome identified using NLP was postoperative complications (n = 14). Compared to traditional non-NLP models, NLP models identified postoperative complications with higher sensitivity [0.92 (0.87-0.95) vs 0.58 (0.33-0.79), P < 0.001]. The specificities were comparable at 0.99 (0.96-1.00) and 0.98 (0.95-0.99), respectively. Using summary of likelihood ratio matrices, traditional non-NLP models have clinical utility for confirming documentation of outcomes/diagnoses, whereas NLP models may be reliably utilized for both confirming and ruling out documentation of outcomes/diagnoses. CONCLUSIONS: NLP usage to extract a range of surgical outcomes, particularly postoperative complications, is accelerating across disciplines and areas of clinical outcomes research. NLP and traditional non-NLP approaches demonstrate similar performance measures, but NLP is superior in ruling out documentation of surgical outcomes.",
"BACKGROUND: Mental health supported housing services are a key component in the rehabilitation of people with severe and complex needs. They are implemented widely in the UK and other deinstitutionalised countries but there have been few empirical studies of their effectiveness due to the logistic challenges and costs of standard research methods. The Clinical Record Interactive Search (CRIS) tool, developed to de-identify and interrogate routinely recorded electronic health records, may provide an alternative to evaluate supported housing services. METHODS: The feasibility of using the Camden and Islington NHS Foundation Trust CRIS database to identify a sample of users of mental health supported accommodation services. Two approaches to data interrogation and case identification were compared; using structured fields indicating individual's accommodation status, and iterative development of free text searches of clinical notes referencing supported housing. The data used were recorded over a 10-year-period (01-January-2008 to 31-December-2017). RESULTS: Both approaches were carried out by one full-time researcher over four weeks (150 hours). Two structured fields indicating accommodation status were found, 2,140 individuals had a value in at least one of the fields representative of supported accommodation. The free text search of clinical notes returned 21,103 records pertaining to 1,105 individuals. A manual review of 10% of the notes indicated an estimated 733 of these individuals had used a supported housing service, a positive predictive value of 66.4%. Over two-thirds of the individuals returned in the free text search (768/1,105, 69.5%) were identified via the structured fields approach. Although the estimated positive predictive value was relatively high, a substantial proportion of the individuals appearing only in the free text search (337/1,105, 30.5%) are likely to be false positives. CONCLUSIONS: It is feasible and requires minimal resources to use de-identified electronic health record search tools to identify large samples of users of mental health supported housing using structured and free text fields. Further work is needed to establish the availability and completion of variables relevant to specific clinical research questions in order to fully assess the utility of electronic health records in evaluating the effectiveness of these services.",
"OBJECTIVES: Despite the availability of many frailty measures to identify older adults at risk, frailty instruments are not routinely used for risk assessment in population health management. Here, we assessed the potential value of electronic health records (EHRs) and administrative claims in providing the necessary data for variables used across various frailty instruments. SETTING AND PARTICIPANTS: The review focused on studies conducted worldwide. Participants included older people aged 50 and older. DESIGN: We identified frailty instruments published between 2011 and 2018. Frailty variables used in each of the frailty instruments were extracted, grouped, and categorized across health determinants and various clinical factors. MEASURES: The availability of the extracted frailty variables across various data sources (e.g., EHRs, administrative claims, and surveys) was evaluated by experts. RESULTS: We identified 135 frailty instruments, which contained 593 unique variables. Clinical determinants of health were the best represented variables across frailty instruments (n = 516; 87 %), unlike social and health services factors (n = 33; ∼5% and n = 32; ∼5%). Most frailty instruments require at least one variable that is not routinely available in EHRs or claims (n = 113; ∼83 %). Only 22 frailty instruments have the potential to completely rely on EHR (structured or free-text data) and/or claims data, and possibly be operationalized on a population-level. CONCLUSIONS AND IMPLICATIONS: Frailty instruments continue to be highly survey-based. More research is therefore needed to develop EHR-based frailty instruments for population health management. This will permit organizations and societies to stratify risk and better allocate resources among different older adult populations.",Administrative claims data | Electronic health records | Frailty instruments | Geriatric frailty | Population-level frailty measurement
"Natural language processing (NLP) is a technology that uses computer-based linguistics and artificial intelligence to identify and extract information from free-text data sources such as progress notes, procedure and pathology reports, and laboratory and radiologic test results. With the creation of large databases and the trajectory of health care reform, NLP holds the promise of enhancing the availability, quality, and utility of clinical information with the goal of improving documentation, quality, and efficiency of health care in the United States. To date, NLP has shown promise in automatically determining appropriate colonoscopy intervals and identifying cases of inflammatory bowel disease from electronic health records. The objectives of this review are to provide background on NLP and its associated terminology, to describe how NLP has been used thus far in the field of digestive diseases, and to identify its potential future uses.",Adenoma Detection Rate | Clinical Decision Support | Colonoscopy | Inflammatory Bowel Disease | Natural Language Processing | Performance Measures | Quality Improvement
"A major challenge for radiologists is obtaining meaningful clinical follow-up information for even a small percentage of cases encountered and dictated. Traditional methods, such as keeping medical record number follow-up lists, discussing cases with rounding clinical teams, and discussing cases at tumor boards, are effective at keeping radiologists informed of clinical outcomes but are time intensive and provide follow-up for a small subset of cases. To this end, the authors developed a picture archiving and communication system-accessible electronic health record (EHR)-integrated program called Correlate, which allows the user to easily enter free-text search queries regarding desired clinical follow-up information, with minimal interruption to the workflow. The program uses natural language processing (NLP) to process the query and parse relevant future clinical data from the EHR. Results are ordered in terms of clinical relevance, and the user is e-mailed a link to results when these are available for viewing. A customizable personal database of queries and results is also maintained for convenient future access. Correlate aids radiologists in efficiently obtaining useful clinical follow-up information that can improve patient care, help keep radiologists integrated with other specialties and referring physicians, and provide valuable experiential learning. The authors briefly review the history of automated clinical follow-up tools and discuss the design and function of the Correlate program, which uses NLP to perform intelligent prospective searches of the EHR. (©) RSNA, 2017.",
"Background: The impact of sex and gender in the incidence and severity of coronavirus disease 2019 (COVID-19) remains controversial. Here, we aim to describe the characteristics of COVID-19 patients at disease onset, with special focus on the diagnosis and management of female patients with COVID-19. Methods: We explored the unstructured free text in the electronic health records (EHRs) within the SESCAM Healthcare Network (Castilla La-Mancha, Spain). The study sample comprised the entire population with available EHRs (1,446,452 patients) from January 1st to May 1st, 2020. We extracted patients' clinical information upon diagnosis, progression, and outcome for all COVID-19 cases. Results: A total of 4,780 patients with a confirmed diagnosis of COVID-19 were identified. Of these, 2,443 (51%) were female, who were on average 1.5 years younger than male patients (61.7 ± 19.4 vs. 63.3 ± 18.3, p = 0.0025). There were more female COVID-19 cases in the 15-59-year-old interval, with the greatest sex ratio (95% confidence interval) observed in the 30-39-year-old range (1.69; 1.35-2.11). Upon diagnosis, headache, anosmia, and ageusia were significantly more frequent in females than males. Imaging by chest X-ray or blood tests were performed less frequently in females (65.5% vs. 78.3% and 49.5% vs. 63.7%, respectively), all p < 0.001. Regarding hospital resource use, females showed less frequency of hospitalization (44.3% vs. 62.0%) and intensive care unit admission (2.8% vs. 6.3%) than males, all p < 0.001. Conclusion: Our results indicate important sex-dependent differences in the diagnosis, clinical manifestation, and treatment of patients with COVID-19. These results warrant further research to identify and close the gender gap in the ongoing pandemic.",COVID-19 | SARS-CoV-2 | artificial intelligence | natural language processing | sex differences
"BACKGROUND: The use of clinical data in electronic health records for machine-learning or data analytics depends on the conversion of free text into machine-readable codes. We have examined the feasibility of capturing the neurological examination as machine-readable codes based on UMLS Metathesaurus concepts. METHODS: We created a target ontology for capturing the neurological examination using 1100 concepts from the UMLS Metathesaurus. We created a dataset of 2386 test-phrases based on 419 published neurological cases. We then mapped the test-phrases to the target ontology. RESULTS: We were able to map all of the 2386 test-phrases to 601 unique UMLS concepts. A neurological examination ontology with 1100 concepts has sufficient breadth and depth of coverage to encode all of the neurologic concepts derived from the 419 test cases. Using only pre-coordinated concepts, component ontologies of the UMLS, such as HPO, SNOMED CT, and OMIM, do not have adequate depth and breadth of coverage to encode the complexity of the neurological examination. CONCLUSION: An ontology based on a subset of UMLS has sufficient breadth and depth of coverage to convert deficits from the neurological examination into machine-readable codes using pre-coordinated concepts. The use of a small subset of UMLS concepts for a neurological examination ontology offers the advantage of improved manageability as well as the opportunity to curate the hierarchy and subsumption relationships.",Electronic health records | Neurological examination | Ontology | SNOMED CT | UMLS Metathesaurus
"Informatics tools to extract and analyze clinical information on patients have lagged behind data-mining developments in bioinformatics. While the analyses of an individual's partial or complete genotype is nearly a reality, the phenotypic characteristics that accompany the genotype are not well known and largely inaccessible in free-text patient health records. As the adoption of electronic medical records increases, there exists an urgent need to extract pertinent phenotypic information and make that available to clinicians and researchers. This usually requires the data to be in a structured format that is both searchable and amenable to computation. Using inflammatory bowel disease as an example, this study demonstrates the utility of a natural language processing system (MedLEE) in mining clinical notes in the paperless VA Health Care System. This adaptation of MedLEE is useful for identifying patients with specific clinical conditions, those at risk for or those with symptoms suggestive of those conditions.",
"During the last two decades, improving the quality and safety of healthcare has become a focus in rheumatology. Widespread use of electronic health records (EHRs) and the availability of digital data have the potential to drive quality improvement, improve patient outcomes, and prevent adverse events. In the coming years, developing and leveraging tools within the EHR will be the key to making the next big strides in improving the health of patients with rheumatoid arthritis and other rheumatic diseases, including building EHR infrastructure to capture patient outcomes and developing automated methods to retrieve information from free text of clinical notes.",Clinical informatics | Patient reported outcomes | Quality of care | Rheumatoid arthritis
"Symptom management is one of the essential functions of nurses in inpatient settings; yet, little is understood about the manner in which nurses use electronic health records for symptom documentation. Therefore, the purpose of this systematic review is to characterize nurses' use of electronic health records for documentation of symptom assessment and management in inpatient settings, to inform design studies that better support electronic health records for patient symptom management by nurses. We searched the Ovid Medline (1946-current), Cumulative Index to Nursing and Allied Health Literature (EBSCO, 1981-current), and Excerpta Medica Database (Embase.com, 1974-current) databases from inception through May 2015 using multiple subject headings and ""free text"" key words, representing the concepts of electronic medical records, symptom documentation, and inpatient setting. One thousand nine hundred eighty-two articles were returned from the search. Eighteen publications from the years 2003 to 2014 were included after abstract and full text review. Studies heavily focused on a pain as symptom. Nurses face challenges when using electronic health records that can threaten quality and safety of care. Clinical, design, and administrative recommendations were identified to overcome the challenges of nurses' electronic health record use. A call for interdisciplinary, comprehensive, systematic interventions and user-centered design of information systems is needed.",
"OBJECTIVE: To summarize the best papers in the field of Knowledge Representation and Management (KRM). METHODS: A comprehensive review of medical informatics literature was performed to select some of the most interesting papers of KRM and natural language processing (NLP) published in 2013. RESULTS: Four articles were selected, one focuses on Electronic Health Record (EHR) interoperability for clinical pathway personalization based on structured data. The other three focus on NLP (corpus creation, de-identification, and co-reference resolution) and highlight the increase in NLP tools performances. CONCLUSION: NLP tools are close to being seriously concurrent to humans in some annotation tasks. Their use could increase drastically the amount of data usable for meaningful use of EHR.",Medical informatics | knowledge representation | natural language processing | ontology | semantic web
"Electronic health records (EHRs) are being increasingly utilized and form a unique source of extensive data gathered during routine clinical care. Through use of codified and free text concepts identified using clinical informatics tools, disease labels can be assigned with a high degree of accuracy. Analysis linking such EHR-assigned disease labels to a biospecimen repository has demonstrated that genetic associations identified in prospective cohorts can be replicated with adequate statistical power and novel phenotypic associations identified. In addition, genetic discovery research can be performed utilizing clinical, laboratory, and procedure data obtained during care. Challenges with such research include the need to tackle variability in quality and quantity of EHR data and importance of maintaining patient privacy and data security. With appropriate safeguards, this novel and emerging field of research offers considerable promise and potential to further scientific research in gastroenterology efficiently, cost-effectively, and with engagement of patients and communities.",Electronic Health Records | Genetics | Informatics | Natural Language Processing
"OBJECTIVE: Electronic health records (EHR) are increasingly being recognized as a major source of data reusable for medical research and quality monitoring, although patient identification and assessment of symptoms (characterization) remain challenging, especially in complex diseases such as systemic lupus erythematosus (SLE). Current coding systems are unable to assess information recorded in the physician's free-text notes. This study shows that text mining can be used as a reliable alternative. METHODS: In a multidisciplinary research team of data scientists and medical experts, a text mining algorithm on 4607 patient records was developed to assess the diagnosis of 14 different immune-mediated inflammatory diseases and the presence of 18 different symptoms in the EHR. The text mining algorithm included key words in the EHR, while mining the context for exclusion phrases. The accuracy of the text mining algorithm was assessed by manually checking the EHR of 100 random patients suspected of having SLE for diagnoses and symptoms and comparing the outcome with the outcome of the text mining algorithm. RESULTS: After evaluation of 100 patient records, the text mining algorithm had a sensitivity of 96.4% and a specificity of 93.3% in assessing the presence of SLE. The algorithm detected potentially life-threatening symptoms (nephritis, pleuritis) with good sensitivity (80%-82%) and high specificity (97%-97%). CONCLUSION: We present a text mining algorithm that can accurately identify and characterize patients with SLE using routinely collected data from the EHR. Our study shows that using text mining, data from the EHR can be reused in research and quality control.",
"BACKGROUND: CDK4/6 inhibitors plus endocrine therapies are the current standard of care in the first-line treatment of HR+/HER2-negative metastatic breast cancer, but there are no well-established clinical or molecular predictive factors for patient response. In the era of personalised oncology, new approaches for developing predictive models of response are needed. MATERIALS AND METHODS: Data derived from the electronic health records (EHRs) of real-world patients with HR+/HER2-negative advanced breast cancer were used to develop predictive models for early and late progression to first-line treatment. Two machine learning approaches were used: a classic approach using a data set of manually extracted features from reviewed (EHR) patients, and a second approach using natural language processing (NLP) of free-text clinical notes recorded during medical visits. RESULTS: Of the 610 patients included, there were 473 (77.5%) progressions to first-line treatment, of which 126 (20.6%) occurred within the first 6 months. There were 152 patients (24.9%) who showed no disease progression before 28 months from the onset of first-line treatment. The best predictive model for early progression using the manually extracted dataset achieved an area under the curve (AUC) of 0.734 (95% CI 0.687-0.782). Using the NLP free-text processing approach, the best model obtained an AUC of 0.758 (95% CI 0.714-0.800). The best model to predict long responders using manually extracted data obtained an AUC of 0.669 (95% CI 0.608-0.730). With NLP free-text processing, the best model attained an AUC of 0.752 (95% CI 0.705-0.799). CONCLUSIONS: Using machine learning methods, we developed predictive models for early and late progression to first-line treatment of HR+/HER2-negative metastatic breast cancer, also finding that NLP-based machine learning models are slightly better than predictive models based on manually obtained data.",Breast cancer | CDK4/6-inhibitors | Electronic health records | Hormone receptor positive | Natural language processing | machine learning
"In recent years, machine learning approaches have been successfully applied to analysis of patient symptom data in the context of disease diagnosis, at least where such data is well codified. However, much of the data present in Electronic Health Records (EHR) is unlikely to prove suitable for classic machine learning approaches. In particular, the use of free (or unstructured) text for clinical notes presents significant analytical opportunities, but also unique difficulties. Furthermore, the wide dispersal of health data relating to individuals necessitates the development of decentralized solutions. We provide, in this paper, an overview of our approach to develop a neural network framework for patient classification in the environment of EHRs where data may be heterogeneous, incomplete (containing missing values), and noisy. In this paper we describe our system which provides prediction of outlier cases which are likely to relate to frequent attender patients, which acheives an Area-Under-the-Curve score of up to 0.92.",
"BACKGROUND: A wealth of clinical information is buried in free text of electronic health records (EHR), and converting clinical information to machine-understandable form is crucial for the secondary use of EHRs. Laboratory test results, as one of the most important types of clinical information, are written in various styles in free text of EHRs. This has brought great difficulties for data integration and utilization of EHRs. Therefore, developing technology to normalize different expressions of laboratory test results in free text is indispensable for the secondary use of EHRs. METHODS: In this study, we developed a knowledge-based method named LATTE (transforming lab test results), which could transform various expressions of laboratory test results into a normalized and machine-understandable format. We first identified the analyte of a laboratory test result with a dictionary-based method and then designed a series of rules to detect information associated with the analyte, including its specimen, measured value, unit of measure, conclusive phrase and sampling factor. We determined whether a test result is normal or abnormal by understanding the meaning of conclusive phrases or by comparing its measured value with an appropriate normal range. Finally, we converted various expressions of laboratory test results, either in numeric or textual form, into a normalized form as ""specimen-analyte-abnormality"". With this method, a laboratory test with the same type of abnormality would have the same representation, regardless of the way that it is mentioned in free text. RESULTS: LATTE was developed and optimized on a training set including 8894 laboratory test results from 756 EHRs, and evaluated on a test set including 3740 laboratory test results from 210 EHRs. Compared to experts' annotations, LATTE achieved a precision of 0.936, a recall of 0.897 and an F1 score of 0.916 on the training set, and a precision of 0.892, a recall of 0.843 and an F1 score of 0.867 on the test set. For 223 laboratory tests with at least two different expression forms in the test set, LATTE transformed 85.7% (2870/3350) of laboratory test results into a normalized form. Besides, LATTE achieved F1 scores above 0.8 for EHRs from 18 of 21 different hospital departments, indicating its generalization capabilities in normalizing laboratory test results. CONCLUSION: In conclusion, LATTE is an effective method for normalizing various expressions of laboratory test results in free text of EHRs. LATTE will facilitate EHR-based applications such as cohort querying, patient clustering and machine learning. AVAILABILITY: LATTE is freely available for download on GitHub (https://github.com/denglizong/LATTE).",Data normalization | EHR-based phenotyping | Knowledge-based system
"BACKGROUND: Automated machine-learning systems are able to de-identify electronic medical records, including free-text clinical notes. Use of such systems would greatly boost the amount of data available to researchers, yet their deployment has been limited due to uncertainty about their performance when applied to new datasets. OBJECTIVE: We present practical options for clinical note de-identification, assessing performance of machine learning systems ranging from off-the-shelf to fully customized. METHODS: We implement a state-of-the-art machine learning de-identification system, training and testing on pairs of datasets that match the deployment scenarios. We use clinical notes from two i2b2 competition corpora, the Physionet Gold Standard corpus, and parts of the MIMIC-III dataset. RESULTS: Fully customized systems remove 97-99% of personally identifying information. Performance of off-the-shelf systems varies by dataset, with performance mostly above 90%. Providing a small labeled dataset or large unlabeled dataset allows for fine-tuning that improves performance over off-the-shelf systems. CONCLUSION: Health organizations should be aware of the levels of customization available when selecting a de-identification deployment solution, in order to choose the one that best matches their resources and target performance level.",Clinical notes | De-identification | Electronic health records | Free text | Natural language processing | Recurrent neural networks
"BACKGROUND: Stroke severity is an important predictor of patient outcomes and is commonly measured with the National Institutes of Health Stroke Scale (NIHSS) scores. Because these scores are often recorded as free text in physician reports, structured real-world evidence databases seldom include the severity. The aim of this study was to use machine learning models to impute NIHSS scores for all patients with newly diagnosed stroke from multi-institution electronic health record (EHR) data. METHODS: NIHSS scores available in the Optum© de-identified Integrated Claims-Clinical dataset were extracted from physician notes by applying natural language processing (NLP) methods. The cohort analyzed in the study consists of the 7149 patients with an inpatient or emergency room diagnosis of ischemic stroke, hemorrhagic stroke, or transient ischemic attack and a corresponding NLP-extracted NIHSS score. A subset of these patients (n = 1033, 14%) were held out for independent validation of model performance and the remaining patients (n = 6116, 86%) were used for training the model. Several machine learning models were evaluated, and parameters optimized using cross-validation on the training set. The model with optimal performance, a random forest model, was ultimately evaluated on the holdout set. RESULTS: Leveraging machine learning we identified the main factors in electronic health record data for assessing stroke severity, including death within the same month as stroke occurrence, length of hospital stay following stroke occurrence, aphagia/dysphagia diagnosis, hemiplegia diagnosis, and whether a patient was discharged to home or self-care. Comparing the imputed NIHSS scores to the NLP-extracted NIHSS scores on the holdout data set yielded an R(2) (coefficient of determination) of 0.57, an R (Pearson correlation coefficient) of 0.76, and a root-mean-squared error of 4.5. CONCLUSIONS: Machine learning models built on EHR data can be used to determine proxies for stroke severity. This enables severity to be incorporated in studies of stroke patient outcomes using administrative and EHR databases.",Database | Outcomes research | Real-world evidence
"The process of documentation is one of the most important parts of electronic health records (EHR). It is time-consuming, and up until now, available documentation procedures have not been able to overcome this type of EHR limitations. Thus, entering information into EHR still has remained a challenge. In this study, by applying the trigram language model, we presented a method to predict the next words while typing free texts. It is hypothesized that using this system may save typing time of free text. The words prediction model introduced in this research was trained and tested on the free texts regarding to colonoscopy, transesophageal echocardiogram, and anterior-cervical-decompression. Required time of typing for each of the above-mentioned reports calculated and compared with manual typing of the same words. It is revealed that 33.36% reduction in typing time and 73.53% reduction in keystroke. The designed system reduced the time of typing free text which might be an approach for EHRs improvement in terms of documentation.",Data capture | Data entry | Electronic health record | Free-text | N-gram | Natural language processing | Trigram model | Word prediction
"The safety of medication use has been a priority in the United States since the late 1930s. Recently, it has gained prominence due to the increasing amount of data suggesting that a large amount of patient harm is preventable and can be mitigated with effective risk strategies that have not been sufficiently adopted. Adverse events from medications are part of clinical practice, but the ability to identify a patient's risk and to minimize that risk must be a priority. The ability to identify adverse events has been a challenge due to limitations of available data sources, which are often free text. The use of natural language processing (NLP) may help to address these limitations. NLP is the artificial intelligence domain of computer science that uses computers to manipulate unstructured data (i.e., narrative text or speech data) in the context of a specific task. In this narrative review, we illustrate the fundamentals of NLP and discuss NLP's application to medication safety in four data sources: electronic health records, Internet-based data, published literature, and reporting systems. Given the magnitude of available data from these sources, a growing area is the use of computer algorithms to help automatically detect associations between medications and adverse effects. The main benefit of NLP is in the time savings associated with automation of various medication safety tasks such as the medication reconciliation process facilitated by computers, as well as the potential for near-real-time identification of adverse events for postmarketing surveillance such as those posted on social media that would otherwise go unanalyzed. NLP is limited by a lack of data sharing between health care organizations due to insufficient interoperability capabilities, inhibiting large-scale adverse event monitoring across populations. We anticipate that future work in this area will focus on the integration of data sources from different domains to improve the ability to identify potential adverse events more quickly and to improve clinical decision support with regard to a patient's estimated risk for specific adverse events at the time of medication prescription or review.",adverse drug reaction reporting systems | electronic health records | medical informatics | natural language processing | patient safety | pharmacovigilance | social media | supervised machine learning
"There is a lack of quantitative evidence concerning UK (United Kingdom) Armed Forces (AF) veterans who access secondary mental health care services-specialist care often delivered in high intensity therapeutic clinics or hospitals-for their mental health difficulties. The current study aimed to investigate the utility and feasibility of identifying veterans accessing secondary mental health care services using National Health Service (NHS) electronic health records (EHRs) in the UK. Veterans were manually identified using the Clinical Record Interactive Search (CRIS) system-a database holding secondary mental health care EHRs for an NHS Trust in the UK. We systematically and manually searched CRIS for veterans, by applying a military-related key word search strategy to the free-text clinical notes completed by clinicians. Relevant data on veterans' socio-demographic characteristics, mental disorder diagnoses and treatment pathways through care were extracted for analysis. This study showed that it is feasible, although time consuming, to identify veterans through CRIS. Using the military-related key word search strategy identified 1600 potential veteran records. Following manual review, 693 (43.3%) of these records were verified as ""probable"" veterans and used for analysis. They had a median age of 74 years (interquartile range (IQR): 53-86); the majority were male (90.8%) and lived alone (38.0%). The most common mental diagnoses overall were depressive disorders (22.9%), followed by alcohol use disorders (10.5%). Differences in care pathways were observed between pre and post national service (NS) era veterans. This feasibility study represents a first step in showing that it is possible to identify veterans through free-text clinical notes. It is also the first to compare veterans from pre and post NS era.",United Kingdom | electronic health records | feasibility study | mental health | national health service | secondary mental health care | veterans
"In line with the current trajectory of healthcare reform, significant emphasis has been placed on improving the utilization of data collected during a clinical encounter. Although the structured fields of electronic health records have provided a convenient foundation on which to begin such efforts, it was well understood that a substantial portion of relevant information is confined in the free-text narratives documenting care. Unfortunately, extracting meaningful information from such narratives is a non-trivial task, traditionally requiring significant manual effort. Today, computational approaches from a field known as Natural Language Processing (NLP) are poised to make a transformational impact in the analysis and utilization of these documents across healthcare practice and research, particularly in procedure-heavy sub-disciplines such as gastroenterology (GI). As such, this manuscript provides a clinically focused review of NLP systems in GI practice. It begins with a detailed synopsis around the state of NLP techniques, presenting state-of-the-art methods and typical use cases in both clinical settings and across other domains. Next, it will present a robust literature review around current applications of NLP within four prominent areas of gastroenterology including endoscopy, inflammatory bowel disease, pancreaticobiliary, and liver diseases. Finally, it concludes with a discussion of open problems and future opportunities of this technology in the field of gastroenterology and health care as a whole.",Artificial intelligence | Gastroenterology | Health care | Natural Language Processing
A natural language processing (NLP) algorithm to extract microbial keratitis morphology measurements from the electronic health record (EHR) was 75-96% sensitive and 91%-96% specific. NLP accurately extracts data from the corneal exam free-text EHR field.,
"BACKGROUND: Electronic medical records (EMRs) are revolutionizing health-related research. One key issue for study quality is the accurate identification of patients with the condition of interest. Information in EMRs can be entered as structured codes or unstructured free text. The majority of research studies have used only coded parts of EMRs for case-detection, which may bias findings, miss cases, and reduce study quality. This review examines whether incorporating information from text into case-detection algorithms can improve research quality. METHODS: A systematic search returned 9659 papers, 67 of which reported on the extraction of information from free text of EMRs with the stated purpose of detecting cases of a named clinical condition. Methods for extracting information from text and the technical accuracy of case-detection algorithms were reviewed. RESULTS: Studies mainly used US hospital-based EMRs, and extracted information from text for 41 conditions using keyword searches, rule-based algorithms, and machine learning methods. There was no clear difference in case-detection algorithm accuracy between rule-based and machine learning methods of extraction. Inclusion of information from text resulted in a significant improvement in algorithm sensitivity and area under the receiver operating characteristic in comparison to codes alone (median sensitivity 78% (codes + text) vs 62% (codes), P = .03; median area under the receiver operating characteristic 95% (codes + text) vs 88% (codes), P = .025). CONCLUSIONS: Text in EMRs is accessible, especially with open source information extraction algorithms, and significantly improves case detection when combined with codes. More harmonization of reporting within EMR studies is needed, particularly standardized reporting of algorithm accuracy metrics like positive predictive value (precision) and sensitivity (recall).",case detection | data quality | electronic health records | review | text mining
"BACKGROUND: Vancomycin, the most common antimicrobial used in US hospitals, can cause diverse adverse reactions, including hypersensitivity reactions (HSRs). Yet, little is known about vancomycin reactions documented in electronic health records. OBJECTIVE: To describe vancomycin HSR epidemiology from electronic health record allergy data. METHODS: This was a cross-sectional study of patients with 1 or more encounter from 2017 to 2019 and an electronic health record vancomycin drug allergy label (DAL) in 2 US health care systems. We determined prevalence and trends of vancomycin DALs and assessed active DALs by HSR phenotype determined from structured (coded) and unstructured (free-text) data using natural language processing. We investigated demographic associations with documentation of vancomycin red man syndrome (RMS). RESULTS: Among 4,490,618 patients, 14,426 (0.3%) had a vancomycin DAL with 18,761 documented reactions (2,248 [12.0%] free-text). Quarterly mean vancomycin DALs added were 253 ± 12 and deleted were 12 ± 2. Of 18,761 vancomycin HSRs, 7,903 (42.1%) were immediate phenotypes and 3,881 (20.7%) were delayed phenotypes. Common HSRs were rash (32% of HSRs) and RMS (16% of HSRs). Anaphylaxis was coded in 6% cases of HSRs. Drug reaction eosinophilia and systemic symptoms syndrome was the most common coded vancomycin severe cutaneous adverse reaction. RMS documentation was more likely for males (odds ratio, 1.30; 95% CI, 1.17-1.44) and less likely for blacks (odds ratio, 0.59; 95% CI, 0.47-0.75). CONCLUSIONS: Vancomycin causes diverse adverse reactions, including common (eg, RMS) and severe (eg, drug reaction eosinophilia and systemic symptoms syndrome) reactions entered as DAL free-text. Anaphylaxis comprised 6% of documented vancomycin HSRs, although true vancomycin IgE-mediated reactions are exceedingly rare. Improving vancomycin DAL documentation requires more coded entry options, including a coded entry for RMS.",Allergy | Drug allergy label | Drug reaction eosinophilia and systemic symptoms syndrome | Electronic health record | Epidemiology | Hypersensitivity | Infusion reaction | Phenotype | Red man syndrome | Vancomycin
"OBJECTIVE: Electronic health record (EHR) systems contain structured data (such as diagnostic codes) and unstructured data (clinical documentation). Clinical insights can be derived from analyzing both. The use of natural language processing (NLP) algorithms to effectively analyze unstructured data has been well demonstrated. Here we examine the utility of NLP for the identification of patients with non-alcoholic fatty liver disease, assess patterns of disease progression, and identify gaps in care related to breakdown in communication among providers. MATERIALS AND METHODS: All clinical notes available on the 38,575 patients enrolled in the Mount Sinai BioMe cohort were loaded into the NLP system. We compared analysis of structured and unstructured EHR data using NLP, free-text search, and diagnostic codes with validation against expert adjudication. We then used the NLP findings to measure physician impression of progression from early-stage NAFLD to NASH or cirrhosis. Similarly, we used the same NLP findings to identify mentions of NAFLD in radiology reports that did not persist into clinical notes. RESULTS: Out of 38,575 patients, we identified 2,281 patients with NAFLD. From the remainder, 10,653 patients with similar data density were selected as a control group. NLP outperformed ICD and text search in both sensitivity (NLP: 0.93, ICD: 0.28, text search: 0.81) and F2 score (NLP: 0.92, ICD: 0.34, text search: 0.81). Of 2281 NAFLD patients, 673 (29.5%) were believed to have progressed to NASH or cirrhosis. Among 176 where NAFLD was noted prior to NASH, the average progression time was 410 days. 619 (27.1%) NAFLD patients had it documented only in radiology notes and not acknowledged in other forms of clinical documentation. Of these, 170 (28.4%) were later identified as having likely developed NASH or cirrhosis after a median 1057.3 days. DISCUSSION: NLP-based approaches were more accurate at identifying NAFLD within the EHR than ICD/text search-based approaches. Suspected NAFLD on imaging is often not acknowledged in subsequent clinical documentation. Many such patients are later found to have more advanced liver disease. Analysis of information flows demonstrated loss of key information that could have been used to help prevent the progression of early NAFLD (NAFL) to NASH or cirrhosis. CONCLUSION: For identification of NAFLD, NLP performed better than alternative selection modalities. It then facilitated analysis of knowledge flow between physician and enabled the identification of breakdowns where key information was lost that could have slowed or prevented later disease progression.",NAFLD | Natural language processing | Patient safety
"Clinical trials often fail to recruit an adequate number of appropriate patients. Identifying eligible trial participants is resource-intensive when relying on manual review of clinical notes, particularly in critical care settings where the time window is short. Automated review of electronic health records (EHR) may help, but much of the information is in free text rather than a computable form. We applied natural language processing (NLP) to free text EHR data using the CogStack platform to simulate recruitment into the LeoPARDS study, a clinical trial aiming to reduce organ dysfunction in septic shock. We applied an algorithm to identify eligible patients using a moving 1-hour time window, and compared patients identified by our approach with those actually screened and recruited for the trial, for the time period that data were available. We manually reviewed records of a random sample of patients identified by the algorithm but not screened in the original trial. Our method identified 376 patients, including 34 patients with EHR data available who were actually recruited to LeoPARDS in our centre. The sensitivity of CogStack for identifying patients screened was 90% (95% CI 85%, 93%). Of the 203 patients identified by both manual screening and CogStack, the index date matched in 95 (47%) and CogStack was earlier in 94 (47%). In conclusion, analysis of EHR data using NLP could effectively replicate recruitment in a critical care trial, and identify some eligible patients at an earlier stage, potentially improving trial recruitment if implemented in real time.",
"OBJECTIVES: Natural language processing (NLP) and machine learning approaches were used to build classifiers to identify genomic-related treatment changes in the free-text visit progress notes of cancer patients. METHODS: We obtained 5889 deidentified progress reports (2439 words on average) for 755 cancer patients who have undergone a clinical next generation sequencing (NGS) testing in Wake Forest Baptist Comprehensive Cancer Center for our data analyses. An NLP system was implemented to process the free-text data and extract NGS-related information. Three types of recurrent neural network (RNN) namely, gated recurrent unit, long short-term memory (LSTM), and bidirectional LSTM (LSTM_Bi) were applied to classify documents to the treatment-change and no-treatment-change groups. Further, we compared the performances of RNNs to 5 machine learning algorithms including Naive Bayes, K-nearest Neighbor, Support Vector Machine for classification, Random forest, and Logistic Regression. RESULTS: Our results suggested that, overall, RNNs outperformed traditional machine learning algorithms, and LSTM_Bi showed the best performance among the RNNs in terms of accuracy, precision, recall, and F1 score. In addition, pretrained word embedding can improve the accuracy of LSTM by 3.4% and reduce the training time by more than 60%. DISCUSSION AND CONCLUSION: NLP and RNN-based text mining solutions have demonstrated advantages in information retrieval and document classification tasks for unstructured clinical progress notes.",cancer | electronic health records | genomics | machine learning | natural language processing
"BACKGROUND: Temporal relations between clinical events play an important role in clinical assessment and decision making. Extracting such relations from free text data is a challenging task because it lies on between medical natural language processing, temporal representation and temporal reasoning. OBJECTIVES: To survey existing methods for extracting temporal relations (TLINKs) between events from clinical free text in English; to establish the state-of-the-art in this field; and to identify outstanding methodological challenges. METHODS: A systematic search in PubMed and the DBLP computer science bibliography was conducted for studies published between January 2006 and December 2018. The relevant studies were identified by examining the titles and abstracts. Then, the full text of selected studies was analyzed in depth and information were collected on TLINK tasks, TLINK types, data sources, features selection, methods used, and reported performance. RESULTS: A total of 2834 publications were identified for title and abstract screening. Of these publications, 51 studies were selected. Thirty-two studies used machine learning approaches, 15 studies used a hybrid approaches, and only four studies used a rule-based approach. The majority of studies use publicly available corpora: THYME (28 studies) and the i2b2 corpus (17 studies). CONCLUSION: The performance of TLINK extraction methods ranges widely depending on relation types and events (e.g. from 32% to 87% F-score for identifying relations between clinical events and document creation time). A small set of TLINKs (before, after, overlap and contains) has been widely studied with relatively good performance, whereas other types of TLINK (e.g., started by, finished by, precedes) are rarely studied and remain challenging. Machine learning classifiers (such as Support Vector Machine and Conditional Random Fields) and Deep Neural Networks were among the best performing methods for extracting TLINKs, but nearly all the work has been carried out and tested on two publicly available corpora only. The field would benefit from the availability of more publicly available, high-quality, annotated clinical text corpora.",Clinical notes | Electronic health records | Natural language processing | Temporal information extraction | Temporal relation extraction | Text mining
"BACKGROUND: Financial codes are often used to extract diagnoses from electronic health records. This approach is prone to false positives. Alternatively, queries are constructed, but these are highly center and language specific. A tantalizing alternative is the automatic identification of patients by employing machine learning on format-free text entries. OBJECTIVE: The aim of this study was to develop an easily implementable workflow that builds a machine learning algorithm capable of accurately identifying patients with rheumatoid arthritis from format-free text fields in electronic health records. METHODS: Two electronic health record data sets were employed: Leiden (n=3000) and Erlangen (n=4771). Using a portion of the Leiden data (n=2000), we compared 6 different machine learning methods and a naïve word-matching algorithm using 10-fold cross-validation. Performances were compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC), and F1 score was used as the primary criterion for selecting the best method to build a classifying algorithm. We selected the optimal threshold of positive predictive value for case identification based on the output of the best method in the training data. This validation workflow was subsequently applied to a portion of the Erlangen data (n=4293). For testing, the best performing methods were applied to remaining data (Leiden n=1000; Erlangen n=478) for an unbiased evaluation. RESULTS: For the Leiden data set, the word-matching algorithm demonstrated mixed performance (AUROC 0.90; AUPRC 0.33; F1 score 0.55), and 4 methods significantly outperformed word-matching, with support vector machines performing best (AUROC 0.98; AUPRC 0.88; F1 score 0.83). Applying this support vector machine classifier to the test data resulted in a similarly high performance (F1 score 0.81; positive predictive value [PPV] 0.94), and with this method, we could identify 2873 patients with rheumatoid arthritis in less than 7 seconds out of the complete collection of 23,300 patients in the Leiden electronic health record system. For the Erlangen data set, gradient boosting performed best (AUROC 0.94; AUPRC 0.85; F1 score 0.82) in the training set, and applied to the test data, resulted once again in good results (F1 score 0.67; PPV 0.97). CONCLUSIONS: We demonstrate that machine learning methods can extract the records of patients with rheumatoid arthritis from electronic health record data with high precision, allowing research on very large populations for limited costs. Our approach is language and center independent and could be applied to any type of diagnosis. We have developed our pipeline into a universally applicable and easy-to-implement workflow to equip centers with their own high-performing algorithm. This allows the creation of observational studies of unprecedented size covering different countries for low cost from already available data in electronic health record systems.",Electronic Health Records | Gradient Boosting | Natural Language Processing | Rheumatoid Arthritis | Supervised machine learning | Support Vector Machine
"CONTEXT: Goals-of-care discussions are an important quality metric in palliative care. However, goals-of-care discussions are often documented as free text in diverse locations. It is difficult to identify these discussions in the electronic health record (EHR) efficiently. OBJECTIVES: To develop, train, and test an automated approach to identifying goals-of-care discussions in the EHR, using natural language processing (NLP) and machine learning (ML). METHODS: From the electronic health records of an academic health system, we collected a purposive sample of 3183 EHR notes (1435 inpatient notes and 1748 outpatient notes) from 1426 patients with serious illness over 2008-2016, and manually reviewed each note for documentation of goals-of-care discussions. Separately, we developed a program to identify notes containing documentation of goals-of-care discussions using NLP and supervised ML. We estimated the performance characteristics of the NLP/ML program across 100 pairs of randomly partitioned training and test sets. We repeated these methods for inpatient-only and outpatient-only subsets. RESULTS: Of 3183 notes, 689 contained documentation of goals-of-care discussions. The mean sensitivity of the NLP/ML program was 82.3% (SD 3.2%), and the mean specificity was 97.4% (SD 0.7%). NLP/ML results had a median positive likelihood ratio of 32.2 (IQR 27.5-39.2) and a median negative likelihood ratio of 0.18 (IQR 0.16-0.20). Performance was better in inpatient-only samples than outpatient-only samples. CONCLUSION: Using NLP and ML techniques, we developed a novel approach to identifying goals-of-care discussions in the EHR. NLP and ML represent a potential approach toward measuring goals-of-care discussions as a research outcome and quality metric.",Natural language processing | electronic health record | goals of care | machine learning | medical informatics | quality improvement
"BACKGROUND AND PURPOSE: Manual annotation and categorization of non-standardized text (""free-text"") of drug orders entered into electronic health records is a labor-intensive task. However, standardization is required for drug order analyses and has implications for clinical decision support. Machine learning could help to speed up manual labelling efforts. The objective of this study was to analyze the performance of deep machine learning methods to annotate non-standardized text of drug order entries with their therapeutically active ingredients. MATERIALS AND METHODS: The data consisted of drug orders entered 8/2009-4/2014 into the electronic health records of inpatients at a large tertiary care academic medical center. We manually annotated the most frequent order entry patterns with the active ingredient they contain (e.g. ""Prograf""⟵""Tacrolimus""). We heuristically included additional orders by means of character sequence comparisons to augment the training dataset. Finally, we trained and employed character-level recurrent deep neural networks to classify non-standardized text of drug order entries according to their active ingredients. RESULTS: A total of 26,611 distinct order patterns were considered in our study, of which the top 7.6% (2028) had been annotated with one of 558 distinct ingredients, leaving 24,583 unlabeled observations. Character-level recurrent deep neural networks achieved a Mean Reciprocal Rank (MRR) of 98% and outperformed the best representative baseline, a trigram-based Support Vector Machine, by 2 percentage points. CONCLUSION: Character-level recurrent deep neural networks can be used to map the active ingredient to non-standardized text of drug order entries, outperforming other representative techniques. While machine learning might help to facilitate categorization tasks, still a considerable amount of manual labelling and reviewing work is required to train such systems.",Character-level models | Drug ordering | Machine learning | Recurrent neural networks
"BACKGROUND: Fungal ocular involvement can develop in patients with fungal bloodstream infections and can be vision-threatening. Ocular involvement has become less common in the current era of improved antifungal therapies. Retrospectively determining the prevalence of fungal ocular involvement is important for informing clinical guidelines, such as the need for routine ophthalmologic consultations. However, manual retrospective record review to detect cases is time-consuming. OBJECTIVE: This study aimed to determine the prevalence of fungal ocular involvement in a critical care database using both structured and unstructured electronic health record (EHR) data. METHODS: We queried microbiology data from 46,467 critical care patients over 12 years (2000-2012) from the Medical Information Mart for Intensive Care III (MIMIC-III) to identify 265 patients with culture-proven fungemia. For each fungemic patient, demographic data, fungal species present in blood culture, and risk factors for fungemia (eg, presence of indwelling catheters, recent major surgery, diabetes, immunosuppressed status) were ascertained. All structured diagnosis codes and free-text narrative notes associated with each patient's hospitalization were also extracted. Screening for fungal endophthalmitis was performed using two approaches: (1) by querying a wide array of eye- and vision-related diagnosis codes, and (2) by utilizing a custom regular expression pipeline to identify and collate relevant text matches pertaining to fungal ocular involvement. Both approaches were validated using manual record review. The main outcome measure was the documentation of any fungal ocular involvement. RESULTS: In total, 265 patients had culture-proven fungemia, with Candida albicans (n=114, 43%) and Candida glabrata (n=74, 28%) being the most common fungal species in blood culture. The in-hospital mortality rate was 121 (46%). In total, 7 patients were identified as having eye- or vision-related diagnosis codes, none of whom had fungal endophthalmitis based on record review. There were 26,830 free-text narrative notes associated with these 265 patients. A regular expression pipeline based on relevant terms yielded possible matches in 683 notes from 108 patients. Subsequent manual record review again demonstrated that no patients had fungal ocular involvement. Therefore, the prevalence of fungal ocular involvement in this cohort was 0%. CONCLUSIONS: MIMIC-III contained no cases of ocular involvement among fungemic patients, consistent with prior studies reporting low rates of ocular involvement in fungemia. This study demonstrates an application of natural language processing to expedite the review of narrative notes. This approach is highly relevant for ophthalmology, where diagnoses are often based on physical examination findings that are documented within clinical notes.",diagnosis codes | electronic health records | fungal endophthalmitis | fungal ocular involvement | fungemia | natural language processing | regular expressions | unstructured data
"Handoff notes are increasingly integrated within electronic health record (EHR) systems and often contain data automatically generated from the EHR and free-text narratives. We examined the quality of data entered by providers in the free-text portion of our institutional EHR handoff tool. Overall, 65% of handoff notes contained at least one error (average 1.7 errors per note). Most errors were omissions in information around patient plan/management or assessment/diagnosis rather than entry of false data. Factors associated with increased error rate were increasing hospital day number; weekend note; medical (vs. surgical) service team; and authorship by a medical student, first or fourth year resident physician, or attending physician. Our findings suggest that errors are common in handoff notes, and while these errors are not completely false data, they may provide individuals caring for patients an inaccurate understanding of patient status.",Electronic Health Records | Patient Handoff | Patient Transfer
"PURPOSE: Increasingly, patient information is stored in electronic medical records, which could be reused for research. Often these records comprise unstructured narrative data, which are cumbersome to analyze. The authors investigated whether text mining can make these data suitable for epidemiological studies and compared a concept recognition approach and a range of machine learning techniques that require a manually annotated training set. The authors show how this training set can be created with minimal effort by using a broad database query. METHODS: The approaches were tested on two data sets: a publicly available set of English radiology reports for which International Classification of Diseases, Ninth Revision, Clinical Modification code needed to be assigned and a set of Dutch GP records that needed to be classified as either liver disorder cases or noncases. Performance was tested against a manually created gold standard. RESULTS: The best overall performance was achieved by a combination of a manually created filter for removing negations and speculations and rule learning algorithms such as RIPPER, with high scores on both the radiology reports (positive predictive value = 0.88, sensitivity = 0.85, specificity = 1.00) and the GP records (positive predictive value = 0.89, sensitivity =0.91, specificity =0.76). CONCLUSIONS: Although a training set still needs to be created manually, text mining can help reduce the amount of manual work needed to incorporate narrative data in an epidemiological study and will make the data extraction more reproducible. An advantage of machine learning is that it is able to pick up specific language use, such as abbreviations and synonyms used by physicians.",
"Electronic health records (EHRs) offer the opportunity to ascertain clinical outcomes at large scale and low cost, thus facilitating cohort studies, quality of care research and clinical trials. For acute myocardial infarction (AMI) the extent to which different EHR sources are accessible and accurate remains uncertain. Using MEDLINE and EMBASE we identified thirty three studies, reporting a total of 128658 patients, published between January 2000 and July 2014 that permitted assessment of the validity of AMI diagnosis drawn from EHR sources against a reference such as manual chart review. In contrast to clinical practice, only one study used EHR-derived markers of myocardial necrosis to identify possible AMI cases, none used electrocardiogram findings and one used symptoms in the form of free text combined with coded diagnosis. The remaining studies relied mostly on coded diagnosis. Thirty one studies reported positive predictive value (PPV)≥ 70% between AMI diagnosis from both secondary care and primary care EHRs and the reference. Among fifteen studies reporting EHR-derived AMI phenotypes, three cross-referenced ST-segment elevation AMI diagnosis (PPV range 71-100%), two non-ST-segment elevation AMI (PPV 91.0, 92.1%), three non-fatal AMI (PPV range 82-92.2%) and six fatal AMI (PPV range 64-91.7%). Clinical coding of EHR-derived AMI diagnosis in primary care and secondary care was found to be accurate in different clinical settings and for different phenotypes. However, markers of myocardial necrosis, ECG and symptoms, the cornerstones of a clinical diagnosis, are underutilised and remain a challenge to retrieve from EHRs.",Acute coronary syndrome | Clinical coding | Electronic health records | Myocardial infarction | Phenotype | Validation studies
"BACKGROUND: Electronic health record (EHR) systems have been widely adopted in hospitals. However, since current EHRs mainly focus on lowering the number of paper documents used, they have suffered from poor search function and reusability capabilities. To overcome these drawbacks, structured clinical templates have been proposed; however, they are not widely used owing to the inconvenience of data entry. OBJECTIVE: This study aims to verify the usability of structured templates by comparing data entry times. METHODS: A Korean tertiary hospital has implemented structured clinical templates with the modeling of clinical contents for the last 6 years. As a result, 1238 clinical content models (ie, body measurements, vital signs, and allergies) have been developed and 492 models for 13 clinical templates, including pathology reports, were applied to EHRs for clinical practice. Then, to verify the usability of the structured templates, data entry times from free-texts and four structured pathology report templates were compared using 4391 entries from structured data entry (SDE) log data and 4265 entries from free-text log data. In addition, a paper-based survey and a focus group interview were conducted with 23 participants from three different groups, including EHR developers, pathology transcriptionists, and clinical data extraction team members. RESULTS: Based on the analysis of time required for data entry, in most cases, beginner users of the structured clinical templates required at most 70.18% more time for data entry. However, as users became accustomed to the templates, they were able to enter data more quickly than via free-text entry: at least 1 minute and 23 seconds (16.8%) up to 5 minutes and 42 seconds (27.6%). Interestingly, well-designed thyroid cancer pathology reports required 14.54% less data entry time from the beginning of the SDE implementation. In the interviews and survey, we confirmed that most of the interviewees agreed on the need for structured templates. However, they were skeptical about structuring all the items included in the templates. CONCLUSIONS: The increase in initial elapsed time led users to hold a negative opinion of SDE, despite its benefits. To overcome these obstacles, it is necessary to structure the clinical templates for optimum use. In addition, user experience in terms of ease of data entry must be considered as an essential aspect in the development of structured clinical templates.",data entry time | electronic health records | structured clinical template | structured data entry | user experience
"PURPOSE: Previous studies have demonstrated differences in atrial fibrillation (AF) detection based on data from hospital sources without data from outpatient sources. We investigated the detection of documented diagnoses of non-valvular AF in a large Israeli health-care organization using electronic health record data from multiple sources. PATIENTS AND METHODS: This was an open-chart validation study. Three distinct algorithms for identifying AF in electronic health records, differing in the source of their International Classification of Diseases, Ninth Revision code and use of the associated free text, were defined. Algorithm 1 incorporated inpatient data with outpatient data and the associated free text. Algorithm 2 incorporated inpatient and outpatient data regardless of the free text associated with AF diagnosis. Algorithm 3 used only inpatient data source. These algorithms were compared to a gold standard and their sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were calculated. To establish the gold standard (documentation of arrhythmia based on electrocardiography interpretation or a cardiologist's written diagnosis), 200 patients at highest risk for having non-valvular AF were randomly selected for open-chart validation by two physicians. RESULTS: The algorithm that included hospital settings, outpatient settings, and incorporated associated free text in the outpatient records had the optimal balance between all validation measures, with a high level of sensitivity (85.4%), specificity (95.0%), PPV (81.4%), and NPV (96.2%). The alternative algorithm that combined inpatient and outpatient data without free text also performed better than the algorithm that included only hospital data (82.9%, 95.0%, 81.0%, and 95.6%, compared to 70.7%, 96.9%, 85.3%, and 92.8%, sensitivity, specificity, PPV, and NPV, respectively). CONCLUSION: In this study, involving a comprehensive data collection from inpatient and outpatient sources, incorporating outpatient data with inpatient data improved the diagnosis of non-valvular AF compared to inpatient data alone.",atrial fibrillation | electronic health records | validation
"BACKGROUND: Free text in electronic health records (EHR) may contain additional phenotypic information beyond structured (coded) information. For major health events - heart attack and death - there is a lack of studies evaluating the extent to which free text in the primary care record might add information. Our objectives were to describe the contribution of free text in primary care to the recording of information about myocardial infarction (MI), including subtype, left ventricular function, laboratory results and symptoms; and recording of cause of death. We used the CALIBER EHR research platform which contains primary care data from the Clinical Practice Research Datalink (CPRD) linked to hospital admission data, the MINAP registry of acute coronary syndromes and the death registry. In CALIBER we randomly selected 2000 patients with MI and 1800 deaths. We implemented a rule-based natural language engine, the Freetext Matching Algorithm, on site at CPRD to analyse free text in the primary care record without raw data being released to researchers. We analysed text recorded within 90 days before or 90 days after the MI, and on or after the date of death. RESULTS: We extracted 10,927 diagnoses, 3658 test results, 3313 statements of negation, and 850 suspected diagnoses from the myocardial infarction patients. Inclusion of free text increased the recorded proportion of patients with chest pain in the week prior to MI from 19 to 27%, and differentiated between MI subtypes in a quarter more patients than structured data alone. Cause of death was incompletely recorded in primary care; in 36% the cause was in coded data and in 21% it was in free text. Only 47% of patients had exactly the same cause of death in primary care and the death registry, but this did not differ between coded and free text causes of death. CONCLUSIONS: Among patients who suffer MI or die, unstructured free text in primary care records contains much information that is potentially useful for research such as symptoms, investigation results and specific diagnoses. Access to large scale unstructured data in electronic health records (millions of patients) might yield important insights.",Chest pain | Free text | Myocardial infarction | Natural language processing | Primary care
"OBJECTIVES: We set out to develop, evaluate and implement a novel application using natural language processing to text mine occupations from the free-text of psychiatric clinical notes. DESIGN: Development and validation of a natural language processing application using General Architecture for Text Engineering software to extract occupations from de-identified clinical records. SETTING AND PARTICIPANTS: Electronic health records from a large secondary mental healthcare provider in south London, accessed through the Clinical Record Interactive Search platform. The text mining application was run over the free-text fields in the electronic health records of 341 720 patients (all aged ≥16 years). OUTCOMES: Precision and recall estimates of the application performance; occupation retrieval using the application compared with structured fields; most common patient occupations; and analysis of key sociodemographic and clinical indicators for occupation recording. RESULTS: Using the structured fields alone, only 14% of patients had occupation recorded. By implementing the text mining application in addition to the structured fields, occupations were identified in 57% of patients. The application performed on gold-standard human-annotated clinical text at a precision level of 0.79 and recall level of 0.77. The most common patient occupations recorded were 'student' and 'unemployed'. Patients with more service contact were more likely to have an occupation recorded, as were patients of a male gender, older age and those living in areas of lower deprivation. CONCLUSION: This is the first time a natural language processing application has been used to successfully derive patient-level occupations from the free-text of electronic mental health records, performing with good levels of precision and recall, and applied at scale. This may be used to inform clinical studies relating to the broader social determinants of health using electronic health records.",adult psychiatry | epidemiology | health informatics | mental health
"BACKGROUND: Duration of untreated psychosis (DUP) is an important clinical construct in the field of mental health, as longer DUP can be associated with worse intervention outcomes. DUP estimation requires knowledge about when psychosis symptoms first started (symptom onset), and when psychosis treatment was initiated. Electronic health records (EHRs) represent a useful resource for retrospective clinical studies on DUP, but the core information underlying this construct is most likely to lie in free text, meaning it is not readily available for clinical research. Natural Language Processing (NLP) is a means to addressing this problem by automatically extracting relevant information in a structured form. As a first step, it is important to identify appropriate documents, i.e., those that are likely to include the information of interest. Next, temporal information extraction methods are needed to identify time references for early psychosis symptoms. This NLP challenge requires solving three different tasks: time expression extraction, symptom extraction, and temporal ""linking"". In this study, we focus on the first step, using two relevant EHR datasets. RESULTS: We applied a rule-based NLP system for time expression extraction that we had previously adapted to a corpus of mental health EHRs from patients with a diagnosis of schizophrenia (first referrals). We extended this work by applying this NLP system to a larger set of documents and patients, to identify additional texts that would be relevant for our long-term goal, and developed a new corpus from a subset of these new texts (early intervention services). Furthermore, we added normalized value annotations (""2011-05"") to the annotated time expressions (""May 2011"") in both corpora. The finalized corpora were used for further NLP development and evaluation, with promising results (normalization accuracy 71-86%). To highlight the specificities of our annotation task, we also applied the final adapted NLP system to a different temporally annotated clinical corpus. CONCLUSIONS: Developing domain-specific methods is crucial to address complex NLP tasks such as symptom onset extraction and retrospective calculation of duration of a preclinical syndrome. To the best of our knowledge, this is the first clinical text resource annotated for temporal entities in the mental health domain.",Electronic health records | Mental health | Natural language processing | Schizophrenia | Temporal information extraction
"BACKGROUND: Geriatric syndromes in older adults are associated with adverse outcomes. However, despite being reported in clinical notes, these syndromes are often poorly captured by diagnostic codes in the structured fields of electronic health records (EHRs) or administrative records. OBJECTIVE: We aim to automatically determine if a patient has any geriatric syndromes by mining the free text of associated EHR clinical notes. We assessed which statistical natural language processing (NLP) techniques are most effective. METHODS: We applied conditional random fields (CRFs), a widely used machine learning algorithm, to identify each of 10 geriatric syndrome constructs in a clinical note. We assessed three sets of features and attributes for CRF operations: a base set, enhanced token, and contextual features. We trained the CRF on 3901 manually annotated notes from 85 patients, tuned the CRF on a validation set of 50 patients, and evaluated it on 50 held-out test patients. These notes were from a group of US Medicare patients over 65 years of age enrolled in a Medicare Advantage Health Maintenance Organization and cared for by a large group practice in Massachusetts. RESULTS: A final feature set was formed through comprehensive feature ablation experiments. The final CRF model performed well at patient-level determination (macroaverage F1=0.834, microaverage F1=0.851); however, performance varied by construct. For example, at phrase-partial evaluation, the CRF model worked well on constructs such as absence of fecal control (F1=0.857) and vision impairment (F1=0.798) but poorly on malnutrition (F1=0.155), weight loss (F1=0.394), and severe urinary control issues (F1=0.532). Errors were primarily due to previously unobserved words (ie, out-of-vocabulary) and a lack of context. CONCLUSIONS: This study shows that statistical NLP can be used to identify geriatric syndromes from EHR-extracted clinical notes. This creates new opportunities to identify patients with geriatric syndromes and study their health outcomes.",clinical notes | conditional random fields | geriatrics | information extraction | natural language processing
"BACKGROUND: Delirium is an acute confusional state, associated with morbidity and mortality in diverse medically ill populations. Delirium is preventable and treatable when diagnosed but the diagnosis is often missed. This important and difficult diagnosis is an attractive candidate for computer-aided decision support if it can be reliably identified at scale. OBJECTIVE: Here, using an electronic health record-based case definition of delirium, we characterize incidence of this highly morbid condition in 2 large academic medical centers. METHODS: Using the electronic health record of 2 large New England academic medical centers, we calculated and compared the rate of the diagnosis of delirium using a range of administrative and discharge summary text-based case definitions over an 8-year period. RESULTS: Depending on case definitions, the overall delirium rate ranged from 2.0-5.4% of 809,512 admissions identified. The identified rate of delirium increased between 2005 and 2013, such that by the final year of the study, one of the two sites reported delirium in 7.0% of cases. The concordance between case definitions was low; only half of the cases identified by text analysis were captured by administrative data. CONCLUSION: Delirium may be better captured by composite outcomes, including both administrative claims data and elements drawn from unstructured data sources. That the rate of delirium observed in this study is far lower than the current literature estimates suggests that further work on case definitions, identification, and documented diagnosis is required.",delirium | electronic health records | epidemiology | predictive modeling.
"BACKGROUND: Patient Priorities Care (PPC) is a model of care that aligns health care recommendations with priorities of older adults who have multiple chronic conditions. Following identification of patient priorities, this information is documented in the patient's electronic health record (EHR). OBJECTIVE: Our goal is to develop and validate a natural language processing (NLP) model that reliably documents when clinicians identify patient priorities (ie, values, outcome goals, and care preferences) within the EHR as a measure of PPC adoption. METHODS: This is a retrospective analysis of unstructured National Veteran Health Administration EHR free-text notes using an NLP model. The data were sourced from 778 patient notes of 658 patients from encounters with 144 social workers in the primary care setting. Each patient's free-text clinical note was reviewed by 2 independent reviewers for the presence of PPC language such as priorities, values, and goals. We developed an NLP model that utilized statistical machine learning approaches. The performance of the NLP model in training and validation with 10-fold cross-validation is reported via accuracy, recall, and precision in comparison to the chart review. RESULTS: Of 778 notes, 589 (75.7%) were identified as containing PPC language (kappa=0.82, P<.001). The NLP model in the training stage had an accuracy of 0.98 (95% CI 0.98-0.99), a recall of 0.98 (95% CI 0.98-0.99), and precision of 0.98 (95% CI 0.97-1.00). The NLP model in the validation stage had an accuracy of 0.92 (95% CI 0.90-0.94), recall of 0.84 (95% CI 0.79-0.89), and precision of 0.84 (95% CI 0.77-0.91). In contrast, an approach using simple search terms for PPC only had a precision of 0.757. CONCLUSIONS: An automated NLP model can reliably measure with high precision, recall, and accuracy when clinicians document patient priorities as a key step in the adoption of PPC.",NLP | decision support | geriatric decision support system | machine learning | natural language processing | pattern recognition | social work note
"Electronic health records (EHRs) have the potential to improve health-care quality by allowing providers to make better decisions at the point of care based on electronically aggregated data and by facilitating clinical research. These goals are easier to achieve when key, disease-specific clinical information is documented as structured data elements (SDEs) that computers can understand and process, rather than as free-text/natural-language narrative. This article reviews the benefits of capturing disease-specific SDEs. It highlights several design and implementation considerations, including the impact on efficiency and expressivity of clinical documentation and the importance of adhering to data standards when available. Pulmonary disease-specific examples of collection instruments are provided from two commonly used commercial EHRs. Future developments that can leverage SDEs to improve clinical quality and research are discussed.",
"INTRODUCTION: Previous biomedical studies identified many lifestyle exposures that could possibly represent risk factors for dementia in general or dementia due to Alzheimer's disease (AD). These lifestyle exposures are mainly mentioned in free-text electronic health records (EHRs). However, automatic extraction and assessment of these exposures using EHRs remains understudied. METHODS: A natural language processing (NLP) approach was adopted to extract lifestyle exposures and intervention strategies from the clinical notes of 260 patients with clinical diagnoses of AD dementia and 260 age-matched cognitively unimpaired persons. Statistics of lifestyle exposures were compared between these two groups. The mapping results of the NLP extraction were evaluated by comparing the results with data captured independently by clinicians. RESULTS: Thirty out of fifty-five potentially relevant lifestyle exposures were mentioned in our clinical note dataset. Twenty-two dietary factors and three substance abuses that were potentially relevant were not found in clinical notes. Patients with AD dementia were significantly exposed to more of the potential risk factors compared to the cognitively unimpaired subjects (χ2 = 120.31, p-value < 0.001). The average accuracy of the automated extraction was 74.0% in comparison with the manual review of randomly selected 50 sample documents. DISCUSSION AND CONCLUSION: We illustrated the feasibility of NLP techniques for the automated evaluation of a large number lifestyle habits using free-text EHR data. We found that AD dementia patients were exposed to more of the potential risk factors than the comparison group. Our results also demonstrated the feasibility and accuracy of investigating putative risk factors using NLP techniques.",Alzheimer’s disease | Electronic health records | Lifestyle exposure | Natural language processing
"PURPOSE: The widespread adoption of electronic health records (EHRs) is creating rich databases documenting the cancer patient's care continuum. However, much of this data, especially narrative ""oncologic histories,"" are ""locked"" within free text (unstructured) portions of notes. Nationwide incentives, ranging from certification (Quality Oncology Practice Initiative) to monetary reimbursement (the Health Information Technology for Economic and Clinical Health Act), increasingly require the translation of these histories into treatment summaries for patient use and into tools to assist in transitions of care. Unfortunately, formulation of treatment summaries from these data is difficult and time-consuming. The rapidly developing field of automated natural language processing may offer a solution to this communication problem. METHODS: We surveyed a cross section of providers at Beth Israel Deaconess Medical Center regarding the importance of treatment summaries and whether these were being formulated on a regular basis. We also developed a program for the Informatics for Integrating Biology and the Bedside challenge, which was designed to extract meaningful information from EHRs. The program was then applied to a sample of narrative oncologic histories. RESULTS: The majority of providers (86%) felt that treatment summaries were important, but only 11% actually implemented them. The most common obstacles identified were lack of time and lack of EHR tools. We demonstrated that relevant medical concepts can be automatically extracted from oncologic histories with reasonable accuracy and precision. CONCLUSION: Natural language processing technology offers a promising method for structuring a free-text oncologic history into a compact treatment summary, creating a robust and accurate means of communication between providers and between provider and patient.",
"Electronic health records (EHRs) contain rich documentation regarding disease symptoms and progression, but EHR data is challenging to use for diagnosis prediction due to its high dimensionality, relative scarcity, and substantial level of noise. We investigated how to best represent EHR data for predicting cervical cancer, a serious disease where early detection is beneficial for the outcome of treatment. A case group of 1321 patients with cervical cancer were matched to ten times as many controls, and for both groups several types of events were extracted from their EHRs. These events included clinical codes, lab results, and contents of free text notes retrieved using a LSTM neural network. Clinical events are described with great variation in EHR texts, leading to a very large feature space. Therefore, an event hierarchy inferred from the textual events was created to represent the clinical texts. Overall, the events extracted from free text notes contributed the most to the final prediction, and the hierarchy of textual events further improved performance. Four classifiers were evaluated for predicting a future cancer diagnosis where Random Forest achieved the best results with an AUC of 0.70 from a year before diagnosis up to 0.97 one day before diagnosis. We conclude that our approach is sound and had excellent discrimination at diagnosis, but only modest discrimination capacity before this point. Since our study objective was earlier disease prediction than such, we propose further work should consider extending patient histories through e.g. the integration of primary health records preceding referral to hospital.",
"Electronic health records contain valuable information on patients' clinical history in the form of free text. Manually analyzing millions of these documents is unfeasible and automatic natural language processing methods are essential for efficiently exploiting these data. Within this, normalization of clinical entities, where the aim is to link entity mentions to reference vocabularies, is of utmost importance to successfully extract knowledge from clinical narratives. In this paper we present sieve-based models combined with heuristics and word embeddings and present results of our participation in the 2019 n2c2 (National NLP Clinical Challenges) shared-task on clinical concept normalization.",clinical concept disambiguation | clinical information extraction | natural language processing | sieve-based model | word embeddings
"For patients with a diagnosis of schizophrenia, determining symptom onset is crucial for timely and successful intervention. In mental health records, information about early symptoms is often documented only in free text, and thus needs to be extracted to support clinical research. To achieve this, natural language processing (NLP) methods can be used. Development and evaluation of NLP systems requires manually annotated corpora. We present a corpus of mental health records annotated with temporal relations for psychosis symptoms. We propose a methodology for document selection and manual annotation to detect symptom onset information, and develop an annotated corpus. To assess the utility of the created corpus, we propose a pilot NLP system. To the best of our knowledge, this is the first temporally-annotated corpus tailored to a specific clinical use-case.",Electronic Health Records | Natural Language Processing | Schizophrenia
"INTRODUCTION: Trauma injury severity scores are currently calculated retrospectively from the electronic health record (EHR) using manual annotation by certified trauma coders. Natural language processing (NLP) of clinical documents in the EHR may enable automated injury scoring. We hypothesize that NLP with machine learning can discriminate between cases of severe and non-severe injury to the thorax after trauma. METHODS: Clinical documents from a trauma center were examined between 2014 and 2018. Severe chest injury was defined as a thorax abbreviated injury score (AIS) >2 and served as the reference standard for supervised learning. Free text unigrams and concept unique identifiers (CUIs) from the Unified Medical Language Systems (UMLS) were extracted from clinical documents collected at one hour, four hours, and eight hours after patient arrival to the emergency department. Logistic regression models with elastic net regularization were tuned to maximize area under the receiver operating characteristic curve (AUROC) using 10-fold cross-validation on the training dataset (80%) and tested on a hold-out 20% dataset. RESULTS: There were 6,891 traumas that met inclusion criteria. The complete data corpus consisted of 473,694 documents. Models trained using the first hour of data had a mean AUROC of 0.88 (95%CI [0.86, 0.89]); model discrimination and reclassification from the first hour significantly improved after eight hours with a mean AUROC of 0.94 (95%CI [0.93, 0.95]). Performance of models using CUIs were similar to unigrams (p>0.05). Models demonstrated excellent clinical face validity. CONCLUSIONS: Both CUIs and unigrams demonstrated excellent discrimination in predicting severity of chest injury using the first eight hours of clinical documents. Our model demonstrates that automated anatomical injury scoring is feasible and may be used for aggregation of data for trauma research and quality programs.",Machine learning | Natural language processing | Trauma | Trauma registry
"The digitalization of health and medicine and the growing availability of electronic health records (EHRs) has encouraged healthcare professionals and clinical researchers to adopt cutting-edge methodologies in the realms of artificial intelligence (AI) and big data analytics to exploit existing large medical databases. In Hospital and Health System pharmacies, the application of natural language processing (NLP) and machine learning to access and analyze the unstructured, free-text information captured in millions of EHRs (e.g., medication safety, patients' medication history, adverse drug reactions, interactions, medication errors, therapeutic outcomes, and pharmacokinetic consultations) may become an essential tool to improve patient care and perform real-time evaluations of the efficacy, safety, and comparative effectiveness of available drugs. This approach has an enormous potential to support share-risk agreements and guide decision-making in pharmacy and therapeutics (P&T) Committees.",Electronic health records | Machine learning | Natural language processing | Pharmacovigilance
"OBJECTIVES: To create test collections for evaluating clinical information retrieval (IR) systems and advancing clinical IR research. MATERIALS AND METHODS: Electronic health record (EHR) data, including structured and free-text data, from 45 000 patients who are a part of the Mayo Clinic Biobank cohort was retrieved from the clinical data warehouse. The clinical IR system indexed a total of 42 million free-text EHR documents. The search queries consisted of 56 topics developed through a collaboration between Mayo Clinic and Oregon Health & Science University. We described the creation of test collections, including a to-be-evaluated document pool using five retrieval models, and human assessment guidelines. We analyzed the relevance judgment results in terms of human agreement and time spent, and results of three levels of relevance, and reported performance of five retrieval models. RESULTS: The two judges had a moderate overall agreement with a Kappa value of 0.49, spent a consistent amount of time judging the relevance, and were able to identify easy and difficult topics. The conventional retrieval model performed best on most topics while a concept-based retrieval model had better performance on the topics requiring conceptual level retrieval. DISCUSSION: IR can provide an alternate approach to leveraging clinical narratives for patient information discovery as it is less dependent on semantics. Our study showed the feasibility of test collections along with a few challenges. CONCLUSION: The conventional test collections for evaluating the IR system show potential for successfully evaluating clinical IR systems with a few challenges to be investigated.",electronic health records | evaluation | information retrieval | relevance judgment | test collections
"PURPOSE: To describe and appraise contrast agent allergy documentation in the electronic health record (EHR). METHODS: We systematically identified medical imaging drugs and class terms in an integrated EHR allergy repository for patients seen at a large health care system between 2000 and 2013. Structured and free-text contrast allergy records were normalized and categorized by inciting agent and nature of adverse reaction. Allergen records were evaluated by their level of specificity. Reaction records were evaluated by whether the reaction was known or unknown and whether known reactions would be categorized as allergic-like or physiologic. RESULTS: Among 2.7 million patients, we identified 36,144 patients (1.3%) with at least one of 40,669 contrast allergy records associated with 49,000 reactions. Contrast allergens were more likely than other allergens to be entered as free text (15.2% versus 6.3%; odds ratio 2.69, 95% confidence interval 2.61-2.76). There were 1,305 unique contrast allergen records, which we grouped into 141 concepts. Most contrast allergen records were ambiguous contrast concepts (69.1%), rather than imaging modality-specific class terms (19.4%) or specific contrast agents (11.5%). Contrast reactions were occasionally entered as free text (24.8%), which together with structured entries were grouped into 183 concepts. A known reaction was documented in 71.8% of cases; however, 12.2% were non-allergic-like reactions. CONCLUSION: Contrast allergy records in EHRs are diverse and commonly low quality. Continued EHR enhancements and training are needed to support contrast allergy documentation to facilitate improved patient care and medical research.",Contrast media | drug hypersensitivity | electronic health records | information storage and retrieval | natural language processing
"BACKGROUND: A significant amount of clinical information captured as free-text narratives could be better used for several applications, such as clinical decision support, ontology development, evidence-based practice, and research. The Human Phenotype Ontology (HPO) is specifically used for semantic comparisons for diagnostic purposes. All these functions require quality coverage of the domain of interest. The authors used natural language processing to capture craniofacial and oral phenotype signatures from electronic health records and then used these signatures for evaluation of existing oral phenotype ontology coverage. METHODS: The authors applied a text-processing pipeline based on the clinical Text Analysis and Knowledge Extraction System to annotate the clinical notes with Unified Medical Language System codes. The authors extracted the disease or disorder phenotype terms, which were then compared with HPO terms and their synonyms. RESULTS: The authors retrieved 2,153 deidentified clinical notes from 558 patients. Finally, 2,416 unique diseases or disorders phenotype terms were extracted, which included 210 craniofacial or oral phenotype terms. Twenty-six of these phenotypes were not found in the HPO. CONCLUSIONS: The authors demonstrated that natural language processing tools could extract relevant phenotype terms from clinical narratives, which could help identify gaps in existing ontologies and enhance craniofacial and dental phenotyping vocabularies. PRACTICAL IMPLICATIONS: The expansion of terms in the dental, oral, and craniofacial domains in the HPO is particularly important as the dental community moves toward electronic health records.",Natural language processing | craniofacial and oral phenotypes | evidence-based dentistry | ontology
"A natural language challenge devised by Informatics for Integrating Biology and the Bedside (i2b2) was to analyze free-text health data to construct a multi-class, multi-label classification system focused on obesity and its co-morbidities. This report presents a case study in which a natural language processing (NLP) toolkit, called NLTK, was used in the challenge. This report provides a brief review of NLP in the context of EHR applications, briefly surveys and contrasts some existing NLP toolkits, and reports on our experiences with the i2b2 case study. Our efforts uncovered issues including the lack of human annotated physician notes for use as NLP training data, differences between conventional free-text and medical notes, and potential hardware and software limitations affecting future projects.",
"OBJECTIVE: Instruments rating risk of harm to self and others are widely used in inpatient forensic psychiatry settings. A potential alternate or supplementary means of risk prediction is from the automated analysis of case notes in Electronic Health Records (EHRs) using Natural Language Processing (NLP). This exploratory study rated presence or absence and frequency of words in a forensic EHR dataset, comparing four reference dictionaries. Seven machine learning algorithms and different time periods of EHR analysis were used to probe which dictionary and which time period were most predictive of risk assessment scores on validated instruments. MATERIALS AND METHODS: The EHR dataset comprised de-identified forensic inpatient notes from the Wilfred Lopes Centre in Tasmania. The data comprised unstructured free-text case note entries and serial ratings of three risk assessment scales: Historical Clinical Risk Management-20 (HCR-20), Short-Term Assessment of Risk and Treatability (START) and Dynamic Appraisal of Situational Aggression (DASA). Four NLP dictionary word lists were selected: 6865 mental health symptom words from the Unified Medical Language System (UMLS), 455 DSM-IV diagnoses from UMLS repository, 6790 English positive and negative sentiment words, and 1837 high frequency words from the Corpus of Contemporary American English (COCA). Seven machine learning methods Bagging, J48, Jrip, Logistic Model Trees (LMT), Logistic Regression, Linear Regression and Support Vector Machine (SVM) were used to identify the combination of dictionaries and algorithms that best predicted risk assessment scores. RESULTS: The most accurate prediction was attained on the DASA dataset using the sentiment dictionary and the LMT and SVM algorithms. CONCLUSIONS: NLP, used in conjunction with NLP dictionaries and machine learning, predicted risk ratings on the HCR-20, START, and DASA, based on EHR content. Further research is required to ascertain the utility of NLP approaches in predicting endpoints of actual self-harm, harm to others or victimisation.",Electronic health record | Mental health | Natural language processing | Psychiatry | Text mining
"This paper describes a paraphrasing approach to improve the performance of question answering (QA) for electronic health records (EHRs). QA systems for structured EHR data usually rely on semantic parsing, which aims to generate machine-understandable logical forms from free-text questions. Training semantic parsers requires large datasets of question-logical form (QL) pairs, which are labor-intensive to create. Considering the scarcity of large QL datasets in the clinical domain, we propose a framework for expanding an existing dataset using paraphrasing. We experiment with different heuristics for multiple sample sizes and iterations to assess the effect of adding paraphrasing to the task of semantic parsing. We found that adding paraphrases to an existing dataset based on TERTHRESHOLD scores results in an improved performance in the majority (74%) of the experimental runs. Hence, the proposed paraphrasing-based framework has the potential to improve the performance of QA systems using a limited set of existing QL annotations.",
"The pointwise mutual information statistic (PMI), which measures how often two words occur together in a document corpus, is a cornerstone of recently proposed popular natural language processing algorithms such as word2vec. PMI and word2vec reveal semantic relationships between words and can be helpful in a range of applications such as document indexing, topic analysis, or document categorization. We use probability theory to demonstrate the relationship between PMI and word2vec. We use the theoretical results to demonstrate how the PMI can be modeled and estimated in a simple and straight forward manner. We further describe how one can obtain standard error estimates that account for within-patient clustering that arises from patterns of repeated words within a patient's health record due to a unique health history. We then demonstrate the usefulness of PMI on the problem of predictive identification of disease from free text notes of electronic health records. Specifically, we use our methods to distinguish those with and without type 2 diabetes mellitus in electronic health record free text data using over 400 000 clinical notes from an academic medical center.",cluster-corrected standard errors | electronic health records | natural language processing | probability models | word2vec
"OBJECTIVE: Comprehensive analysis of ophthalmic surgical outcomes is often restricted by limited methodologies for efficiently and accurately extracting clinical information from electronic health record (EHR) systems because much is in free-text form. This study aims to utilize advanced methods to automate extraction of clinical concepts from the EHR free text to study visual acuity (VA), intraocular pressure (IOP), and medication outcomes of cataract and glaucoma surgeries. METHODS: Patients who underwent cataract or glaucoma surgery at an academic medical center between 2009 and 2018 were identified by Current Procedural Terminology codes. Rule-based algorithms were developed and used on EHR clinical narrative text to extract intraocular lens (IOL) power and implant type, as well as to create a surgery laterality classifier. MedEx (version 1.3.7) was used on free-text clinical notes to extract information on eye medications and compared to information from medication orders. Random samples of free-text notes were reviewed by two independent masked annotators to assess inter-annotator agreement on outcome variable classification and accuracy of classifiers. VA and IOP were available from semi-structured fields. RESULTS: This study cohort included 6347 unique patients, with 8550 stand-alone cataract surgeries, 451 combined cataract/glaucoma surgeries, and 961 glaucoma surgeries without concurrent cataract surgery. The rule-based laterality classifier achieved 100% accuracy compared to manual review of a sample of operative notes by independent masked annotators. For cataract surgery alone, glaucoma surgery alone, or combined cataract/glaucoma surgeries, our automated extraction algorithm achieved 99-100% accuracy compared to manual annotation of samples of notes from each group, including IOL model and IOL power for cataract surgeries, and glaucoma implant for glaucoma surgeries. For glaucoma medications, there was 90.7% inter-annotator agreement. After adjudication, 85.0% of medications identified by MedEx determined to be correct. Determination of surgical laterality enabled evaluation of pre- and postoperative VA and IOP for operative eyes. CONCLUSION: This text-processing pipeline can accurately capture surgical laterality and implant model usage from free-text operative notes of cataract and glaucoma surgeries, enabling extraction of clinical outcomes including visual acuities, intraocular pressure, and medications from the EHR system. Use of this approach with EHRs to assess ophthalmic surgical outcomes can benefit research groups interested in studying the safety and clinical efficacies of different surgical approaches.",Cataract surgery | Electronic health record | Glaucoma surgery | Natural language processing | Ophthalmology
"BACKGROUND: Electronic health care records (EHRs) are a rich source of health-related information, with potential for secondary research use. In the United Kingdom, there is no national marker for identifying those who have previously served in the Armed Forces, making analysis of the health and well-being of veterans using EHRs difficult. OBJECTIVE: This study aimed to develop a tool to identify veterans from free-text clinical documents recorded in a psychiatric EHR database. METHODS: Veterans were manually identified using the South London and Maudsley (SLaM) Biomedical Research Centre Clinical Record Interactive Search-a database holding secondary mental health care electronic records for the SLaM National Health Service Foundation Trust. An iterative approach was taken; first, a structured query language (SQL) method was developed, which was then refined using natural language processing and machine learning to create the Military Service Identification Tool (MSIT) to identify if a patient was a civilian or veteran. Performance, defined as correct classification of veterans compared with incorrect classification, was measured using positive predictive value, negative predictive value, sensitivity, F1 score, and accuracy (otherwise termed Youden Index). RESULTS: A gold standard dataset of 6672 free-text clinical documents was manually annotated by human coders. Of these documents, 66.00% (4470/6672) were then used to train the SQL and MSIT approaches and 34.00% (2202/6672) were used for testing the approaches. To develop the MSIT, an iterative 2-stage approach was undertaken. In the first stage, an SQL method was developed to identify veterans using a keyword rule-based approach. This approach obtained an accuracy of 0.93 in correctly predicting civilians and veterans, a positive predictive value of 0.81, a sensitivity of 0.75, and a negative predictive value of 0.95. This method informed the second stage, which was the development of the MSIT using machine learning, which, when tested, obtained an accuracy of 0.97, a positive predictive value of 0.90, a sensitivity of 0.91, and a negative predictive value of 0.98. CONCLUSIONS: The MSIT has the potential to be used in identifying veterans in the United Kingdom from free-text clinical documents, providing new and unique insights into the health and well-being of this population and their use of mental health care services.",electronic health care records | machine learning | mental health | military personnel | natural language processing | veteran
"BACKGROUND: Patient histories in electronic health records currently exist mainly in free text format thereby limiting the possibility that decision support technology may contribute to the accuracy and timeliness of clinical diagnoses. Structuring and/or coding make patient histories potentially computable. METHODS: A systematic review was undertaken of the benefits and risks of structuring and/or coding patient history by searching nine international databases for published and unpublished studies over the period 1990-2010. The focus was on the current patient history, defined as information reported by a patient or the patient's caregiver about the patient's present health situation and health status. Findings were synthesised through a theoretically based textural analysis. FINDINGS: Of the 9207 potentially eligible papers identified, 10 studies satisfied the eligibility criteria. There was evidence of a modest number of benefits associated with structuring the current patient history, including obtaining more complete clinical histories, improved accuracy of patient self-documented histories, and better associated decision-making by professionals. However, no studies demonstrated any resulting improvements in patient care or outcomes. When more detailed records were obtained through the use of a structured format no attempt was made to confirm if this additional information was clinically useful. No studies investigated possible risks associated with structuring the patient history. No studies examined coding of the patient history. CONCLUSIONS: There is an insufficient evidence base for sound policy making on the benefits and risks of structuring and/or coding patient history. The authors suggest this field of enquiry warrants further investigation given the interest in use of decision support technology to aid diagnoses.",
"BACKGROUND: Problem-oriented electronic health record (EHR) systems can help physicians to track a patient's status and progress, and organize clinical documentation, which could help improving quality of clinical data and enable data reuse. The problem list is central in a problem-oriented medical record. However, current problem lists remain incomplete because of the lack of end-user training and inaccurate content of underlying terminologies. This leads to modifications of diagnosis code descriptions and use of free-text notes, limiting reuse of data. OBJECTIVES: We aimed to investigate factors that influence acceptance and actual use of the problem list, and used these to propose recommendations, to increase the value of problem lists for (re)use. METHODS: Semistructured interviews were conducted with physicians, heads of medical departments, and data quality experts, who were invited through snowball sampling. The interviews were transcribed and coded. Comments were fitted in constructs of the validated framework unified theory of acceptance user technology (UTAUT), and were discussed in terms of facilitators and barriers. RESULTS: In total, 24 interviews were conducted. We found large variability in attitudes toward problem list use. Barriers included uncertainty about the responsibility for maintaining the problem list and little perceived benefits. Facilitators included the (re)design of policies, improved (peer-to-peer) training to increase motivation, and positive peer feedback and monitoring. Motivation is best increased through sharing benefits relevant in the care process, such as providing overview, timely generation of discharge or referral letters, and reuse of data. Furthermore, content of the underlying terminology should be improved and the problem list should be better presented in the EHR system. CONCLUSION: To let physicians accept and use the problem list, policies and guidelines should be redesigned, and prioritized by supervising staff. Additionally, peer-to-peer training on the benefits of using the problem list is needed.",
"BACKGROUND: The aging population has led to an increase in cognitive impairment (CI) resulting in significant costs to patients, their families, and society. A research endeavor on a large cohort to better understand the frequency and severity of CI is urgent to respond to the health needs of this population. However, little is known about temporal trends of patient health functions (i.e., activity of daily living [ADL]) and how these trends are associated with the onset of CI in elderly patients. Also, the use of a rich source of clinical free text in electronic health records (EHRs) to facilitate CI research has not been well explored. The aim of this study is to characterize and better understand early signals of elderly patient CI by examining temporal trends of patient ADL and analyzing topics of patient medical conditions in clinical free text using topic models. METHODS: The study cohort consists of physician-diagnosed CI patients (n = 1,435) and cognitively unimpaired (CU) patients (n = 1,435) matched by age and sex, selected from patients 65 years of age or older at the time of enrollment in the Mayo Clinic Biobank. A corpus analysis was performed to examine the basic statistics of event types and practice settings where the physician first diagnosed CI. We analyzed the distribution of ADL in three different age groups over time before the development of CI. Furthermore, we applied three different topic modeling approaches on clinical free text to examine how patients' medical conditions change over time when they were close to CI diagnosis. RESULTS: The trajectories of ADL deterioration became steeper in CI patients than CU patients approximately 1 to 1.5 year(s) before the actual physician diagnosis of CI. The topic modeling showed that the topic terms were mostly correlated and captured the underlying semantics relevant to CI when approaching to CI diagnosis. CONCLUSIONS: There exist notable differences in temporal trends of basic and instrumental ADL between CI and CU patients. The trajectories of certain individual ADL, such as bathing and responsibility of own medication, were closely associated with CI development. The topic terms obtained by topic modeling methods from clinical free text have a potential to show how CI patients' conditions evolve and reveal overlooked conditions when they close to CI diagnosis.",Activity of daily living | Cognitive impairment | Deep learning | Early diagnosis | Topic modeling
"Electronic Health Records (EHR) contain large amounts of useful information that could potentially be used for building models for predicting onset of diseases. In this study, we have investigated the use of free-text and coded data in Marshfield Clinic's EHR, individually and in combination for building machine learning based models to predict the first ever episode of atrial fibrillation and/or atrial flutter (AFF). We trained and evaluated our AFF models on the EHR data across different time intervals (1, 3, 5 and all years) prior to first documented onset of AFF. We applied several machine learning methods, including naïve bayes, support vector machines (SVM), logistic regression and random forests for building AFF prediction models and evaluated these using 10-fold cross-validation approach. On text-based datasets, the best model achieved an F-measure of 60.1%, when applied exclusively to coded data. The combination of textual and coded data achieved comparable performance. The study results attest to the relative merit of utilizing textual data to complement the use of coded data for disease onset prediction modeling.",
"Electronic Health Records (EHRs) have increased the utility and portability of health information by storing it in structured formats. However, EHRs separate this structured data from the rich, free-text descriptions of clinical notes. The ultimate objective of our research is to develop an interactive progress note that unifies entry, access, and retrieval of structured and unstructured health information. In this study we present the design and subsequent testing with eight clinicians of a core element of this envisioned note: free-text order entry. Clinicians saw this new order-entry paradigm as a way to save time and preserve data quality by reducing double-documentation. However, they wanted the prototype to recognize more diverse types of shorthand and apply default values to fields that remain fairly constant across orders, such as number of refills and pickup location. Future work will test more complex orders, such as cascading orders, with a broader range of clinicians.",
"The combination of improved genomic analysis methods, decreasing genotyping costs, and increasing computing resources has led to an explosion of clinical genomic knowledge in the last decade. Similarly, healthcare systems are increasingly adopting robust electronic health record (EHR) systems that not only can improve health care, but also contain a vast repository of disease and treatment data that could be mined for genomic research. Indeed, institutions are creating EHR-linked DNA biobanks to enable genomic and pharmacogenomic research, using EHR data for phenotypic information. However, EHRs are designed primarily for clinical care, not research, so reuse of clinical EHR data for research purposes can be challenging. Difficulties in use of EHR data include: data availability, missing data, incorrect data, and vast quantities of unstructured narrative text data. Structured information includes billing codes, most laboratory reports, and other variables such as physiologic measurements and demographic information. Significant information, however, remains locked within EHR narrative text documents, including clinical notes and certain categories of test results, such as pathology and radiology reports. For relatively rare observations, combinations of simple free-text searches and billing codes may prove adequate when followed by manual chart review. However, to extract the large cohorts necessary for genome-wide association studies, natural language processing methods to process narrative text data may be needed. Combinations of structured and unstructured textual data can be mined to generate high-validity collections of cases and controls for a given condition. Once high-quality cases and controls are identified, EHR-derived cases can be used for genomic discovery and validation. Since EHR data includes a broad sampling of clinically-relevant phenotypic information, it may enable multiple genomic investigations upon a single set of genotyped individuals. This chapter reviews several examples of phenotype extraction and their application to genetic research, demonstrating a viable future for genomic discovery using EHR-linked data.",
"OBJECTIVES: Postoperative delirium is a common complication after major surgery among the elderly. Despite its potentially serious consequences, the complication often goes undetected and undiagnosed. In order to provide diagnosis support one could potentially exploit the information hidden in free text documents from electronic health records using data-driven clinical decision support tools. However, these tools depend on labeled training data and can be both time consuming and expensive to create. METHODS: The recent learning with anchors framework resolves this problem by transforming key observations (anchors) into labels. This is a promising framework, but it is heavily reliant on clinicians knowledge for specifying good anchor choices in order to perform well. In this paper we propose a novel method for specifying anchors from free text documents, following an exploratory data analysis approach based on clustering and data visualization techniques. We investigate the use of the new framework as a way to detect postoperative delirium. RESULTS: By applying the proposed method to medical data gathered from a Norwegian university hospital, we increase the area under the precision-recall curve from 0.51 to 0.96 compared to baselines. CONCLUSIONS: The proposed approach can be used as a framework for clinical decision support for postoperative delirium.",Clustering | Data-driven clinical decision support | Electronic health records | Learning with anchors framework | Postoperative delirium | Semi-supervised learning
"CONTEXT: Adverse events in healthcare are often collated in incident reports which contain unstructured free text. Learning from these events may improve patient safety. Natural language processing (NLP) uses computational techniques to interrogate free text, reducing the human workload associated with its analysis. There is growing interest in applying NLP to patient safety, but the evidence in the field has not been summarised and evaluated to date. OBJECTIVE: To perform a systematic literature review and narrative synthesis to describe and evaluate NLP methods for classification of incident reports and adverse events in healthcare. METHODS: Data sources included Medline, Embase, The Cochrane Library, CINAHL, MIDIRS, ISI Web of Science, SciELO, Google Scholar, PROSPERO, hand searching of key articles, and OpenGrey. Data items were manually abstracted to a standardised extraction form. RESULTS: From 428 articles screened for eligibility, 35 met the inclusion criteria of using NLP to perform a classification task on incident reports, or with the aim of detecting adverse events. The majority of studies used free text from incident reporting systems or electronic health records. Models were typically designed to classify by type of incident, type of medication error, or harm severity. A broad range of NLP techniques are demonstrated to perform these classification tasks with favourable performance outcomes. There are methodological challenges in how these results can be interpreted in a broader context. CONCLUSION: NLP can generate meaningful information from unstructured data in the specific domain of the classification of incident reports and adverse events. Understanding what or why incidents are occurring is important in adverse event analysis. If NLP enables these insights to be drawn from larger datasets it may improve the learning from adverse events in healthcare.",Adverse event analysis | Incident reporting | Machine learning | Natural language processing | Patient safety | Text classification
"BACKGROUND: Gastroenterology specialty societies have advocated that providers routinely assess their performance on colonoscopy quality measures. Such routine measurement has been hampered by the costs and time required to manually review colonoscopy and pathology reports. Natural language processing (NLP) is a field of computer science in which programs are trained to extract relevant information from text reports in an automated fashion. OBJECTIVE: To demonstrate the efficiency and potential of NLP-based colonoscopy quality measurement. DESIGN: In a cross-sectional study design, we used a previously validated NLP program to analyze colonoscopy reports and associated pathology notes. The resulting data were used to generate provider performance on colonoscopy quality measures. SETTING: Nine hospitals in the University of Pittsburgh Medical Center health care system. PATIENTS: Study sample consisted of the 24,157 colonoscopy reports and associated pathology reports from 2008 to 2009. MAIN OUTCOME MEASUREMENTS: Provider performance on 7 quality measures. RESULTS: Performance on the colonoscopy quality measures was generally poor, and there was a wide range of performance. For example, across hospitals, the adequacy of preparation was noted overall in only 45.7% of procedures (range 14.6%-86.1% across 9 hospitals), cecal landmarks were documented in 62.7% of procedures (range 11.6%-90.0%), and the adenoma detection rate was 25.2% (range 14.9%-33.9%). LIMITATIONS: Our quality assessment was limited to a single health care system in western Pennsylvania. CONCLUSIONS: Our study illustrates how NLP can mine free-text data in electronic records to measure and report on the quality of care. Even within a single academic hospital system, there is considerable variation in the performance on colonoscopy quality measures, demonstrating the need for better methods to regularly and efficiently assess quality.",
"Electronic health record systems are ubiquitous and the majority of patients' data are now being collected electronically in the form of free text. Deep learning has significantly advanced the field of natural language processing and the self-supervised representation learning and the transfer learning have become the methods of choice in particular when the high quality annotated data are limited. Identification of medical concepts and information extraction is a challenging task, yet important ingredient for parsing unstructured data into structured and tabulated format for downstream analytical tasks. In this work we introduced a named-entity recognition (NER) model for clinical natural language processing. The model is trained to recognise seven categories: drug names, route of administration, frequency, dosage, strength, form, duration. The model was first pre-trained on the task of predicting the next word, using a collection of 2 million free-text patients' records from MIMIC-III corpora followed by fine-tuning on the named-entity recognition task. The model achieved a micro-averaged F1 score of 0.957 across all seven categories. Additionally, we evaluated the transferability of the developed model using the data from the Intensive Care Unit in the US to secondary care mental health records (CRIS) in the UK. A direct application of the trained NER model to CRIS data resulted in reduced performance of F1 = 0.762, however after fine-tuning on a small sample from CRIS, the model achieved a reasonable performance of F1 = 0.944. This demonstrated that despite a close similarity between the data sets and the NER tasks, it is essential to fine-tune the target domain data in order to achieve more accurate results. The resulting model and the pre-trained embeddings are available at https://github.com/kormilitzin/med7.",Active learning | Clinical natural language processing | Neural networks | Noisy labelling | Self-supervised learning
"PURPOSE: Reduction in unplanned episodes of care, such as emergency department visits and unplanned hospitalizations, are important quality outcome measures. However, many events are only documented in free-text clinician notes and are labor intensive to detect by manual medical record review. METHODS: We studied 308,096 free-text machine-readable documents linked to individual entries in our electronic health records, representing care for patients with breast, GI, or thoracic cancer, whose treatment was initiated at one academic medical center, Stanford Health Care (SHC). Using a clinical text-mining tool, we detected unplanned episodes documented in clinician notes (for non-SHC visits) or in coded encounter data for SHC-delivered care and the most frequent symptoms documented in emergency department (ED) notes. RESULTS: Combined reporting increased the identification of patients with one or more unplanned care visits by 32% (15% using coded data; 20% using all the data) among patients with 3 months of follow-up and by 21% (23% using coded data; 28% using all the data) among those with 1 year of follow-up. Based on the textual analysis of SHC ED notes, pain (75%), followed by nausea (54%), vomiting (47%), infection (36%), fever (28%), and anemia (27%), were the most frequent symptoms mentioned. Pain, nausea, and vomiting co-occur in 35% of all ED encounter notes. CONCLUSION: The text-mining methods we describe can be applied to automatically review free-text clinician notes to detect unplanned episodes of care mentioned in these notes. These methods have broad application for quality improvement efforts in which events of interest occur outside of a network that allows for patient data sharing.",
"With an aging patient population and increasing complexity in patient disease trajectories, physicians are often met with complex patient histories from which clinical decisions must be made. Due to the increasing rate of adverse events and hospitals facing financial penalties for readmission, there has never been a greater need to enforce evidence-led medical decision-making using available health care data. In the present work, we studied a cohort of 7,741 patients, of whom 4,080 were diagnosed with cancer, surgically treated at a University Hospital in the years 2004-2012. We have developed a methodology that allows disease trajectories of the cancer patients to be estimated from free text in electronic health records (EHRs). By using these disease trajectories, we predict 80% of patient events ahead in time. By control of confounders from 8326 quantified events, we identified 557 events that constitute high subsequent risks (risk > 20%), including six events for cancer and seven events for metastasis. We believe that the presented methodology and findings could be used to improve clinical decision support and personalize trajectories, thereby decreasing adverse events and optimizing cancer treatment.",
"BACKGROUND: Nonvalvular atrial fibrillation (NVAF) affects almost 6 million Americans and is a major contributor to stroke but is significantly undiagnosed and undertreated despite explicit guidelines for oral anticoagulation. OBJECTIVE: The aim of this study is to investigate whether the use of semisupervised natural language processing (NLP) of electronic health record's (EHR) free-text information combined with structured EHR data improves NVAF discovery and treatment and perhaps offers a method to prevent thousands of deaths and save billions of dollars. METHODS: We abstracted 96,681 participants from the University of Buffalo faculty practice's EHR. NLP was used to index the notes and compare the ability to identify NVAF, congestive heart failure, hypertension, age ≥75 years, diabetes mellitus, stroke or transient ischemic attack, vascular disease, age 65 to 74 years, sex category (CHA(2)DS(2)-VASc), and Hypertension, Abnormal liver/renal function, Stroke history, Bleeding history or predisposition, Labile INR, Elderly, Drug/alcohol usage (HAS-BLED) scores using unstructured data (International Classification of Diseases codes) versus structured and unstructured data from clinical notes. In addition, we analyzed data from 63,296,120 participants in the Optum and Truven databases to determine the NVAF frequency, rates of CHA(2)DS(2)‑VASc ≥2, and no contraindications to oral anticoagulants, rates of stroke and death in the untreated population, and first year's costs after stroke. RESULTS: The structured-plus-unstructured method would have identified 3,976,056 additional true NVAF cases (P<.001) and improved sensitivity for CHA(2)DS(2)-VASc and HAS-BLED scores compared with the structured data alone (P=.002 and P<.001, respectively), causing a 32.1% improvement. For the United States, this method would prevent an estimated 176,537 strokes, save 10,575 lives, and save >US $13.5 billion. CONCLUSIONS: Artificial intelligence-informed bio-surveillance combining NLP of free-text information with structured EHR data improves data completeness, prevents thousands of strokes, and saves lives and funds. This method is applicable to many disorders with profound public health consequences.",CHA2DS2-VASc | HAS-BLED | NVAF | afib | artificial intelligence | atrial fibrillation | bio-surveillance | bleed risk | natural language processing | stroke risk
"BACKGROUND: Electronic health records are invaluable for medical research, but much of the information is recorded as unstructured free text which is time-consuming to review manually. AIM: To develop an algorithm to identify relevant free texts automatically based on labelled examples. METHODS: We developed a novel machine learning algorithm, the 'Semi-supervised Set Covering Machine' (S3CM), and tested its ability to detect the presence of coronary angiogram results and ovarian cancer diagnoses in free text in the General Practice Research Database. For training the algorithm, we used texts classified as positive and negative according to their associated Read diagnostic codes, rather than by manual annotation. We evaluated the precision (positive predictive value) and recall (sensitivity) of S3CM in classifying unlabelled texts against the gold standard of manual review. We compared the performance of S3CM with the Transductive Vector Support Machine (TVSM), the original fully-supervised Set Covering Machine (SCM) and our 'Freetext Matching Algorithm' natural language processor. RESULTS: Only 60% of texts with Read codes for angiogram actually contained angiogram results. However, the S3CM algorithm achieved 87% recall with 64% precision on detecting coronary angiogram results, outperforming the fully-supervised SCM (recall 78%, precision 60%) and TSVM (recall 2%, precision 3%). For ovarian cancer diagnoses, S3CM had higher recall than the other algorithms tested (86%). The Freetext Matching Algorithm had better precision than S3CM (85% versus 74%) but lower recall (62%). CONCLUSIONS: Our novel S3CM machine learning algorithm effectively detected free texts in primary care records associated with angiogram results and ovarian cancer diagnoses, after training on pre-classified test sets. It should be easy to adapt to other disease areas as it does not rely on linguistic rules, but needs further testing in other electronic health record datasets.",
"OBJECTIVE: To develop scalable natural language processing (NLP) infrastructure for processing the free text in electronic health records (EHRs). MATERIALS AND METHODS: We extend the open-source Apache cTAKES NLP software with several standard technologies for scalability. We remove processing bottlenecks by monitoring component queue size. We process EHR free text for patients in the PrecisionLink Biobank at Boston Children's Hospital. The extracted concepts are made searchable via a web-based portal. RESULTS: We processed over 1.2 million notes for over 8000 patients, extracting 154 million concepts. Our largest tested configuration processes over 1 million notes per day. DISCUSSION: The unique information represented by extracted NLP concepts has great potential to provide a more complete picture of patient status. CONCLUSION: NLP large EHR document collections can be done efficiently, in service of high throughput phenotyping.",biobanking | medical informatics | natural language processing | phenotyping
"BACKGROUND: With higher adoption of electronic health records at health-care centers, electronic search algorithms (computable phenotype) for identifying acute decompensated heart failure (ADHF) among hospitalized patients can be an invaluable tool to enhance data abstraction accuracy and efficacy in order to improve clinical research accrual and patient centered outcomes. We aimed to derive and validate a computable phenotype for ADHF in hospitalized patients. METHODS: We screened 256, 443 eligible (age > 18 years and with prior research authorization) individuals who were admitted to Mayo Clinic Hospital in Rochester, MN, from January 1, 2006, through December 31, 2014. Using a randomly selected derivation cohort of 938 patients, several iterations of a free-text electronic search were developed and refined. The computable phenotype was subsequently validated in an independent cohort 100 patients. The sensitivity and specificity of the computable phenotype were compared to the gold standard (expert review of charts) and International Classification of Diseases-9 (ICD-9) codes for Acute Heart Failure. RESULTS: In the derivation cohort, the computable phenotype achieved a sensitivity of 97.5%, and specificity of 100%, whereas ICD-9 codes for Acute Heart Failure achieved a sensitivity of 47.5% and specificity of 96.7%. When all Heart Failure codes (ICD-9) were used, sensitivity and specificity were 97.5 and 86.6%, respectively. In the validation cohort, the sensitivity and specificity of the computable phenotype were 100 and 98.5%. The sensitivity and specificity for the ICD-9 codes (Acute Heart Failure) were 42 and 98.5%. Upon use of all Heart Failure codes (ICD-9), sensitivity and specificity were 96.8 and 91.3%. CONCLUSIONS: Our results suggest that using computable phenotype to ascertain ADHF from the clinical notes contained within the electronic medical record are feasible and reliable. Our computable phenotype outperformed ICD-9 codes for the detection of ADHF.",ADHF | Acute decompensated heart failure | Acute heart failure | Electronic algorithm | ICD-9
"BACKGROUND: The prognosis, diagnosis, and treatment of many genetic disorders and familial diseases significantly improve if the family history (FH) of a patient is known. Such information is often written in the free text of clinical notes. OBJECTIVE: The aim of this study is to develop automated methods that enable access to FH data through natural language processing. METHODS: We performed information extraction by using transformers to extract disease mentions from notes. We also experimented with rule-based methods for extracting family member (FM) information from text and coreference resolution techniques. We evaluated different transfer learning strategies to improve the annotation of diseases. We provided a thorough error analysis of the contributing factors that affect such information extraction systems. RESULTS: Our experiments showed that the combination of domain-adaptive pretraining and intermediate-task pretraining achieved an F1 score of 81.63% for the extraction of diseases and FMs from notes when it was tested on a public shared task data set from the National Natural Language Processing Clinical Challenges (N2C2), providing a statistically significant improvement over the baseline (P<.001). In comparison, in the 2019 N2C2/Open Health Natural Language Processing Shared Task, the median F1 score of all 17 participating teams was 76.59%. CONCLUSIONS: Our approach, which leverages a state-of-the-art named entity recognition model for disease mention detection coupled with a hybrid method for FM mention detection, achieved an effectiveness that was close to that of the top 3 systems participating in the 2019 N2C2 FH extraction challenge, with only the top system convincingly outperforming our approach in terms of precision.",clinical natural language processing | data augmentation | information extraction | named entity recognition | natural language processing | neural language modeling | sequence tagging
"BACKGROUND: Healthcare data from electronic health records (EHRs) and related health information technology (IT) tools are critical data sources for pragmatic clinical trials and observational studies aimed at producing real-world evidence. To unlock the full potential of such data to advance science, the data must be complete and in structured formats to facilitate research use. METHODS: A Health IT survey was conducted within the National Drug Abuse Treatment Clinical Trials Network (CTN) to explore information related to data completeness and presence of unstructured data (e.g., clinical notes, free text) for conducting the EHR-based research for substance use disorders (SUDs). The analysis was based on 36 participants from 36 facilities located in 14 states and affiliated with the CTN. RESULTS: The mean age of the participants (n = 34) was 48.0 years (SD = 9.8). Of the participants enrolled, 50.0% were female and 82.4% were white. Participants' facilities were from four census-defined regions (South 35.3%, Northeast 29.4%, West 20.6%, Midwest 11.8%, Missing 2.9%) and represented diverse settings. The EHR was used by all surveyed facilities including 17 different kinds of EHR platforms or vendors, and 17.6% (n = 6) of surveyed facilities also used a separate EHR for behavioral health care (e.g., SUD care). Paper records were also used by 76.5% of surveyed facilities for clinical care (e.g., for health risk appraisal questionnaires, substance use screening or assessment, check-in screening, substance use specific intervention/treatment or referral, or labs/testing). The prevalence of using a patient portal, practice management system, and mHealth for patient care was 76.5%, 50.0%, and 29.4%, respectively. CONCLUSION: While results are descriptive in nature, they reveal the heterogeneity in the existing EHRs and frequent use of paper records to document patient care tasks, especially for SUD care. The use of a separate EHR for behavioral healthcare also suggests the challenge of obtaining complete EHR data to support research for SUDs. Much EHR development, integration, and standardization needs to be done especially in regard to SUD treatment to facilitate research across disparate healthcare systems.",Clinical Trials Network | Common data elements | Electronic health records | Health information technology | Substance use disorder
"BACKGROUND AND OBJECTIVE: Percutaneous coronary intervention (PCI) using drug-eluting stents (DES) is an indispensable treatment for coronary artery disease. However, to evaluate the performance of various types of stents for PCI, numerous resources are required. We extracted clinical information from free-text records and, using practice-based evidence, compared the efficacy of various DES. MATERIALS AND METHODS: We developed a text mining tool based on regular expression and applied it to PCI reports stored in the electronic health records (EHRs) of Ajou University Hospital from 2010-2014. The PCI data were extracted from EHRs with a sensitivity of 0.996, a specificity of 1.000, and an F-measure of 0.995 when compared with a sample of 200 reports. Using these data, we compared the performance of stents by Kaplan-Meier analysis and the Cox hazard proportional regression. RESULTS: In the self-validation analysis comparing the first-generation to the second-generation DES, the second-generation DES was superior to the first-generation DES (hazard ratio [HR]: 0.423, 95% confidence interval [CI]: 0.284-0.630) in terms of target vessel revascularization (TVR), showing similar findings to the established results of previous studies. Among the second-generation DES, the biodegradable-polymer DES tended to be superior, with a risk of TVR (HR: 0.568, 95% CI: 0.281-1.147) falling below than that for the durable-polymer DES approximately 1 year after the index procedure. The Endeavor stent had the highest TVR risk among the newer generation DES (HR: 2.576, 95% CI: 1.273-5.210). CONCLUSIONS: In this study, we demonstrated how to construct a PCI data warehouse of PCI-related parameters obtained from free-text electronic records with high accuracy for use in the post surveillance of coronary stents in a time- and cost effective manner. Post surveillance of the practice based evidence in the PCI data warehouse indicated that the biodegradable-polymer DES might have a lower risk of TVR than the durable-polymer DES.",
"PURPOSE: To explore the impacts that structuring of electronic health records (EHRs) has had from the perspective of secondary use of patient data as reflected in currently published literature. This paper presents the results of a systematic literature review aimed at answering the following questions; (1) what are the common methods of structuring patient data to serve secondary use purposes; (2) what are the common methods of evaluating patient data structuring in the secondary use context, and (3) what impacts or outcomes of EHR structuring have been reported from the secondary use perspective. METHODS: The reported study forms part of a wider systematic literature review on the impacts of EHR structuring methods and evaluations of their impact. The review was based on a 12-step systematic review protocol adapted from the Cochrane methodology. Original articles included in the study were divided into three groups for analysis and reporting based on their use focus: nursing documentation, medical use and secondary use (presented in this paper). The analysis from the perspective of secondary use of data includes 85 original articles from 1975 to 2010 retrieved from 15 bibliographic databases. RESULTS: The implementation of structured EHRs can be roughly divided into applications for documenting patient data at the point of care and application for retrieval of patient data (post hoc structuring). Two thirds of the secondary use articles concern EHR structuring methods which were still under development or in the testing phase. METHODS: of structuring patient data such as codes, terminologies, reference information models, forms or templates and documentation standards were usually applied in combination. Most of the identified benefits of utilizing structured EHR data for secondary use purposes concentrated on information content and quality or on technical quality and reliability, particularly in the case of Natural Language Processing (NLP) studies. A few individual articles evaluated impacts on care processes, productivity and costs, patient safety, care quality or other health impacts. In most articles these endpoints were usually discussed as goals of secondary use and less as evidence-supported impacts, resulting from the use of structured EHR data for secondary purposes. CONCLUSIONS: Further studies and more sound evaluation methods are needed for evidence on how EHRs are utilized for secondary purposes, and how structured documentation methods can serve different users' needs, e.g. administration, statistics and research and development, in parallel to medical use purposes.",Electronic health records | Free text | Secondary use of patient data | Structured data | Systematic literature review
"A growing quantity of health data is being stored in Electronic Health Records (EHR). The free-text section of these clinical notes contains important patient and treatment information for research but also contains Personally Identifiable Information (PII), which cannot be freely shared within the research community without compromising patient confidentiality and privacy rights. Significant work has been invested in investigating automated approaches to text de-identification, the process of removing or redacting PII. Few studies have examined the performance of existing de-identification pipelines in a controlled comparative analysis. In this study, we use publicly available corpora to analyze speed and accuracy differences between three de-identification systems that can be run off-the-shelf: Amazon Comprehend Medical PHId, Clinacuity's CliniDeID, and the National Library of Medicine's Scrubber. No single system dominated all the compared metrics. NLM Scrubber was the fastest while CliniDeID generally had the highest accuracy.",
"BACKGROUND: The use of antipsychotic drugs in dementia has been reported to be associated with increased risk of cerebrovascular events and mortality. There is an international drive to reduce the use of these agents in patients with dementia and to improve the safety of prescribing and monitoring in this area. OBJECTIVES: The aim of this project was to use enhanced automated regular feedback of information from electronic health records to improve the quality of antipsychotic prescribing and monitoring in people with dementia. METHODS: The South London and Maudsley NHS Foundation Trust (SLaM) incorporated antipsychotic monitoring forms into its electronic health records. The SLaM Clinical Record Interactive Search (CRIS) platform provides researcher access to de-identified health records, and natural language processing is used in CRIS to derive structured data from unstructured free text, including recorded diagnoses and medication. Algorithms were thus developed to ascertain patients with dementia receiving antipsychotic treatment and to determine whether monitoring forms had been completed. We used two improvement plan-do-study-act cycles to improve the accuracy of the algorithm for automated evaluation and provided monthly feedback on team performance. RESULTS: A steady increase in antipsychotic monitoring form completion was observed across the study period. The percentage of our sample with a completed antipsychotic monitoring form more than doubled from October 2017 (22%) to January 2019 (58%). CONCLUSION: 'Real time' monitoring and regular feedback to teams offer a time-effective approach, complementary to standard audit methods, to enhance the safer prescribing of high risk drugs.",checklists | clinical audit | continuous quality improvement | dementia | electronic health records
"PURPOSE OF REVIEW: Electronic health records (EHRs) contain valuable data for identifying health outcomes, but these data also present numerous challenges when creating computable phenotyping algorithms. Machine learning methods could help with some of these challenges. In this review, we discuss four common scenarios that researchers may find helpful for thinking critically about when and for what tasks machine learning may be used to identify health outcomes from EHR data. RECENT FINDINGS: We first consider the conditions in which machine learning may be especially useful with respect to two dimensions of a health outcome: 1) the characteristics of its diagnostic criteria, and 2) the format in which its diagnostic data are usually stored within EHR systems. In the first dimension, we propose that for health outcomes with diagnostic criteria involving many clinical factors, vague definitions, or subjective interpretations, machine learning may be useful for modeling the complex diagnostic decision-making process from a vector of clinical inputs to identify individuals with the health outcome. In the second dimension, we propose that for health outcomes where diagnostic information is largely stored in unstructured formats such as free text or images, machine learning may be useful for extracting and structuring this information as part of a natural language processing system or an image recognition task. We then consider these two dimensions jointly to define four common scenarios of health outcomes. For each scenario, we discuss the potential uses for machine learning - first assuming accurate and complete EHR data and then relaxing these assumptions to accommodate the limitations of real-world EHR systems. We illustrate these four scenarios using concrete examples and describe how recent studies have used machine learning to identify these health outcomes from EHR data. SUMMARY: Machine learning has great potential to improve the accuracy and efficiency of health outcome identification from EHR systems, especially under certain conditions. To promote the use of machine learning in EHR-based phenotyping tasks, future work should prioritize efforts to increase the transportability of machine learning algorithms for use in multi-site settings.",cohort identification | electronic health records | health outcomes | machine learning | phenotyping
"Crohn's disease (CD) and ulcerative colitis (UC) are heterogeneous. With availability of therapeutic classes with distinct immunologic mechanisms of action, it has become imperative to identify markers that predict likelihood of response to each drug class. However, robust development of such tools has been challenging because of need for large prospective cohorts with systematic and careful assessment of treatment response using validated indices. Most hospitals in the United States use electronic health records (EHRs) that warehouse a large amount of narrative (free-text) and codified (administrative) data generated during routine clinical care. These data have been used to construct virtual disease cohorts for epidemiologic research as well as for defining genetic basis of disease states or discrete laboratory values.(1-3) Whether EHR-based data can be used to validate genetic associations for more nuanced outcomes such as treatment response has not been examined previously.",
"A health record database contains structured data fields that identify the patient, such as patient ID, patient name, e-mail and phone number. These data are fairly easy to de-identify, that is, replace with other identifiers. However, these data also occur in fields with doctors' free-text notes written in an abbreviated style that cannot be analyzed grammatically. If we replace a word that looks like a name, but isn't, we degrade readability and medical correctness. If we fail to replace it when we should, we degrade confidentiality. We de-identified an existing Danish electronic health record database, ending up with 323,122 patient health records. We had to invent many methods for de-identifying potential identifiers in the free-text notes. The de-identified health records should be used with caution for statistical purposes because we removed health records that were so special that they couldn't be de-identified. Furthermore, we distorted geography by replacing zip codes with random zip codes.",anonymity | consistency | correctness | de-identification | electronic health records | readability
"The COVID-19 pandemic swept across the world rapidly infecting millions of people. An efficient tool that can accurately recognize important clinical concepts of COVID-19 from free text in electronic health records will be significantly valuable to accelerate various applications of COVID-19 research. To this end, the existing clinical NLP tool CLAMP was quickly adapted to COVID-19 information and generated an automated tool called COVID-19 SignSym, which can extract and signs/symptoms and their eight attributes such as temporal information and negations from clinical text. The extracted information is also mapped to standard clinical concepts in the common data model of OHDSI OMOP. Evaluation on clinical notes and medical dialogues demonstrated promising results. It is freely accessible to the community as a downloadable package of APIs (https://clamp.uth.edu/covid/nlp.php). We believe COVID-19 SignSym will provide fundamental supports to the secondary use of EHRs, thus accelerating the global research of COVID-19.",
"OBJECTIVE: Incomplete and static reaction picklists in the allergy module led to free-text and missing entries that inhibit the clinical decision support intended to prevent adverse drug reactions. We developed a novel, data-driven, ""dynamic"" reaction picklist to improve allergy documentation in the electronic health record (EHR). MATERIALS AND METHODS: We split 3 decades of allergy entries in the EHR of a large Massachusetts healthcare system into development and validation datasets. We consolidated duplicate allergens and those with the same ingredients or allergen groups. We created a reaction value set via expert review of a previously developed value set and then applied natural language processing to reconcile reactions from structured and free-text entries. Three association rule-mining measures were used to develop a comprehensive reaction picklist dynamically ranked by allergen. The dynamic picklist was assessed using recall at top k suggested reactions, comparing performance to the static picklist. RESULTS: The modified reaction value set contained 490 reaction concepts. Among 4 234 327 allergy entries collected, 7463 unique consolidated allergens and 469 unique reactions were identified. Of the 3 dynamic reaction picklists developed, the 1 with the optimal ranking achieved recalls of 0.632, 0.763, and 0.822 at the top 5, 10, and 15, respectively, significantly outperforming the static reaction picklist ranked by reaction frequency. CONCLUSION: The dynamic reaction picklist developed using EHR data and a statistical measure was superior to the static picklist and suggested proper reactions for allergy documentation. Further studies might evaluate the usability and impact on allergy documentation in the EHR.",
"BACKGROUND CONTEXT: The increasing volume of free-text notes available in electronic health records has created an opportunity for natural language processing (NLP) algorithms to mine this unstructured data in order to detect and predict adverse outcomes. Given the volume and diversity of documentation available in spine surgery, it remains unclear which types of documentation offer the greatest value for prediction of adverse outcomes. STUDY DESIGN/SETTING: Retrospective review of medical records at two academic and three community hospitals. PURPOSE: The purpose of this study was to conduct an exploratory analysis in order to examine the utility of free-text notes generated during the index hospitalization for lumbar spine fusion for prediction of 90-day unplanned readmission. PATIENT SAMPLE: Adult patients 18 years or older undergoing lumbar spine fusion for lumbar spondylolisthesis or lumbar spinal stenosis between January 1, 2016 and December 31, 2020. OUTCOME MEASURES: The primary outcome was inpatient admission within 90-days of discharge from the index hospitalization. METHODS: The predictive performance of NLP algorithms developed by using discharge summary notes, operative notes, nursing notes, physical therapy notes, case management notes, medical doctor (MD) (resident or attending), and allied practice professional (APP) (nurse practitioner or physician assistant) notes were assessed by discrimination, calibration, overall performance. RESULTS: Overall, 708 patients were included in the study and 83 (11.7%) had 90-day inpatient readmission. In the independent testing set of patients (n=141) not used for model development, the area under the receiver operating curve of NLP algorithms for prediction of 90-day readmission using discharge summary notes, operative notes, nursing notes, physical therapy notes, case management notes, MD/APP notes was 0.70, 0.57, 0.57, 0.60, 0.60, and 0.49 respectively. CONCLUSION: In this exploratory analysis, discharge summary, physical therapy, and case management notes had the most utility and daily MD/APP progress notes had the least utility for prediction of 90-day inpatient readmission in lumbar fusion patients among the free-text documentation generated during the index hospitalization.",Artificial intelligence | Machine learning | Natural language processing | Prediction | Readmission | Spine
"As the complexity of biomedical data increases, so do the opportunities to leverage them to advance science and clinical care. Electronic health records form a rich but complex source of large amounts of data gathered during routine clinical care. Through the use of codified and free-text concepts identified using clinical informatics tools such as natural language processing, disease phenotyping can be performed with a high degree of accuracy. Technologies such as genome sequencing, gene expression profiling, proteomic and metabolomic analyses, and electronic devices and wearables are generating large amounts of data from various populations, cell types, and disorders (big data). However, to make these data useable for the next step of biomarker discovery, precision medicine, and clinical practice, it is imperative to harmonize and integrate these diverse data sources. In this article, we introduce important building blocks for precision medicine, including common data models, text mining and natural language processing, privacy-preserved record linkage, machine learning for predictive modeling, and health information exchange.",
"OBJECTIVE: Hepatorenal Syndrome (HRS) is a devastating form of acute kidney injury (AKI) in advanced liver disease patients with high morbidity and mortality, but phenotyping algorithms have not yet been developed using large electronic health record (EHR) databases. We evaluated and compared multiple phenotyping methods to achieve an accurate algorithm for HRS identification. MATERIALS AND METHODS: A national retrospective cohort of patients with cirrhosis and AKI admitted to 124 Veterans Affairs hospitals was assembled from electronic health record data collected from 2005 to 2013. AKI was defined by the Kidney Disease: Improving Global Outcomes criteria. Five hundred and four hospitalizations were selected for manual chart review and served as the gold standard. Electronic Health Record based predictors were identified using structured and free text clinical data, subjected through NLP from the clinical Text Analysis Knowledge Extraction System. We explored several dimension reduction techniques for the NLP data, including newer high-throughput phenotyping and word embedding methods, and ascertained their effectiveness in identifying the phenotype without structured predictor variables. With the combined structured and NLP variables, we analyzed five phenotyping algorithms: penalized logistic regression, naïve Bayes, support vector machines, random forest, and gradient boosting. Calibration and discrimination metrics were calculated using 100 bootstrap iterations. In the final model, we report odds ratios and 95% confidence intervals. RESULTS: The area under the receiver operating characteristic curve (AUC) for the different models ranged from 0.73 to 0.93; with penalized logistic regression having the best discriminatory performance. Calibration for logistic regression was modest, but gradient boosting and support vector machines were superior. NLP identified 6985 variables; a priori variable selection performed similarly to dimensionality reduction using high-throughput phenotyping and semantic similarity informed clustering (AUC of 0.81 - 0.82). CONCLUSION: This study demonstrated improved phenotyping of a challenging AKI etiology, HRS, over ICD-9 coding. We also compared performance among multiple approaches to EHR-derived phenotyping, and found similar results between methods. Lastly, we showed that automated NLP dimension reduction is viable for acute illness.",Acute kidney injury | Cirrhosis | Dimension reduction | Hepatorenal syndrome | Natural language processing | Phenotyping
"For typically developing adolescents, being bullied is associated with increased risk of suicidality. Although adolescents with autism spectrum disorder (ASD) are at increased risk of both bullying and suicidality, there is very little research that examines the extent to which an experience of being bullied may increase suicidality within this specific population. To address this, we conducted a retrospective cohort study to investigate the longitudinal association between experiencing bullying and suicidality in a clinical population of 680 adolescents with ASD. Electronic health records of adolescents (13-17 years), using mental health services in South London, with a diagnosis of ASD were analyzed. Natural language processing was employed to identify mentions of bullying and suicidality in the free text fields of adolescents' clinical records. Cox regression analysis was employed to investigate the longitudinal relationship between bullying and suicidality outcomes. Reported experience of bullying in the first month of clinical contact was associated with an increased risk suicidality over the follow-up period (hazard ratio = 1.82; 95% confidence interval = 1.28-2.59). In addition, female gender, psychosis, affective disorder diagnoses, and higher intellectual ability were all associated with suicidality at follow-up. This study is the first to demonstrate the strength of longitudinal associations between bullying and suicidality in a clinical population of adolescents with ASD, using automated approaches to detect key life events within clinical records. Our findings provide support for identifying and dealing with bullying in schools, and for antibullying strategy's incorporation into wider suicide prevention programs for young people with ASD. Autism Res 2020, 13: 988-997. © 2020 The Authors. Autism Research published by International Society for Autism Research published by Wiley Periodicals, Inc. LAY SUMMARY: We investigated the relationship between bullying and suicidality in young people with autism spectrum disorder (ASD). We examined the clinical records of adolescents (aged 13-18 years old) with ASD in South London who were receiving treatment from Child and Adolescent Mental Health Services. We found that if they reported being bullied in the first month after they were first seen by mental health services, they were nearly twice as likely to go on to develop suicidal thoughts or behaviors.",adolescents | clinical psychiatry | comorbid conditions | data-driven techniques | epidemiology | longitudinal data analysis
"Socioeconomic status (SES) is a fundamental contributor to health, and a key factor underlying racial disparities in disease. However, SES data are rarely included in genetic studies due in part to the difficultly of collecting these data when studies were not originally designed for that purpose. The emergence of large clinic-based biobanks linked to electronic health records (EHRs) provides research access to large patient populations with longitudinal phenotype data captured in structured fields as billing codes, procedure codes, and prescriptions. SES data however, are often not explicitly recorded in structured fields, but rather recorded in the free text of clinical notes and communications. The content and completeness of these data vary widely by practitioner. To enable gene-environment studies that consider SES as an exposure, we sought to extract SES variables from racial/ethnic minority adult patients (n=9,977) in BioVU, the Vanderbilt University Medical Center biorepository linked to de-identified EHRs. We developed several measures of SES using information available within the de-identified EHR, including broad categories of occupation, education, insurance status, and homelessness. Two hundred patients were randomly selected for manual review to develop a set of seven algorithms for extracting SES information from de-identified EHRs. The algorithms consist of 15 categories of information, with 830 unique search terms. SES data extracted from manual review of 50 randomly selected records were compared to data produced by the algorithm, resulting in positive predictive values of 80.0% (education), 85.4% (occupation), 87.5% (unemployment), 63.6% (retirement), 23.1% (uninsured), 81.8% (Medicaid), and 33.3% (homelessness), suggesting some categories of SES data are easier to extract in this EHR than others. The SES data extraction approach developed here will enable future EHR-based genetic studies to integrate SES information into statistical analyses. Ultimately, incorporation of measures of SES into genetic studies will help elucidate the impact of the social environment on disease risk and outcomes.",
"OBJECTIVE: Medication-indication information is a key part of the information needed for providing decision support for and promoting appropriate use of medications. However, this information is not readily available to end users, and a lot of the resources only contain this information in unstructured form (free text). A number of public knowledge bases (KBs) containing structured medication-indication information have been developed over the years, but a direct comparison of these resources has not yet been conducted. MATERIAL AND METHODS: We conducted a systematic review of the literature to identify all medication-indication KBs and critically appraised these resources in terms of their scope as well as their support for complex indication information. RESULTS: We identified 7 KBs containing medication-indication data. They notably differed from each other in terms of their scope, coverage for on- or off-label indications, source of information, and choice of terminologies for representing the knowledge. The majority of KBs had issues with granularity of the indications as well as with representing duration of therapy, primary choice of treatment, and comedications or comorbidities. DISCUSSION AND CONCLUSION: This is the first study directly comparing public KBs of medication indications. We identified several gaps in the existing resources, which can motivate future research.",appropriateness | drug therapy | electronic health records | indication | off-label use
"Adverse drug events (ADEs) are unintended responses to medical treatment. They can greatly affect a patient's quality of life and present a substantial burden on healthcare. Although Electronic health records (EHRs) document a wealth of information relating to ADEs, they are frequently stored in the unstructured or semi-structured free-text narrative requiring Natural Language Processing (NLP) techniques to mine the relevant information. Here we present a rule-based ADE detection and classification pipeline built and tested on a large Psychiatric corpus comprising 264k patients using the de-identified EHRs of four UK-based psychiatric hospitals. The pipeline uses characteristics specific to Psychiatric EHRs to guide the annotation process, and distinguishes: a) the temporal value associated with the ADE mention (whether it is historical or present), b) the categorical value of the ADE (whether it is assertive, hypothetical, retrospective or a general discussion) and c) the implicit contextual value where the status of the ADE is deduced from surrounding indicators, rather than explicitly stated. We manually created the rulebase in collaboration with clinicians and pharmacists by studying ADE mentions in various types of clinical notes. We evaluated the open-source Adverse Drug Event annotation Pipeline (ADEPt) using 19 ADEs specific to antipsychotics and antidepressants medication. The ADEs chosen vary in severity, regularity and persistence. The average F-measure and accuracy achieved by our tool across all tested ADEs were 0.83 and 0.83 respectively. In addition to annotation power, the ADEPT pipeline presents an improvement to the state of the art context-discerning algorithm, ConText.",
"Accurate electronic health records are important for clinical care and research as well as ensuring patient safety. It is crucial for misspelled words to be corrected in order to ensure that medical records are interpreted correctly. This paper describes the development of a spelling correction system for medical text. Our spell checker is based on Shannon's noisy channel model, and uses an extensive dictionary compiled from many sources. We also use named entity recognition, so that names are not wrongly corrected as misspellings. We apply our spell checker to three different types of free-text data: clinical notes, allergy entries, and medication orders; and evaluate its performance on both misspelling detection and correction. Our spell checker achieves detection performance of up to 94.4% and correction accuracy of up to 88.2%. We show that high-performance spelling correction is possible on a variety of clinical documents.",Electronic health record | Named entity recognition | Natural language processing | Spelling correction
"BACKGROUND: Adverse drug reactions (ADRs) are estimated to be the fifth cause of hospital death. Up to 50% are potentially preventable and a significant number are recurrent (reADRs). Clinical decision support systems have been used to prevent reADRs using structured reporting concerning the patient's ADR experience, which in current clinical practice is poorly performed. Identifying ADRs directly from free text in electronic health records (EHRs) could circumvent this. AIM: To develop strategies to identify ADRs from free-text notes in electronic hospital health records. METHODS: In stage I, the EHRs of 10 patients were reviewed to establish strategies for identifying ADRs. In stage II, complete EHR histories of 45 patients were reviewed for ADRs and compared to the strategies programmed into a rule-based model. ADRs were classified using MedDRA and included in the study if the Naranjo causality score was ≥1. Seriousness was assessed using the European Medicine Agency's important medical event list. RESULTS: In stage I, two main search strategies were identified: keywords indicating an ADR and specific prepositions followed by medication names. In stage II, the EHRs contained a median of 7.4 (range 0.01-18) years of medical history covering over 35 000 notes. A total of 318 unique ADRs were identified of which 63 were potentially serious and 179 (sensitivity 57%) were identified by the rule. The method falsely identified 377 ADRs (positive predictive value 32%). However, it also identified an additional eight ADRs. CONCLUSION: Two key strategies were developed to identify ADRs from hospital EHRs using free-text notes. The results appear promising and warrant further study.",adverse drug event | adverse drug reaction | clinical decision support | clinical decision support system | drug allergy | free-text | natural language processing | text-mining
"A semantic lexicon which associates words and phrases in text to concepts is critical for extracting and encoding clinical information in free text and therefore achieving semantic interoperability between structured and unstructured data in Electronic Health Records (EHRs). Directly using existing standard terminologies may have limited coverage with respect to concepts and their corresponding mentions in text. In this paper, we analyze how tokens and phrases in a large corpus distribute and how well the UMLS captures the semantics. A corpus-driven semantic lexicon, MedLex, has been constructed where the semantics is based on the UMLS assisted with variants mined and usage information gathered from clinical text. The detailed corpus analysis of tokens, chunks, and concept mentions shows the UMLS is an invaluable source for natural language processing. Increasing the semantic coverage of tokens provides a good foundation in capturing clinical information comprehensively. The study also yields some insights in developing practical NLP systems.",
"OBJECTIVE: Physicians who more intensively interact with electronic health records (EHRs) through their documentation style may pay greater attention to coded fields and clinical decision support and thus may deliver higher quality care. We measured the quality of care of physicians who used three predominating EHR documentation styles: dictation, structured documentation, and free text. METHODS: We conducted a retrospective analysis of visits by patients with coronary artery disease and diabetes to the Partners Primary Care Practice Based Research Network. The main outcome measures were 15 EHR-based coronary artery disease and diabetes measures assessed 30 days after primary care visits. RESULTS: During the 9-month study period, 7000 coronary artery disease and diabetes patients made 18 569 visits to 234 primary care physicians of whom 20 (9%) predominantly dictated their notes, 68 (29%) predominantly used structured documentation, and 146 (62%) predominantly typed free text notes. In multivariable modeling adjusted for clustering by patient and physician, quality of care appeared significantly worse for dictators than for physicians using the other two documentation styles on three of 15 measures (antiplatelet medication, tobacco use documentation, and diabetic eye exam); better for structured documenters for three measures (blood pressure documentation, body mass index documentation, and diabetic foot exam); and better for free text documenters on one measure (influenza vaccination). There was no measure for which dictators had higher quality of care than physicians using the other two documentation styles. CONCLUSIONS: EHR-assessed quality is necessarily documentation-dependent, but physicians who dictated their notes appeared to have worse quality of care than physicians who used structured EHR documentation. CLINICAL TRIAL REGISTRATION NUMBER: ClinicalTrials.gov Identifier: NCT00235040.",
"BACKGROUND: How neighbourhood characteristics affect the physical safety of people with mental illness is unclear. AIMS: To examine neighbourhood effects on physical victimisation towards people using mental health services. METHOD: We developed and evaluated a machine-learning-derived free-text-based natural language processing (NLP) algorithm to ascertain clinical text referring to physical victimisation. This was applied to records on all patients attending National Health Service mental health services in Southeast London. Sociodemographic and clinical data, and diagnostic information on use of acute hospital care (from Hospital Episode Statistics, linked to Clinical Record Interactive Search), were collected in this group, defined as 'cases' and concurrently sampled controls. Multilevel logistic regression models estimated associations (odds ratios, ORs) between neighbourhood-level fragmentation, crime, income deprivation, and population density and physical victimisation. RESULTS: Based on a human-rated gold standard, the NLP algorithm had a positive predictive value of 0.92 and sensitivity of 0.98 for (clinically recorded) physical victimisation. A 1 s.d. increase in neighbourhood crime was accompanied by a 7% increase in odds of physical victimisation in women and an 13% increase in men (adjusted OR (aOR) for women: 1.07, 95% CI 1.01-1.14, aOR for men: 1.13, 95% CI 1.06-1.21, P for gender interaction, 0.218). Although small, adjusted associations for neighbourhood fragmentation appeared greater in magnitude for women (aOR = 1.05, 95% CI 1.01-1.11) than men, where this association was not statistically significant (aOR = 1.00, 95% CI 0.95-1.04, P for gender interaction, 0.096). Neighbourhood income deprivation was associated with victimisation in men and women with similar magnitudes of association. CONCLUSIONS: Neighbourhood factors influencing safety, as well as individual characteristics including gender, may be relevant to understanding pathways to physical victimisation towards people with mental illness.",Natural language processing | data linkage | electronic health records | neighbourhood characteristics | violence
"BACKGROUND: Alcohol use is a significant part of a patient's history, but details about consumption are not always documented. Electronic Health Record (EHR) systems have the potential to improve assessment of alcohol use and misuse; however, a challenge is that critical information may be documented primarily in free-text rather than in a structured and standardized format, thereby limiting its use. OBJECTIVE: To characterize the use and contents of free-text documentation for alcohol use in the social history module of an EHR. METHODS: This study involved a retrospective analysis of 500 alcohol use entries that include structured fields as well as a free-text comment field. Two coding schemes were developed and used to analyze these entries for: (1) quantifying the reasons for using free-text comments and (2) categorizing information in the free-text into separate elements. In addition, for entries indicating possible alcohol misuse, a preliminary review of other structured parts of the EHR was conducted to determine if this was also documented elsewhere. RESULTS: The top three reasons for using free-text were limited ability to describe alcohol use frequency (75%), amount (22%), and status (18%) with available structured fields. Within the free-text, descriptions of frequency were most common (79%) using words or phrases conveying occasional (61%), daily (13%), or weekly (12%) use. Of the 36 cases suggesting alcohol misuse, 44% had mention of alcohol problems in the problem list or past medical history. CONCLUSIONS: BASED ON THE EARLY FINDINGS, IMPLICATIONS FOR IMPROVING THE STRUCTURED COLLECTION AND USE OF ALCOHOL USE INFORMATION IN THE EHR ARE PROVIDED IN FOUR AREAS: (1) system enhancements, (2) user training, (3) decision support, and (4) standards. Next steps include examining how alcohol use is documented in other parts of the EHR (e.g., clinical notes) and how documentation practices vary based on patient, provider, and clinic characteristics.",Alcohol drinking | alcoholism | electronic health records | medical history taking
"BACKGROUND: Bleeding is associated with a significantly increased morbidity and mortality. Bleeding events are often described in the unstructured text of electronic health records, which makes them difficult to identify by manual inspection. OBJECTIVES: To develop a deep learning model that detects and visualizes bleeding events in electronic health records. PATIENTS/METHODS: Three hundred electronic health records with International Classification of Diseases, Tenth Revision diagnosis codes for bleeding or leukemia were extracted. Each sentence in the electronic health record was annotated as positive or negative for bleeding. The annotated sentences were used to develop a deep learning model that detects bleeding at sentence and note level. RESULTS: On a balanced test set of 1178   sentences, the best-performing deep learning model achieved a sensitivity of 0.90, specificity of 0.90, and negative predictive value of 0.90. On a test set consisting of 700 notes, of which 49 were positive for bleeding, the model achieved a note-level sensitivity of 1.00, specificity of 0.52, and negative predictive value of 1.00. By using a sentence-level model on a note level, the model can explain its predictions by visualizing the exact sentence in a note that contains information regarding bleeding. Moreover, we found that the model performed consistently well across different types of bleedings. CONCLUSIONS: A deep learning model can be used to detect and visualize bleeding events in the free text of electronic health records. The deep learning model can thus facilitate systematic assessment of bleeding risk, and thereby optimize patient care and safety.",decision support systems (clinical) | deep learning | electronic health record | hemorrhage | international classification of diseases | machine learning
"Recent initiatives have emphasized the potential role of Electronic Health Record (EHR) systems for improving tobacco use assessment and cessation. In support of these efforts, the goal of the present study was to examine tobacco use documentation in the EHR with an emphasis on free-text. Three coding schemes were developed and applied to analyze 525 tobacco use entries, including structured fields and a free-text comment field, from the social history module of an EHR system to characterize: (1) potential reasons for using free-text, (2) contents within the free-text, and (3) data quality issues. Free-text was most commonly used due to limitations for describing tobacco use amount (23.2%), frequency (26.9%), and start or quit dates (28.2%) as well as secondhand smoke exposure (17.9%) using a variety of words and phrases. The collective results provide insights for informing system enhancements, user training, natural language processing, and standards for tobacco use documentation.",
"BACKGROUND: Antipsychotic prescription information is commonly derived from structured fields in clinical health records. However, utilising diverse and comprehensive sources of information is especially important when investigating less frequent patterns of medication prescribing such as antipsychotic polypharmacy (APP). This study describes and evaluates a novel method of extracting APP data from both structured and free-text fields in electronic health records (EHRs), and its use for research purposes. METHODS: Using anonymised EHRs, we identified a cohort of patients with serious mental illness (SMI) who were treated in South London and Maudsley NHS Foundation Trust mental health care services between 1 January and 30 June 2012. Information about antipsychotic co-prescribing was extracted using a combination of natural language processing and a bespoke algorithm. The validity of the data derived through this process was assessed against a manually coded gold standard to establish precision and recall. Lastly, we estimated the prevalence and patterns of antipsychotic polypharmacy. RESULTS: Individual instances of antipsychotic prescribing were detected with high precision (0.94 to 0.97) and moderate recall (0.57-0.77). We detected baseline APP (two or more antipsychotics prescribed in any 6-week window) with 0.92 precision and 0.74 recall and long-term APP (antipsychotic co-prescribing for 6 months) with 0.94 precision and 0.60 recall. Of the 7,201 SMI patients receiving active care during the observation period, 338 (4.7 %; 95 % CI 4.2-5.2) were identified as receiving long-term APP. Two second generation antipsychotics (64.8 %); and first -second generation antipsychotics were most commonly co-prescribed (32.5 %). CONCLUSIONS: These results suggest that this is a potentially practical tool for identifying polypharmacy from mental health EHRs on a large scale. Furthermore, extracted data can be used to allow researchers to characterize patterns of polypharmacy over time including different drug combinations, trends in polypharmacy prescribing, predictors of polypharmacy prescribing and the impact of polypharmacy on patient outcomes.",
"PURPOSE: We aimed to evaluate the current situation of electronic health records (EHRs) and patient registries in the oncology departments of hospitals in Spain. METHODS: This was a cross-sectional study conducted from December 2018 to September 2019. The survey was designed ad hoc by the Outcomes Evaluation and Clinical Practice Section of the Spanish Society of Medical Oncology (SEOM) and was distributed to all head of medical oncology department members of SEOM. RESULTS: We invited 148 heads of oncology departments, and 81 (54.7%) questionnaires were completed, with representation from all 17 Spanish autonomous communities. Seventy-seven (95%) of the respondents had EHRs implemented at their hospitals; of them, over 80% considered EHRs to have a positive impact on work organization and clinical practice, and 73% considered that EHRs improve the quality of patient care. In contrast, 27 (35.1%) of these respondents felt that EHRs worsened the physician-patient relationship and conveyed an additional workload (n = 29; 37.6%). Several drawbacks in the implementation of EHRs were identified, including the limited inclusion of information on both outpatients and inpatients, information recorded in free text data fields, and the availability of specific informed consent. Forty-six (56.7%) respondents had patient registries where they recorded information from all patients seen in the department. CONCLUSION: Our study indicates that EHRs are almost universally implemented in the hospitals surveyed and are considered to have a positive impact on work organization and clinical practice. However, EHRs currently have several drawbacks that limit their use for investigational purposes. CLINICAL TRIAL REGISTRATION: Not applicable.",Clinical practice | Clinical research | Electronic health records | Medical oncology | Patient registries | Work organization
"OBJECTIVE: Evaluate the effectiveness and robustness of Anonym, a tool for de-identifying free-text health records based on conditional random fields classifiers informed by linguistic and lexical features, as well as features extracted by pattern matching techniques. De-identification of personal health information in electronic health records is essential for the sharing and secondary usage of clinical data. De-identification tools that adapt to different sources of clinical data are attractive as they would require minimal intervention to guarantee high effectiveness. METHODS AND MATERIALS: The effectiveness and robustness of Anonym are evaluated across multiple datasets, including the widely adopted Integrating Biology and the Bedside (i2b2) dataset, used for evaluation in a de-identification challenge. The datasets used here vary in type of health records, source of data, and their quality, with one of the datasets containing optical character recognition errors. RESULTS: Anonym identifies and removes up to 96.6% of personal health identifiers (recall) with a precision of up to 98.2% on the i2b2 dataset, outperforming the best system proposed in the i2b2 challenge. The effectiveness of Anonym across datasets is found to depend on the amount of information available for training. CONCLUSION: Findings show that Anonym compares to the best approach from the 2006 i2b2 shared task. It is easy to retrain Anonym with new datasets; if retrained, the system is robust to variations of training size, data type and quality in presence of sufficient training data.",Conditional random fields | De-identification | Health records | Pattern matching
"BACKGROUND: Usage of structured fields in Electronic Health Records (EHRs) to ascertain smoking history is important but fails in capturing the nuances of smoking behaviors. Knowledge of smoking behaviors, such as pack year history and most recent cessation date, allows care providers to select the best care plan for patients at risk of smoking attributable diseases. METHODS: We developed and evaluated a health informatics pipeline for identifying complete smoking history from clinical notes in EHRs. We utilized 758 patient-visit notes (from visits between 03/28/2016 and 04/04/2016) from our local EHR in addition to a public dataset of 502 clinical notes from the 2006 i2b2 Challenge to assess the performance of this pipeline. We used a machine-learning classifier to extract smoking status and a comprehensive set of text processing regular expressions to extract pack years and cessation date information from these clinical notes. RESULTS: We identified smoking status with an F1 score of 0.90 on both the i2b2 and local data sets. Regular expression identification of pack year history in the local test set was 91.7% sensitive and 95.2% specific, but due to variable context the pack year extraction was incomplete in 25% of cases, extracting packs per day or years smoked only. Regular expression identification of cessation date was 63.2% sensitive and 94.6% specific. CONCLUSIONS: Our work indicates that the development of an EHR-based Smokers' Registry containing information relating to smoking behaviors, not just status, from free-text clinical notes using an informatics pipeline is feasible. This pipeline is capable of functioning in external EHRs, reducing the amount of time and money needed at the institute-level to create a Smokers' Registry for improved identification of patient risk and eligibility for preventative and early detection services.",Electronic health records | Informatics pipeline | Natural language processing | Smokers registry
"BACKGROUND: Electronic health records (EHRs) are important data resources for clinical studies and applications. Physicians or clinicians describe patients' disorders or treatment procedures in EHRs using free text (unstructured) clinical notes. The narrative information plays an important role in patient treatment and clinical research. However, it is challenging to make machines understand the clinical narratives. OBJECTIVE: This study aimed to automatically identify Chinese clinical entities from free text in EHRs and make machines semantically understand diagnoses, tests, body parts, symptoms, treatments, and so on. METHODS: The dataset we used for this study is the benchmark dataset with human annotated Chinese EHRs, released by the China Conference on Knowledge Graph and Semantic Computing 2017 clinical named entity recognition challenge task. Overall, 2 machine learning models, the conditional random fields (CRF) method and bidirectional long short-term memory (LSTM)-CRF, were applied to recognize clinical entities from Chinese EHR data. To train the CRF-based model, we selected features such as bag of Chinese characters, part-of-speech tags, character types, and the position of characters. For the bidirectional LSTM-CRF-based model, character embeddings and segmentation information were used as features. In addition, we also employed a dictionary-based approach as the baseline for the purpose of performance evaluation. Precision, recall, and the harmonic average of precision and recall (F1 score) were used to evaluate the performance of the methods. RESULTS: Experiments on the test set showed that our methods were able to automatically identify types of Chinese clinical entities such as diagnosis, test, symptom, body part, and treatment simultaneously. With regard to overall performance, CRF and bidirectional LSTM-CRF achieved a precision of 0.9203 and 0.9112, recall of 0.8709 and 0.8974, and F1 score of 0.8949 and 0.9043, respectively. The results also indicated that our methods performed well in recognizing each type of clinical entity, in which the ""symptom"" type achieved the best F1 score of over 0.96. Moreover, as the number of features increased, the F1 score of the CRF model increased from 0.8547 to 0.8949. CONCLUSIONS: In this study, we employed two computational methods to simultaneously identify types of Chinese clinical entities from free text in EHRs. With training, these methods can effectively identify various types of clinical entities (eg, symptom and treatment) with high accuracy. The deep learning model, bidirectional LSTM-CRF, can achieve better performance than the CRF model with little feature engineering. This study contributed to translating human-readable health information into machine-readable information.",bidirectional LSTM-CRF | clinical named entity recognition | diagnosis | electronic health records | human body | machine learning | physical examination | syndrome | treatment
"BACKGROUND: Patient portals are becoming more common, and with them, the ability of patients to access their personal electronic health records (EHRs). EHRs, in particular the free-text EHR notes, often contain medical jargon and terms that are difficult for laypersons to understand. There are many Web-based resources for learning more about particular diseases or conditions, including systems that directly link to lay definitions or educational materials for medical concepts. OBJECTIVE: Our goal is to determine whether use of one such tool, NoteAid, leads to higher EHR note comprehension ability. We use a new EHR note comprehension assessment tool instead of patient self-reported scores. METHODS: In this work, we compare a passive, self-service educational resource (MedlinePlus) with an active resource (NoteAid) where definitions are provided to the user for medical concepts that the system identifies. We use Amazon Mechanical Turk (AMT) to recruit individuals to complete ComprehENotes, a new test of EHR note comprehension. RESULTS: Mean scores for individuals with access to NoteAid are significantly higher than the mean baseline scores, both for raw scores (P=.008) and estimated ability (P=.02). CONCLUSIONS: In our experiments, we show that the active intervention leads to significantly higher scores on the comprehension test as compared with a baseline group with no resources provided. In contrast, there is no significant difference between the group that was provided with the passive intervention and the baseline group. Finally, we analyze the demographics of the individuals who participated in our AMT task and show differences between groups that align with the current understanding of health literacy between populations. This is the first work to show improvements in comprehension using tools such as NoteAid as measured by an EHR note comprehension assessment tool as opposed to patient self-reported scores.",MedlinePlus | crowdsourcing | health literacy | information storage and retrieval | natural language processing | psychometrics
"INTRODUCTION: As the number of clinical decision support systems (CDSSs) incorporated into electronic medical records (EMRs) increases, so does the need to evaluate their effectiveness. The use of medical record review and similar manual methods for evaluating decision rules is laborious and inefficient. The authors use machine learning and Natural Language Processing (NLP) algorithms to accurately evaluate a clinical decision support rule through an EMR system, and they compare it against manual evaluation. METHODS: Modeled after the EMR system EPIC at Maine Medical Center, we developed a dummy data set containing physician notes in free text for 3,621 artificial patients records undergoing a head computed tomography (CT) scan for mild traumatic brain injury after the incorporation of an electronic best practice approach. We validated the accuracy of the Best Practice Advisories (BPA) using three machine learning algorithms-C-Support Vector Classification (SVC), Decision Tree Classifier (DecisionTreeClassifier), k-nearest neighbors classifier (KNeighborsClassifier)-by comparing their accuracy for adjudicating the occurrence of a mild traumatic brain injury against manual review. We then used the best of the three algorithms to evaluate the effectiveness of the BPA, and we compared the algorithm's evaluation of the BPA to that of manual review. RESULTS: The electronic best practice approach was found to have a sensitivity of 98.8 percent (96.83-100.0), specificity of 10.3 percent, PPV = 7.3 percent, and NPV = 99.2 percent when reviewed manually by abstractors. Though all the machine learning algorithms were observed to have a high level of prediction, the SVC displayed the highest with a sensitivity 93.33 percent (92.49-98.84), specificity of 97.62 percent (96.53-98.38), PPV = 50.00, NPV = 99.83. The SVC algorithm was observed to have a sensitivity of 97.9 percent (94.7-99.86), specificity 10.30 percent, PPV 7.25 percent, and NPV 99.2 percent for evaluating the best practice approach, after accounting for 17 cases (0.66 percent) where the patient records had to be reviewed manually due to the NPL systems inability to capture the proper diagnosis. DISCUSSION: CDSSs incorporated into EMRs can be evaluated in an automatic fashion by using NLP and machine learning techniques.",classification | data collection | data use and quality | electronic health records | health information technology | machine learning | natural language processing | quality improvement
"INTRODUCTION: Cardiovascular diseases (CVDs) are among the leading causes of death globally. Electronic health records (EHRs) provide a rich data source for research on CVD risk factors, treatments and outcomes. Researchers must be confident in the validity of diagnoses in EHRs, particularly when diagnosis definitions and use of EHRs change over time. Our systematic review provides an up-to-date appraisal of the validity of stroke, acute coronary syndrome (ACS) and heart failure (HF) diagnoses in European primary and secondary care EHRs. METHODS AND ANALYSIS: We will systematically review the published and grey literature to identify studies validating diagnoses of stroke, ACS and HF in European EHRs. MEDLINE, EMBASE, SCOPUS, Web of Science, Cochrane Library, OpenGrey and EThOS will be searched from the dates of inception to April 2019. A prespecified search strategy of subject headings and free-text terms in the title and abstract will be used. Two reviewers will independently screen titles and abstracts to identify eligible studies, followed by full-text review. We require studies to compare clinical codes with a suitable reference standard. Additionally, at least one validation measure (sensitivity, specificity, positive predictive value or negative predictive value) or raw data, for the calculation of a validation measure, is necessary. We will then extract data from the eligible studies using standardised tables and assess risk of bias in individual studies using the Quality Assessment of Diagnostic Accuracy Studies 2 tool. Data will be synthesised into a narrative format and heterogeneity assessed. Meta-analysis will be considered when a sufficient number of homogeneous studies are available. The overall quality of evidence will be assessed using the Grading of Recommendations, Assessment, Development and Evaluation tool. ETHICS AND DISSEMINATION: This is a systematic review, so it does not require ethical approval. Our results will be submitted for peer-review publication. PROSPERO REGISTRATION NUMBER: CRD42019123898.",cardiovascular diseases | electronic health records | systematic review | validation
"BACKGROUND: Total joint replacements are high-volume and high-cost procedures that should be monitored for cost and quality control. Models that can identify patients at high risk of readmission might help reduce costs by suggesting who should be enrolled in preventive care programs. Previous models for risk prediction have relied on structured data of patients rather than clinical notes in electronic health records (EHRs). The former approach requires manual feature extraction by domain experts, which may limit the applicability of these models. OBJECTIVE: This study aims to develop and evaluate a machine learning model for predicting the risk of 30-day readmission following knee and hip arthroplasty procedures. The input data for these models come from raw EHRs. We empirically demonstrate that unstructured free-text notes contain a reasonably predictive signal for this task. METHODS: We performed a retrospective analysis of data from 7174 patients at Partners Healthcare collected between 2006 and 2016. These data were split into train, validation, and test sets. These data sets were used to build, validate, and test models to predict unplanned readmission within 30 days of hospital discharge. The proposed models made predictions on the basis of clinical notes, obviating the need for performing manual feature extraction by domain and machine learning experts. The notes that served as model inputs were written by physicians, nurses, pathologists, and others who diagnose and treat patients and may have their own predictions, even if these are not recorded. RESULTS: The proposed models output readmission risk scores (propensities) for each patient. The best models (as selected on a development set) yielded an area under the receiver operating characteristic curve of 0.846 (95% CI 82.75-87.11) for hip and 0.822 (95% CI 80.94-86.22) for knee surgery, indicating reasonable discriminative ability. CONCLUSIONS: Machine learning models can predict which patients are at a high risk of readmission within 30 days following hip and knee arthroplasty procedures on the basis of notes in EHRs with reasonable discriminative power. Following further validation and empirical demonstration that the models realize predictive performance above that which clinical judgment may provide, such models may be used to build an automated decision support tool to help caretakers identify at-risk patients.",30-days readmission | auto ML | deep learning | electronic health records | hip arthroplasty | knee arthroplasty | natural language processing
"With increasing adoption of electronic health records (EHRs), there is an opportunity to use the free-text portion of EHRs for pharmacovigilance. We present novel methods that annotate the unstructured clinical notes and transform them into a deidentified patient-feature matrix encoded using medical terminologies. We demonstrate the use of the resulting high-throughput data for detecting drug-adverse event associations and adverse events associated with drug-drug interactions. We show that these methods flag adverse events early (in most cases before an official alert), allow filtering of spurious signals by adjusting for potential confounding, and compile prevalence information. We argue that analyzing large volumes of free-text clinical notes enables drug safety surveillance using a yet untapped data source. Such data mining can be used for hypothesis generation and for rapid analysis of suspected adverse event risk.",
"BACKGROUND: Epidemiological research on small cell lung cancer (SCLC) is limited and based on cancer registry data. We evaluated the feasibility and validity of using primary care electronic health records (The Health Improvement Network [THIN]) in the UK to identify and characterise SCLC. METHODS: We searched THIN records of individuals aged 18-89 years between 2000 and 2014 for a first diagnostic code suggestive of lung cancer (group 1) or small cell cancer (SCC; group 2) and for text strings among free text comments to identify and characterise incident SCLC cases. We validated our case identification strategy by manual review of patient EHRs, including free text comments, for a random sample of 400 individuals initially detected with a diagnostic code (300 from group 1 and 100 from group 2). RESULTS: Twenty five thousand two hundred fourty one individuals had a code for lung cancer (n = 24,508 [97.1%]) or SCC (733 [2.9%]). Following free-text searches, there were 3530 incident SCLC cases (2956 from group 1; 574 from group 2) corresponding to an incidence rate of 1.01 per 10,000 person-years. In the validation exercise, SCLC confirmation rates were 99% (group 1) and 85% (group 2). Mean age at diagnosis among confirmed cases was 68.5 years; staging information was present in 63.5% of cases of whom 17.8% had limited disease and 82.2% had extensive disease. The majority (84.5%) had a recorded symptom suggestive of lung cancer; chest infection was the most common (18%) followed by cough (15.8%) and chest/abdominal/back pain (15.2%). The first year crude mortality rates was 9.9 per 100 person-months (95% confidence interval [CI] 9.5-10.4), was higher among men and those aged 80 years and above. A total of 144 (37.8%) confirmed cases had metastases recorded. Median survival among the whole study cohort was 7.37 months. CONCLUSIONS: Our SCLC case identification method appears to be valid and could potentially be adapted to identify other cancer types. However, complete characterisation of staging requires information from additional data sources including cancer registries.",Database | Incidence | Small cell lung carcinoma
"Healthcare Information Systems should capture clinical data in a structured and preferably coded format. This is crucial for data exchange between health information systems, epidemiological analysis, quality and research, clinical decision support systems, administrative functions, among others. Structured data entry is an obstacle for the usability of electronic health record (EHR) applications and their acceptance by physicians who prefer to document patient EHRs using ""free text"". Natural language allows for rich expressiveness but at the same time is ambiguous; it has great dependence on context and uses jargon and acronyms. Although much progress has been made in knowledge and natural language processing techniques, the result is not yet satisfactory enough for the use of free text in all dimensions of clinical documentation. In order to address the trade-off between capturing data with free text and at the same time coding data for computer processing, numerous terminological systems for the systematic recording of clinical data have been developed. The purpose of terminology services consists of representing facts that happen in the real world through database management in order to allow for semantic interoperability and computerized applications. These systems interrelate concepts of a particular domain and provide references to related terms with standards codes. In this way, standard terminologies allow the creation of a controlled medical vocabulary, making terminology services a fundamental component for health data management in the healthcare environment. The Hospital Italiano de Buenos Aires has been working in the development of its own terminology server. This work describes its experience in the field.",
"OBJECTIVE: Clinical care guidelines recommend that newly diagnosed prostate cancer patients at high risk for metastatic spread receive a bone scan prior to treatment and that low risk patients not receive it. The objective was to develop an automated pipeline to interrogate heterogeneous data to evaluate the use of bone scans using a two different Natural Language Processing (NLP) approaches. MATERIALS AND METHODS: Our cohort was divided into risk groups based on Electronic Health Records (EHR). Information on bone scan utilization was identified in both structured data and free text from clinical notes. Our pipeline annotated sentences with a combination of a rule-based method using the ConText algorithm (a generalization of NegEx) and a Convolutional Neural Network (CNN) method using word2vec to produce word embeddings. RESULTS: A total of 5500 patients and 369,764 notes were included in the study. A total of 39% of patients were high-risk and 73% of these received a bone scan; of the 18% low risk patients, 10% received one. The accuracy of CNN model outperformed the rule-based model one (F-measure = 0.918 and 0.897 respectively). We demonstrate a combination of both models could maximize precision or recall, based on the study question. CONCLUSION: Using structured data, we accurately classified patients' cancer risk group, identified bone scan documentation with two NLP methods, and evaluated guideline adherence. Our pipeline can be used to provide concrete feedback to clinicians and guide treatment decisions.",Electronic health records | Machine learning | Natural language processing | Prostate cancer
"Atrial fibrillation (AF) is the most common arrhythmia and significantly increases stroke risk. This risk is effectively managed by oral anticoagulation. Recent studies using national registry data indicate increased use of anticoagulation resulting from changes in guidelines and the availability of newer drugs. The aim of this study is to develop and validate an open source risk scoring pipeline for free-text electronic health record data using natural language processing. AF patients discharged from 1st January 2011 to 1st October 2017 were identified from discharge summaries (N = 10,030, 64.6% male, average age 75.3 ± 12.3 years). A natural language processing pipeline was developed to identify risk factors in clinical text and calculate risk for ischaemic stroke (CHA2DS2-VASc) and bleeding (HAS-BLED). Scores were validated vs two independent experts for 40 patients. Automatic risk scores were in strong agreement with the two independent experts for CHA2DS2-VASc (average kappa 0.78 vs experts, compared to 0.85 between experts). Agreement was lower for HAS-BLED (average kappa 0.54 vs experts, compared to 0.74 between experts). In high-risk patients (CHA2DS2-VASc ≥2) OAC use has increased significantly over the last 7 years, driven by the availability of DOACs and the transitioning of patients from AP medication alone to OAC. Factors independently associated with OAC use included components of the CHA2DS2-VASc and HAS-BLED scores as well as discharging specialty and frailty. OAC use was highest in patients discharged under cardiology (69%). Electronic health record text can be used for automatic calculation of clinical risk scores at scale. Open source tools are available today for this task but require further validation. Analysis of routinely collected EHR data can replicate findings from large-scale curated registries.",
"OBJECTIVES: Relatively little is known about rates of outpatient adverse drug events (ADEs), and most health systems do not routinely identify them. We developed a computerized ADE measurement process and used it to detect ADEs from electronic health records and then categorized them according to type, preventability, and severity. METHODS: The rules used represent combinations of variables including coded medication names, laboratory results, diagnoses, and specific items such as symptoms from free text clinician notes, all obtained from electronic health records. Rules targeted various diagnostic and laboratory abnormalities potentially caused by a broad range of outpatient medications commonly used in primary care. The rules were run on 4 months of data on primary care patients seen in the outpatient setting in 2 large health systems; possible incidents were identified by chart review and validated as ADEs by clinician reviewers, then rated by severity and preventability. RESULTS: The rates of ADEs were 75 ADEs/1000 person-years and 198/1000 person-years at the 2 sites, respectively. The overall rate was 138 ADEs/1000 person-years across the 2 sites. Eleven percent of ADEs were preventable, with a rate of 15 preventable ADEs/1000 person-years across sites. Approximately one-fourth of ADEs were serious or life threatening at both sites. The highest yield rules for identifying preventable ADEs included rules based on drug classes and symptoms, and drug-laboratory rules. CONCLUSIONS: Adverse drug events occurred frequently in routine outpatient care, and many were serious and preventable. Computerized monitoring represents an efficacious approach for identifying ambulatory ADEs, although it needs additional refinement. In addition, site-specific variations need further exploration.",
"The wide-scale adoption of electronic health records (EHR)s has increased the availability of routinely collected clinical data in electronic form that can be used to improve the reporting of quality of care. However, the bulk of information in the EHR is in unstructured form (e.g., free-text clinical notes) and not amenable to automated reporting. Traditional methods are based on structured diagnostic and billing data that provide efficient, but inaccurate or incomplete summaries of actual or relevant care processes and patient outcomes. To assess the feasibility and benefit of implementing enhanced EHR- based physician quality measurement and reporting, which includes the analysis of unstructured free- text clinical notes, we conducted a retrospective study to compare traditional and enhanced approaches for reporting ten physician quality measures from multiple National Quality Strategy domains. We found that our enhanced approach enabled the calculation of five Physician Quality and Performance System measures not measureable in billing or diagnostic codes and resulted in over a five-fold increase in event at an average precision of 88 percent (95 percent CI: 83-93 percent). Our work suggests that enhanced EHR-based quality measurement can increase event detection for establishing value-based payment arrangements and can expedite quality reporting for physician practices, which are increasingly burdened by the process of manual chart review for quality reporting.",
"Prescription information is an important component of electronic health records (EHRs). This information contains detailed medication instructions that are crucial for patients' well-being and is often detailed in the narrative portions of EHRs. As a result, narratives of EHRs need to be processed with natural language processing (NLP) methods that can extract medication and prescription information from free text. However, automatic methods for medication and prescription extraction from narratives face two major challenges: (1) dictionaries can fall short even when identifying well-defined and syntactically consistent categories of medication entities, (2) some categories of medication entities are sparse, and at the same time lexically (and syntactically) diverse. In this paper, we describe FABLE, a system for automatically extracting prescription information from discharge summaries. FABLE utilizes unannotated data to enhance annotated training data: it performs semi-supervised extraction of medication information using pseudo-labels with Conditional Random Fields (CRFs) to improve its understanding of incomplete, sparse, and diverse medication entities. When evaluated against the official benchmark set from the 2009 i2b2 Shared Task and Workshop on Medication Extraction, FABLE achieves a horizontal phrase-level F1-measure of 0.878, giving state-of-the-art performance and significantly improving on nearly all entity categories.",
"BACKGROUND: Natural language processing (NLP) methods have the capability to process clinical free text in electronic health records, decreasing the need for costly manual chart review, and improving data quality. We developed rule-based NLP algorithms to automatically extract surgery specific data elements from knee arthroplasty operative notes. METHODS: Within a cohort of 20,000 knee arthroplasty operative notes from 2000 to 2017 at a large tertiary institution, we randomly selected independent pairs of training and test sets to develop and evaluate NLP algorithms to detect five major data elements. The size of the training and test datasets were similar and ranged between 420 to 1592 surgeries. Expert rules using keywords in operative notes were used to implement NLP algorithms capturing: (1) category of surgery (total knee arthroplasty, unicompartmental knee arthroplasty, patellofemoral arthroplasty), (2) laterality of surgery, (3) constraint type, (4) presence of patellar resurfacing, and (5) implant model (catalog numbers). We used institutional registry data as our gold standard to evaluate the NLP algorithms. RESULTS: NLP algorithms to detect the category of surgery, laterality, constraint, and patellar resurfacing achieved 98.3%, 99.5%, 99.2%, and 99.4% accuracy on test datasets, respectively. The implant model algorithm achieved an F1-score (harmonic mean of precision and recall) of 99.9%. CONCLUSIONS: NLP algorithms are a promising alternative to costly manual chart review to automate the extraction of embedded information within knee arthroplasty operative notes. Further validation in other hospital settings will enhance widespread implementation and efficiency in data capture for research and clinical purposes. LEVEL OF EVIDENCE: Level III.",artificial intelligence | constraint | electronic health records | natural language processing | patella resurfacing | total knee arthroplasty
"BACKGROUND: Free-text clinical records provide a source of information that complements traditional disease surveillance. To electronically harness these records, they need to be transformed into codified fields by natural language processing algorithms. OBJECTIVE: The aim of this study was to develop, train, and validate Clinical History Extractor for Syndromic Surveillance (CHESS), an natural language processing algorithm to extract clinical information from free-text primary care records. METHODS: CHESS is a keyword-based natural language processing algorithm to extract 48 signs and symptoms suggesting respiratory infections, gastrointestinal infections, constitutional, as well as other signs and symptoms potentially associated with infectious diseases. The algorithm also captured the assertion status (affirmed, negated, or suspected) and symptom duration. Electronic medical records from the National Healthcare Group Polyclinics, a major public sector primary care provider in Singapore, were randomly extracted and manually reviewed by 2 human reviewers, with a third reviewer as the adjudicator. The algorithm was evaluated based on 1680 notes against the human-coded result as the reference standard, with half of the data used for training and the other half for validation. RESULTS: The symptoms most commonly present within the 1680 clinical records at the episode level were those typically present in respiratory infections such as cough (744/7703, 9.66%), sore throat (591/7703, 7.67%), rhinorrhea (552/7703, 7.17%), and fever (928/7703, 12.04%). At the episode level, CHESS had an overall performance of 96.7% precision and 97.6% recall on the training dataset and 96.0% precision and 93.1% recall on the validation dataset. Symptoms suggesting respiratory and gastrointestinal infections were all detected with more than 90% precision and recall. CHESS correctly assigned the assertion status in 97.3%, 97.9%, and 89.8% of affirmed, negated, and suspected signs and symptoms, respectively (97.6% overall accuracy). Symptom episode duration was correctly identified in 81.2% of records with known duration status. CONCLUSIONS: We have developed an natural language processing algorithm dubbed CHESS that achieves good performance in extracting signs and symptoms from primary care free-text clinical records. In addition to the presence of symptoms, our algorithm can also accurately distinguish affirmed, negated, and suspected assertion statuses and extract symptom durations.",communicable diseases | electronic health records | epidemiology | natural language processing | surveillance | syndromic surveillance
"BACKGROUND: Free-text imposes a challenge in health data analysis since the lack of structure makes the extraction and integration of information difficult, particularly in the case of massive data. An appropriate machine-interpretation of electronic health records in Chile can unleash knowledge contained in large volumes of clinical texts, expanding clinical management and national research capabilities. AIM: To illustrate the use of a weighted frequency algorithm to find keywords. This finding was carried out in the diagnostic suspicion field of the Chilean specialty consultation waiting list, for diseases not covered by the Chilean Explicit Health Guarantees plan. MATERIAL AND METHODS: The waiting lists for a first specialty consultation for the period 2008-2018 were obtained from 17 out of 29 Chilean health services, and total of 2,592,925 diagnostic suspicions were identified. A natural language processing technique called Term Frequency-Inverse Document Frequency was used for the retrieval of diagnostic suspicion keywords. RESULTS: For each specialty, four key words with the highest weighted frequency were determined. Word clouds showing words weighted by their importance were created to obtain a visual representation. These are available at cimt.uchile.cl/lechile/. CONCLUSIONS: The algorithm allowed to summarize unstructured clinical free-text data, improving its usefulness and accessibility.",
"Documenting clinical notes in electronic health records might affect physician's workflow. In this paper, an Ontology-based clinical information extraction system, OB-CIE, has been developed. OB-CIE system provides a method for extracting clinical concepts from physician's free-text notes and converts the unstructured clinical notes to structured information to be accessed in electronic health records. OB-CIE system can help physicians to document visit notes without changing their workflow. For recognizing named entities of clinical concepts, ontology concepts have been used to construct a dictionary of semantic categories, then, exact dictionary matching method has been used to match noun phrases to their semantic categories. A rule-based approach has been used to classify clinical sentences to their predefined categories. The system evaluation results have achieved an F-measure of 94.90% and 97.80% for concepts classification and sentences classification, respectively. The results have showed that OB-CIE system performed well on extracting clinical concepts compared with data mining techniques. The system can be used in another field by adapting its ontology and extraction rule set.",Electronic health records | Information extraction | Natural language processing
"OBJECTIVE: To develop a comprehensive value set for documenting and encoding adverse reactions in the allergy module of an electronic health record. MATERIALS AND METHODS: We analyzed 2 471 004 adverse reactions stored in Partners Healthcare's Enterprise-wide Allergy Repository (PEAR) of 2.7 million patients. Using the Medical Text Extraction, Reasoning, and Mapping System, we processed both structured and free-text reaction entries and mapped them to Systematized Nomenclature of Medicine - Clinical Terms. We calculated the frequencies of reaction concepts, including rare, severe, and hypersensitivity reactions. We compared PEAR concepts to a Federal Health Information Modeling and Standards value set and University of Nebraska Medical Center data, and then created an integrated value set. RESULTS: We identified 787 reaction concepts in PEAR. Frequently reported reactions included: rash (14.0%), hives (8.2%), gastrointestinal irritation (5.5%), itching (3.2%), and anaphylaxis (2.5%). We identified an additional 320 concepts from Federal Health Information Modeling and Standards and the University of Nebraska Medical Center to resolve gaps due to missing and partial matches when comparing these external resources to PEAR. This yielded 1106 concepts in our final integrated value set. The presence of rare, severe, and hypersensitivity reactions was limited in both external datasets. Hypersensitivity reactions represented roughly 20% of the reactions within our data. DISCUSSION: We developed a value set for encoding adverse reactions using a large dataset from one health system, enriched by reactions from 2 large external resources. This integrated value set includes clinically important severe and hypersensitivity reactions. CONCLUSION: This work contributes a value set, harmonized with existing data, to improve the consistency and accuracy of reaction documentation in electronic health records, providing the necessary building blocks for more intelligent clinical decision support for allergies and adverse reactions.",
"General practice in the United Kingdom has been using electronic health records for over two decades, but coding clinical information remains poor. Lack of interest and training are considerable barriers preventing code use levels improvement. Tailored training could be the way forward, to break barriers in the uptake of coding; to do so it is paramount to understand coding use of the particular clinicians, to recognise their needs. It should be possible to easily assess text quantity and quality in medical consultations. A tool to measure these parameters, which could be used to tailor training needs and assess change, is demonstrated. The tool is presented and a preliminary study using a randomised sample of five recent consultations from thirteen different clinicians is used as an example. The tool, based on using a word processor and a spread-sheet, allowed quantitative analysis among clinicians while word clouds permitted a qualitative comparison between coded and free text. The average amount of free text per consultation was 68.2 words, (ranging from 25.4 and 130.2 among clinicians); an average of 6% of the text was coded (ranging from 0 to 13%). Patterns among clinicians could be identified. Using Word cloud, a different text use was demonstrated depending on its purpose. Some free text could be turned into code but nomenclature probably prevented some of the codings, like the expression of time. This proof of concept demonstrated that it is possible to calculate what percentage of consultations are coded and what codes are used. This allowed understanding clinicians' preferences; training needs and gaps in nomenclature.",Clinical coding | Electronic health record | Family practice | General practice | Records | Systematized Nomenclature of Medicine
"The ability of caregivers and investigators to share patient data is fundamental to many areas of clinical practice and biomedical research. Prior to sharing, it is often necessary to remove identifiers such as names, contact details, and dates in order to protect patient privacy. Deidentification, the process of removing identifiers, is challenging, however. High-quality annotated data for developing models is scarce; many target identifiers are highly heterogenous (for example, there are uncountable variations of patient names); and in practice anything less than perfect sensitivity may be considered a failure. As a result, patient data is often withheld when sharing would be beneficial, and identifiable patient data is often divulged when a deidentified version would suffice. In recent years, advances in machine learning methods have led to rapid performance improvements in natural language processing tasks, in particular with the advent of large-scale pretrained language models. In this paper we develop and evaluate an approach for deidentification of clinical notes based on a bidirectional transformer model. We propose human interpretable evaluation measures and demonstrate state of the art performance against modern baseline models. Finally, we highlight current challenges in deidentification, including the absence of clear annotation guidelines, lack of portability of models, and paucity of training data. Code to develop our model is open source, allowing for broad reuse.",HIPAA | PHI | deidentification | electronic health records | named entity recognition | natural language processing | neural networks
"OBJECTIVE: Accurate food adverse sensitivity documentation in electronic health records (EHRs) is crucial to patient safety. This study examined, encoded, and grouped foods that caused any adverse sensitivity in a large allergy repository using natural language processing and standard terminologies. METHODS: Using the Medical Text Extraction, Reasoning, and Mapping System (MTERMS), we processed both structured and free-text entries stored in an enterprise-wide allergy repository (Partners' Enterprise-wide Allergy Repository), normalized diverse food allergen terms into concepts, and encoded these concepts using the Systematized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) and Unique Ingredient Identifiers (UNII) terminologies. Concept coverage also was assessed for these two terminologies. We further categorized allergen concepts into groups and calculated the frequencies of these concepts by group. Finally, we conducted an external validation of MTERMS's performance when identifying food allergen terms, using a randomized sample from a different institution. RESULTS: We identified 158 552 food allergen records (2140 unique terms) in the Partners repository, corresponding to 672 food allergen concepts. High-frequency groups included shellfish (19.3%), fruits or vegetables (18.4%), dairy (9.0%), peanuts (8.5%), tree nuts (8.5%), eggs (6.0%), grains (5.1%), and additives (4.7%). Ambiguous, generic concepts such as ""nuts"" and ""seafood"" accounted for 8.8% of the records. SNOMED-CT covered more concepts than UNII in terms of exact (81.7% vs 68.0%) and partial (14.3% vs 9.7%) matches. DISCUSSION: Adverse sensitivities to food are diverse, and existing standard terminologies have gaps in their coverage of the breadth of allergy concepts. CONCLUSION: New strategies are needed to represent and standardize food adverse sensitivity concepts, to improve documentation in EHRs.",allergy and immunology | controlled | electronic health records | food hypersensitivity | natural language processing | systematized nomenclature of medicine | vocabulary
"BACKGROUND: Electronic health records store large amounts of patient clinical data. Despite efforts to structure patient data, clinical notes containing rich patient information remain stored as free text, greatly limiting its exploitation. This includes family history, which is highly relevant for applications such as diagnosis and prognosis. OBJECTIVE: This study aims to develop automatic strategies for annotating family history information in clinical notes, focusing not only on the extraction of relevant entities such as family members and disease mentions but also on the extraction of relations between the identified entities. METHODS: This study extends a previous contribution for the 2019 track on family history extraction from national natural language processing clinical challenges by improving a previously developed rule-based engine, using deep learning (DL) approaches for the extraction of entities from clinical notes, and combining both approaches in a hybrid end-to-end system capable of successfully extracting family member and observation entities and the relations between those entities. Furthermore, this study analyzes the impact of factors such as the use of external resources and different types of embeddings in the performance of DL models. RESULTS: The approaches developed were evaluated in a first task regarding entity extraction and in a second task concerning relation extraction. The proposed DL approach improved observation extraction, obtaining F(1) scores of 0.8688 and 0.7907 in the training and test sets, respectively. However, DL approaches have limitations in the extraction of family members. The rule-based engine was adjusted to have higher generalizing capability and achieved family member extraction F(1) scores of 0.8823 and 0.8092 in the training and test sets, respectively. The resulting hybrid system obtained F(1) scores of 0.8743 and 0.7979 in the training and test sets, respectively. For the second task, the original evaluator was adjusted to perform a more exact evaluation than the original one, and the hybrid system obtained F(1) scores of 0.6480 and 0.5082 in the training and test sets, respectively. CONCLUSIONS: We evaluated the impact of several factors on the performance of DL models, and we present an end-to-end system for extracting family history information from clinical notes, which can help in the structuring and reuse of this type of information. The final hybrid solution is provided in a publicly available code repository.",clinical notes | contextual embeddings | deep learning | electronic health record | family medical history | information extraction | natural language processing | rule-based | word embeddings
"BACKGROUND: Reliably abstracting outcomes from free-text electronic health records remains a challenge. While automated classification of free text has been a popular medical informatics topic, performance validation using real-world clinical data has been limited. The two main approaches are linguistic (natural language processing [NLP]) and statistical (machine learning). The authors have developed a hybrid system for abstracting computed tomography (CT) reports for specified outcomes. OBJECTIVES: The objective was to measure performance of a hybrid NLP and machine learning system for automated outcome classification of emergency department (ED) CT imaging reports. The hypothesis was that such a system is comparable to medical personnel doing the data abstraction. METHODS: A secondary analysis was performed on a prior diagnostic imaging study on 3,710 blunt facial trauma victims. Staff radiologists dictated CT reports as free text, which were then deidentified. A trained data abstractor manually coded the reference standard outcome of acute orbital fracture, with a random subset double-coded for reliability. The data set was randomly split evenly into training and testing sets. Training patient reports were used as input to the Medical Language Extraction and Encoding (MedLEE) NLP tool to create structured output containing standardized medical terms and modifiers for certainty and temporal status. Findings were filtered for low certainty and past/future modifiers and then combined with the manual reference standard to generate decision tree classifiers using data mining tools Waikato Environment for Knowledge Analysis (WEKA) 3.7.5 and Salford Predictive Miner 6.6. Performance of decision tree classifiers was evaluated on the testing set with or without NLP processing. RESULTS: The performance of machine learning alone was comparable to prior NLP studies (sensitivity = 0.92, specificity = 0.93, precision = 0.95, recall = 0.93, f-score = 0.94), and the combined use of NLP and machine learning showed further improvement (sensitivity = 0.93, specificity = 0.97, precision = 0.97, recall = 0.96, f-score = 0.97). This performance is similar to, or better than, that of medical personnel in previous studies. CONCLUSIONS: A hybrid NLP and machine learning automated classification system shows promise in coding free-text electronic clinical data.",
"BACKGROUND: The identification of sections in narrative content of Electronic Health Records (EHR) has demonstrated to improve the performance of clinical extraction tasks; however, there is not yet a shared understanding of the concept and its existing methods. The objective is to report the results of a systematic review concerning approaches aimed at identifying sections in narrative content of EHR, using both automatic or semi-automatic methods. METHODS: This review includes articles from the databases: SCOPUS, Web of Science and PubMed (from January 1994 to September 2018). The selection of studies was done using predefined eligibility criteria and applying the PRISMA recommendations. Search criteria were elaborated by using an iterative and collaborative keyword enrichment. RESULTS: Following the eligibility criteria, 39 studies were selected for analysis. The section identification approaches proposed by these studies vary greatly depending on the kind of narrative, the type of section, and the application. We observed that 57% of them proposed formal methods for identifying sections and 43% adapted a previously created method. Seventy-eight percent were intended for English texts and 41% for discharge summaries. Studies that are able to identify explicit (with headings) and implicit sections correspond to 46%. Regarding the level of granularity, 54% of the studies are able to identify sections, but not subsections. From the technical point of view, the methods can be classified into rule-based methods (59%), machine learning methods (22%) and a combination of both (19%). Hybrid methods showed better results than those relying on pure machine learning approaches, but lower than rule-based methods; however, their scope was more ambitious than the latter ones. Despite all the promising performance results, very few studies reported tests under a formal setup. Almost all the studies relied on custom dictionaries; however, they used them in conjunction with a controlled terminology, most commonly the UMLSⓇ metathesaurus. CONCLUSIONS: Identification of sections in EHR narratives is gaining popularity for improving clinical extraction projects. This study enabled the community working on clinical NLP to gain a formal analysis of this task, including the most successful ways to perform it.",Clinical narrative | Electronic health record | Free text | Machine learning | Natural language processing | Section identification
"Free text fields are often used to store clinical drug data in electronic health records. The use of free text facilitates rapid data entry by the clinician. Errors in spelling, abbreviations, and jargon, however, limit the utility of these data. We designed and implemented an algorithm, using open source tools and RxNorm, to extract and normalize drug data stored in free text fields of an anesthesia electronic health record. The algorithm was developed using a training set containing drug data from 49,518 cases, and validated using a validation set containing data from 14,655 cases. Overall sensitivity and specificity for the validation set were 92.2% and 95.7% respectively. The mains sources of error were misspellings and unknown but valid drug names. These preliminary results demonstrate that free text clinical drug data can be efficiently extracted and mapped to a controlled drug nomenclature.",
"BACKGROUND: Tuberculosis (TB) is a major cause of death worldwide. TB research draws heavily on clinical cohorts which can be generated using electronic health records (EHR), but granular information extracted from unstructured EHR data is limited. The St. Michael's Hospital TB database (SMH-TB) was established to address gaps in EHR-derived TB clinical cohorts and provide researchers and clinicians with detailed, granular data related to TB management and treatment. METHODS: We collected and validated multiple layers of EHR data from the TB outpatient clinic at St. Michael's Hospital, Toronto, Ontario, Canada to generate the SMH-TB database. SMH-TB contains structured data directly from the EHR, and variables generated using natural language processing (NLP) by extracting relevant information from free-text within clinic, radiology, and other notes. NLP performance was assessed using recall, precision and F1 score averaged across variable labels. We present characteristics of the cohort population using binomial proportions and 95% confidence intervals (CI), with and without adjusting for NLP misclassification errors. RESULTS: SMH-TB currently contains retrospective patient data spanning 2011 to 2018, for a total of 3298 patients (N = 3237 with at least 1 associated dictation). Performance of TB diagnosis and medication NLP rulesets surpasses 93% in recall, precision and F1 metrics, indicating good generalizability. We estimated 20% (95% CI: 18.4-21.2%) were diagnosed with active TB and 46% (95% CI: 43.8-47.2%) were diagnosed with latent TB. After adjusting for potential misclassification, the proportion of patients diagnosed with active and latent TB was 18% (95% CI: 16.8-19.7%) and 40% (95% CI: 37.8-41.6%) respectively. CONCLUSION: SMH-TB is a unique database that includes a breadth of structured data derived from structured and unstructured EHR data by using NLP rulesets. The data are available for a variety of research applications, such as clinical epidemiology, quality improvement and mathematical modeling studies.",
"OBJECTIVE: Geriatric syndromes such as functional disability and lack of social support are often not encoded in electronic health records (EHRs), thus obscuring the identification of vulnerable older adults in need of additional medical and social services. In this study, we automatically identify vulnerable older adult patients with geriatric syndrome based on clinical notes extracted from an EHR system, and demonstrate how contextual information can improve the process. MATERIALS AND METHODS: We propose a novel end-to-end neural architecture to identify sentences that contain geriatric syndromes. Our model learns a representation of the sentence and augments it with contextual information: surrounding sentences, the entire clinical document, and the diagnosis codes associated with the document. We trained our system on annotated notes from 85 patients, tuned the model on another 50 patients, and evaluated its performance on the rest, 50 patients. RESULTS: Contextual information improved classification, with the most effective context coming from the surrounding sentences. At sentence level, our best performing model achieved a micro-F1 of 0.605, significantly outperforming context-free baselines. At patient level, our best model achieved a micro-F1 of 0.843. DISCUSSION: Our solution can be used to expand the identification of vulnerable older adults with geriatric syndromes. Since functional and social factors are often not captured by diagnosis codes in EHRs, the automatic identification of the geriatric syndrome can reduce disparities by ensuring consistent care across the older adult population. CONCLUSION: EHR free-text can be used to identify vulnerable older adults with a range of geriatric syndromes.",clinical notes | deep neural network | electronic health records | geriatric syndrome | natural language processing | sentence classification | vulnerable geriatric population
"Patients with multiple disorders usually have long diagnosis lists, constitute by ICD-10 codes together with individual free-text descriptions. These text snippets are produced by overwriting standardized ICD-Code topics by the physicians at the point of care. They provide highly compact expert descriptions within a 50-character long text field frequently not assigned to a specific ICD-10 code. The high redundancy of these lists would benefit from content-based categorization within different hospital-based application scenarios. This work demonstrates how to accurately group diagnosis lists via a combination of natural language processing and hierarchical clustering with an overall F-measure value of 0.87. In addition, it compresses the initial diagnosis list up to 89%. The manuscript discusses pitfall and challenges as well as the potential of a large-scale approach for tackling this problem.",Cluster Analysis | Electronic Health Records | Natural Language Processing | Semantics
"Background. Family health history (FHH) can be used to identify individuals at elevated risk for familial cancers. Risk criteria for common cancers rely on age of onset, which is documented inconsistently as structured and unstructured data in electronic health records (EHRs). Objective. To investigate a natural language processing (NLP) approach to extract age of onset and age of death from free-text EHR fields. Methods. Using 474,651 FHH entries from 89,814 patients, we investigated two methods - frequent patterns (baseline) and NLP classifier. Results. For age of onset, the NLP classifier outperformed the baseline in precision (96% vs. 83%; 95% CI [94, 97] and [80, 86]) with equivalent recall (both 93%; 95% CI [91, 95]). When applied to the full dataset, the NLP approach increased the percentage of FHH entries for which cancer risk criteria could be applied from 10% to 15%. Conclusion. NLP combined with structured data may improve the computation of familial cancer risk criteria for various use cases.",
"Clinical documentation using free text to describe a patient's medical status is an essential component of electronic health records (EHRs), and the quality of information in documents plays a critical role in clinical practice and translational research. Physicians are the primary creators of EHRs, but their clinical practices vary substantially, resulting in variations in clinical documentation. These variations can represent a source for potential bias in clinical outcomes and downstream applications using EHRs. Asthma is one example, presenting an inconsistent ascertainment process and criteria. A recent study revealed that resident physicians' knowledge of asthma diagnosis and management is relatively limited. In this study, we examined clinical documentation variations in asthma care between staff and resident physicians using individual words, topics, and asthma-related concepts in EHR clinical narratives. Additionally, we discuss potential biases in building an informatics model and further compare asthma diagnosis and outcomes between two physician groups.",Asthma | Documentation | Electronic Health Records
"BACKGROUND: Oxford Mental Illness and Suicide tool (OxMIS) is a brief, scalable, freely available, structured risk assessment tool to assess suicide risk in patients with severe mental illness (schizophrenia-spectrum disorders or bipolar disorder). OxMIS requires further external validation, but a lack of large-scale cohorts with relevant variables makes this challenging. Electronic health records provide possible data sources for external validation of risk prediction tools. However, they contain large amounts of information within free-text that is not readily extractable. In this study, we examined the feasibility of identifying suicide predictors needed to validate OxMIS in routinely collected electronic health records. METHODS: In study 1, we manually reviewed electronic health records of 57 patients with severe mental illness to calculate OxMIS risk scores. In study 2, we examined the feasibility of using natural language processing to scale up this process. We used anonymized free-text documents from the Clinical Record Interactive Search database to train a named entity recognition model, a machine learning technique which recognizes concepts in free-text. The model identified eight concepts relevant for suicide risk assessment: medication (antidepressant/antipsychotic treatment), violence, education, self-harm, benefits receipt, drug/alcohol use disorder, suicide, and psychiatric admission. We assessed model performance in terms of precision (similar to positive predictive value), recall (similar to sensitivity) and F1 statistic (an overall performance measure). RESULTS: In study 1, we estimated suicide risk for all patients using the OxMIS calculator, giving a range of 12 month risk estimates from 0.1-3.4%. For 13 out of 17 predictors, there was no missing information in electronic health records. For the remaining 4 predictors missingness ranged from 7-26%; to account for these missing variables, it was possible for OxMIS to estimate suicide risk using a range of scores. In study 2, the named entity recognition model had an overall precision of 0.77, recall of 0.90 and F1 score of 0.83. The concept with the best precision and recall was medication (precision 0.84, recall 0.96) and the weakest were suicide (precision 0.37), and drug/alcohol use disorder (recall 0.61). CONCLUSIONS: It is feasible to estimate suicide risk with the OxMIS tool using predictors identified in routine clinical records. Predictors could be extracted using natural language processing. However, electronic health records differ from other data sources, particularly for family history variables, which creates methodological challenges.",OxMIS | bipolar disorder | electronic health records | feasibility | natural language processing | risk assessment | schizophrenia | suicide
"Eligibility criteria are important for clinical research protocols or clinical practice guidelines for determining who qualify for studies and to whom clinical evidence is applicable, but the free-text format is not amenable for computational processing. In this paper, we described a practical method for transforming free-text clinical research eligibility criteria of Alzheimer's clinical trials into a structured relational database compliant with standards for medical terminologies and clinical data models. We utilized a hybrid natural language processing system and a concept normalization tool to extract medical terms in clinical research eligibility criteria and represent them using the OMOP Common Data Model (CDM) v5. We created a database schema design to store syntactic relations to facilitate efficient cohort queries. We further discussed the potential of applying this method to trials on other diseases and the promise of using it to accelerate clinical research with electronic health records.",Clinical Research Informatics | Electronic Health Record | Relational Data Management
"BACKGROUND: Socioeconomically disadvantaged newborns receive care from primary care providers (PCPs) and Women, Infants, and Children (WIC) nutritionists. However, care is not coordinated between these settings, which can result in conflicting messages. Stakeholders support an integrated approach that coordinates services between settings with care tailored to patient-centered needs. OBJECTIVE: This analysis describes the usability of advanced health information technologies aiming to engage parents in self-reporting parenting practices, integrate data into electronic health records to inform and facilitate documentation of provided responsive parenting (RP) care, and share data between settings to create opportunities to coordinate care between PCPs and WIC nutritionists. METHODS: Parents and newborns (dyads) who were eligible for WIC care and received pediatric care in a single health system were recruited and randomized to a RP intervention or control group. For the 6-month intervention, electronic systems were created to facilitate documentation, data sharing, and coordination of provided RP care. Prior to PCP visits, parents were prompted to respond to the Early Healthy Lifestyles (EHL) self-assessment tool to capture current RP practices. Responses were integrated into the electronic health record and shared with WIC. Documentation of RP care and an 80-character, free-text comment were shared between WIC and PCPs. A care coordination opportunity existed when the dyad attended a WIC visit and these data were available from the PCP, and vice versa. Care coordination was demonstrated when WIC or PCPs interacted with data and documented RP care provided at the visit. RESULTS: Dyads (N=131) attended 459 PCP (3.5, SD 1.0 per dyad) and 296 WIC (2.3, SD 1.0 per dyad) visits. Parents completed the EHL tool prior to 53.2% (244/459) of PCP visits (1.9, SD 1.2 per dyad), PCPs documented provided RP care at 35.3% (162/459) of visits, and data were shared with WIC following 100% (459/459) of PCP visits. A WIC visit followed a PCP visit 50.3% (231/459) of the time; thus, there were 1.8 (SD 0.8 per dyad) PCP to WIC care coordination opportunities. WIC coordinated care by documenting RP care at 66.7% (154/231) of opportunities (1.2, SD 0.9 per dyad). WIC visits were followed by a PCP visit 58.9% (116/197) of the time; thus, there were 0.9 (SD 0.8 per dyad) WIC to PCP care coordination opportunities. PCPs coordinated care by documenting RP care at 44.0% (51/116) of opportunities (0.4, SD 0.6 per dyad). CONCLUSIONS: Results support the usability of advanced health information technology strategies to collect patient-reported data and share these data between multiple providers. Although PCPs and WIC shared data, WIC nutritionists were more likely to use data and document RP care to coordinate care than PCPs. Variability in timing, sequence, and frequency of visits underscores the need for flexibility in pragmatic studies. TRIAL REGISTRATION: ClinicalTrials.gov NCT03482908; https://clinicaltrials.gov/ct2/show/NCT03482908. INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID): RR2-10.1186/s12887-018-1263-z.",clinical care | coordination of care | data sharing | early obesity prevention | health information technology | pragmatic intervention | responsive parenting
"OBJECTIVES: The radiology report is the most important source of clinical imaging information. It documents critical information about the patient's health and the radiologist's interpretation of medical findings. It also communicates information to the referring physicians and records that information for future clinical and research use. Although efforts to structure some radiology report information through predefined templates are beginning to bear fruit, a large portion of radiology report information is entered in free text. The free text format is a major obstacle for rapid extraction and subsequent use of information by clinicians, researchers, and healthcare information systems. This difficulty is due to the ambiguity and subtlety of natural language, complexity of described images, and variations among different radiologists and healthcare organizations. As a result, radiology reports are used only once by the clinician who ordered the study and rarely are used again for research and data mining. In this work, machine learning techniques and a large multi-institutional radiology report repository are used to extract the semantics of the radiology report and overcome the barriers to the re-use of radiology report information in clinical research and other healthcare applications. MATERIAL AND METHODS: We describe a machine learning system to annotate radiology reports and extract report contents according to an information model. This information model covers the majority of clinically significant contents in radiology reports and is applicable to a wide variety of radiology study types. Our automated approach uses discriminative sequence classifiers for named-entity recognition to extract and organize clinically significant terms and phrases consistent with the information model. We evaluated our information extraction system on 150 radiology reports from three major healthcare organizations and compared its results to a commonly used non-machine learning information extraction method. We also evaluated the generalizability of our approach across different organizations by training and testing our system on data from different organizations. RESULTS: Our results show the efficacy of our machine learning approach in extracting the information model's elements (10-fold cross-validation average performance: precision: 87%, recall: 84%, F1 score: 85%) and its superiority and generalizability compared to the common non-machine learning approach (p-value<0.05). CONCLUSIONS: Our machine learning information extraction approach provides an effective automatic method to annotate and extract clinically significant information from a large collection of free text radiology reports. This information extraction system can help clinicians better understand the radiology reports and prioritize their review process. In addition, the extracted information can be used by researchers to link radiology reports to information from other data sources such as electronic health records and the patient's genome. Extracted information also can facilitate disease surveillance, real-time clinical decision support for the radiologist, and content-based image retrieval.",Discriminative sequence classifier | Information extraction | Natural language processing | Radiology report narrative
"Research into suicide prevention has been hampered by methodological limitations such as low sample size and recall bias. Recently, Natural Language Processing (NLP) strategies have been used with Electronic Health Records to increase information extraction from free text notes as well as structured fields concerning suicidality and this allows access to much larger cohorts than previously possible. This paper presents two novel NLP approaches - a rule-based approach to classify the presence of suicide ideation and a hybrid machine learning and rule-based approach to identify suicide attempts in a psychiatric clinical database. Good performance of the two classifiers in the evaluation study suggest they can be used to accurately detect mentions of suicide ideation and attempt within free-text documents in this psychiatric database. The novelty of the two approaches lies in the malleability of each classifier if a need to refine performance, or meet alternate classification requirements arises. The algorithms can also be adapted to fit infrastructures of other clinical datasets given sufficient clinical recording practice knowledge, without dependency on medical codes or additional data extraction of known risk factors to predict suicidal behaviour.",
"BACKGROUND: Electronic health records (EHRs) have potential to facilitate reliable communication and follow-up of test results. However, limitations in EHR functionality remain, leading practitioners to use workarounds while managing test results. Workarounds can lead to patient safety concerns and signify indications as to how to build better EHR systems that meet provider needs. OBJECTIVE: To understand why primary care practitioners (PCPs) use workarounds to manage test results by analyzing data from a previously conducted national cross-sectional survey on test result management. METHODS: We conducted a secondary data analysis of quantitative and qualitative data from a national survey of PCPs practicing in the Department of Veterans Affairs (VA) and explored the use of workarounds in test results management. We used multivariate logistic regression analysis to examine the association between key sociotechnical factors that could affect test results follow-up (e.g., both technology-related and those unrelated to technology, such as organizational support for patient notification) and workaround use. We conducted a qualitative content analysis of free text survey data to examine reasons for use of workarounds. RESULTS: Of 2554 survey respondents, 1104 (43%) reported using workarounds related to test results management. Of these 1028 (93%) described the type of workaround they were using; 719 (70%) reported paper-based methods, while 230 (22%) used a combination of paper- and computer-based workarounds. Primary care practitioners who self-reported limited administrative support to help them notify patients of test results or described an instance where they personally (or a colleague) missed results, were more likely to use workarounds (p=0.02 and p=0.001, respectively). Qualitative analysis identified three main reasons for workaround use: 1) as a memory aid, 2) for improved efficiency and 3) for facilitating internal and external care coordination. CONCLUSION: Workarounds to manage EHR-based test results are common, and their use results from unmet provider information management needs. Future EHRs and the respective work systems around them need to evolve to meet these needs.",Workarounds | diagnostic test result follow-up | missed test results | paper-based methods
"Introduction: Despite the growing efforts to standardize coding for social determinants of health (SDOH), they are infrequently captured in electronic health records (EHRs). Most SDOH variables are still captured in the unstructured fields (i.e., free-text) of EHRs. In this study we attempt to evaluate a practical text mining approach (i.e., advanced pattern matching techniques) in identifying phrases referring to housing issues, an important SDOH domain affecting value-based healthcare providers, using EHR of a large multispecialty medical group in the New England region, United States. To present how this approach would help the health systems to address the SDOH challenges of their patients we assess the demographic and clinical characteristics of patients with and without housing issues and briefly look into the patterns of healthcare utilization among the study population and for those with and without housing challenges. Methods: We identified five categories of housing issues [i.e., homelessness current (HC), homelessness history (HH), homelessness addressed (HA), housing instability (HI), and building quality (BQ)] and developed several phrases addressing each one through collaboration with SDOH experts, consulting the literature, and reviewing existing coding standards. We developed pattern-matching algorithms (i.e., advanced regular expressions), and then applied them in the selected EHR. We assessed the text mining approach for recall (sensitivity) and precision (positive predictive value) after comparing the identified phrases with manually annotated free-text for different housing issues. Results: The study dataset included EHR structured data for a total of 20,342 patients and 2,564,344 free-text clinical notes. The mean (SD) age in the study population was 75.96 (7.51). Additionally, 58.78% of the cohort were female. BQ and HI were the most frequent housing issues documented in EHR free-text notes and HH was the least frequent one. The regular expression methodology, when compared to manual annotation, had a high level of precision (positive predictive value) at phrase, note, and patient levels (96.36, 95.00, and 94.44%, respectively) across different categories of housing issues, but the recall (sensitivity) rate was relatively low (30.11, 32.20, and 41.46%, respectively). Conclusion: Results of this study can be used to advance the research in this domain, to assess the potential value of EHR's free-text in identifying patients with a high risk of housing issues, to improve patient care and outcomes, and to eventually mitigate socioeconomic disparities across individuals and communities.",clinical notes | electronic health record | free-text | housing | natural language processing | social determinants of health
"OBJECTIVE: To develop a cost-effective, case-based reasoning framework for clinical research eligibility screening by only reusing the electronic health records (EHRs) of minimal enrolled participants to represent the target patient for each trial under consideration. MATERIALS AND METHODS: The EHR data--specifically diagnosis, medications, laboratory results, and clinical notes--of known clinical trial participants were aggregated to profile the ""target patient"" for a trial, which was used to discover new eligible patients for that trial. The EHR data of unseen patients were matched to this ""target patient"" to determine their relevance to the trial; the higher the relevance, the more likely the patient was eligible. Relevance scores were a weighted linear combination of cosine similarities computed over individual EHR data types. For evaluation, we identified 262 participants of 13 diversified clinical trials conducted at Columbia University as our gold standard. We ran a 2-fold cross validation with half of the participants used for training and the other half used for testing along with other 30 000 patients selected at random from our clinical database. We performed binary classification and ranking experiments. RESULTS: The overall area under the ROC curve for classification was 0.95, enabling the highlight of eligible patients with good precision. Ranking showed satisfactory results especially at the top of the recommended list, with each trial having at least one eligible patient in the top five positions. CONCLUSIONS: This relevance-based method can potentially be used to identify eligible patients for clinical trials by processing patient EHR data alone without parsing free-text eligibility criteria, and shows promise of efficient ""case-based reasoning"" modeled only on minimal trial participants.",artificial intelligence | clinical trials | electronic health records | information storage and retrieval
"BACKGROUND: Acute and chronic low back pain (LBP) are different conditions with different treatments. However, they are coded in electronic health records with the same International Classification of Diseases, 10th revision (ICD-10) code (M54.5) and can be differentiated only by retrospective chart reviews. This prevents an efficient definition of data-driven guidelines for billing and therapy recommendations, such as return-to-work options. OBJECTIVE: The objective of this study was to evaluate the feasibility of automatically distinguishing acute LBP episodes by analyzing free-text clinical notes. METHODS: We used a dataset of 17,409 clinical notes from different primary care practices; of these, 891 documents were manually annotated as acute LBP and 2973 were generally associated with LBP via the recorded ICD-10 code. We compared different supervised and unsupervised strategies for automated identification: keyword search, topic modeling, logistic regression with bag of n-grams and manual features, and deep learning (a convolutional neural network-based architecture [ConvNet]). We trained the supervised models using either manual annotations or ICD-10 codes as positive labels. RESULTS: ConvNet trained using manual annotations obtained the best results with an area under the receiver operating characteristic curve of 0.98 and an F score of 0.70. ConvNet's results were also robust to reduction of the number of manually annotated documents. In the absence of manual annotations, topic models performed better than methods trained using ICD-10 codes, which were unsatisfactory for identifying LBP acuity. CONCLUSIONS: This study uses clinical notes to delineate a potential path toward systematic learning of therapeutic strategies, billing guidelines, and management options for acute LBP at the point of care.",clinical notes | electronic health records | low back pain | machine learning | natural language processing
"The determination of vital signs is a fundamental aspect of patient care. Electronic health records have a structured format for their registration. It is known that the frequency with which this data is recollected is not representative of reality. To complement the missing data we have created a tool that extract the information regarding blood pressure, heart rate, respiratory rate, height, weight and pain level recorded in free text in the clinical notes of outpatients.",Electronic Health Records | Natural Language Processing | Vital Signs
"BACKGROUND: The penicillin adverse drug reaction (ADR) label is common in electronic health records (EHRs). However, there is significant misclassification between allergy and intolerance within the EHR and most patients can be delabelled after an immunologic assessment. Machine learning natural language processing may be able to assist with the categorisation and risk stratification of penicillin ADRs. OBJECTIVE: The aim of this study was to use text entered into an EHR to derive and evaluate machine learning models to classify penicillin ADRs and assess the risk of true allergy. METHODS: Machine learning natural language processing was applied to free-text penicillin ADR data extracted from a public health system EHR. The model was developed by training on labelled dataset. ADR entries were split into training and testing datasets and used to develop and test a variety of machine learning models. These were compared to categorisation with a simple algorithm using keyword search. RESULTS: The best performing model for the classification of penicillin ADRs as being consistent with allergy or intolerance was the artificial neural network (AUC 0.994, sensitivity 0.99, specificity 0.96). The artificial neural network also achieved the highest AUC in the classification of high- or low-risk of true allergy (AUC 0.988, sensitivity 0.99, specificity 0.99). All ADR labels were able to be classified using these machine learning models, whereas a small proportion were unclassifiable using the simple algorithm as they contained no keywords. CONCLUSION: Machine learning natural language processing performed similarly to expert criteria in classifying and risk stratifying penicillin ADRs labels. These models outperformed simpler algorithms in their ability to interpret free-text data contained in the EHR. The automated evaluation of penicillin ADR labels may allow real-time risk stratification to facilitate delabelling and improve the specificity of prescribing alerts.",Adverse drug reaction | Electronic health records | Machine learning | Natural language processing | Penicillin
"We describe the development and evaluation of a system that uses machine learning and natural language processing techniques to identify potential candidates for surgical intervention for drug-resistant pediatric epilepsy. The data are comprised of free-text clinical notes extracted from the electronic health record (EHR). Both known clinical outcomes from the EHR and manual chart annotations provide gold standards for the patient's status. The following hypotheses are then tested: 1) machine learning methods can identify epilepsy surgery candidates as well as physicians do and 2) machine learning methods can identify candidates earlier than physicians do. These hypotheses are tested by systematically evaluating the effects of the data source, amount of training data, class balance, classification algorithm, and feature set on classifier performance. The results support both hypotheses, with F-measures ranging from 0.71 to 0.82. The feature set, classification algorithm, amount of training data, class balance, and gold standard all significantly affected classification performance. It was further observed that classification performance was better than the highest agreement between two annotators, even at one year before documented surgery referral. The results demonstrate that such machine learning methods can contribute to predicting pediatric epilepsy surgery candidates and reducing lag time to surgery referral.",epilepsy | epilepsy surgery | machine learning | natural language processing | neurosurgery
"Problem: Clinical practice requires the production of a time- and resource-consuming great amount of notes. They contain relevant information, but their secondary use is almost impossible, due to their unstructured nature. Researchers are trying to address this problems, with traditional and promising novel techniques. Application in real hospital settings seems not to be possible yet, though, both because of relatively small and dirty dataset, and for the lack of language-specific pre-trained models. Aim: Our aim is to demonstrate the potential of the above techniques, but also raise awareness of the still open challenges that the scientific communities of IT and medical practitioners must jointly address to realize the full potential of unstructured content that is daily produced and digitized in hospital settings, both to improve its data quality and leverage the insights from data-driven predictive models. Methods: To this extent, we present a narrative literature review of the most recent and relevant contributions to leverage the application of Natural Language Processing techniques to the free-text content electronic patient records. In particular, we focused on four selected application domains, namely: data quality, information extraction, sentiment analysis and predictive models, and automated patient cohort selection. Then, we will present a few empirical studies that we undertook at a major teaching hospital specializing in musculoskeletal diseases. Results: We provide the reader with some simple and affordable pipelines, which demonstrate the feasibility of reaching literature performance levels with a single institution non-English dataset. In such a way, we bridged literature and real world needs, performing a step further toward the revival of notes fields.",clinical intelligence | data quality | information extraction | literature review | machine learning | natural language processing (NLP) | sentiment analysis | text mining
"Background Conventional prognostic scores usually require predefined clinical variables to predict outcome. The advancement of natural language processing has made it feasible to derive meaning from unstructured data. We aimed to test whether using unstructured text in electronic health records can improve the prediction of functional outcome after acute ischemic stroke. Methods and Results Patients hospitalized for acute ischemic stroke were identified from 2 hospital stroke registries (3847 and 2668 patients, respectively). Prediction models developed using the first cohort were externally validated using the second cohort, and vice versa. Free text in the history of present illness and computed tomography reports was used to build machine learning models using natural language processing to predict poor functional outcome at 90 days poststroke. Four conventional prognostic models were used as baseline models. The area under the receiver operating characteristic curves of the model using history of present illness in the internal and external validation sets were 0.820 and 0.792, respectively, which were comparable to the National Institutes of Health Stroke Scale score (0.811 and 0.807). The model using computed tomography reports achieved area under the receiver operating characteristic curves of 0.758 and 0.658. Adding information from clinical text significantly improved the predictive performance of each baseline model in terms of area under the receiver operating characteristic curves, net reclassification improvement, and integrated discrimination improvement indices (all P<0.001). Swapping the study cohorts led to similar results. Conclusions By using natural language processing, unstructured text in electronic health records can provide an alternative tool for stroke prognostication, and even enhance the performance of existing prognostic scores.",acute ischemic stroke | machine learning | natural language processing | outcome prediction | risk score
"OBJECTIVES: To examine the value of unstructured electronic health record (EHR) data (free-text notes) in identifying a set of geriatric syndromes. DESIGN: Retrospective analysis of unstructured EHR notes using a natural language processing (NLP) algorithm. SETTING: Large multispecialty group. PARTICIPANTS: Older adults (N=18,341; average age 75.9, 58.9% female). MEASUREMENTS: We compared the number of geriatric syndrome cases identified using structured claims and structured and unstructured EHR data. We also calculated these rates using a population-level claims database as a reference and identified comparable epidemiological rates in peer-reviewed literature as a benchmark. RESULTS: Using insurance claims data resulted in a geriatric syndrome prevalence ranging from 0.03% for lack of social support to 8.3% for walking difficulty. Using structured EHR data resulted in similar prevalence rates, ranging from 0.03% for malnutrition to 7.85% for walking difficulty. Incorporating unstructured EHR notes, enabled by applying the NLP algorithm, identified considerably higher rates of geriatric syndromes: absence of fecal control (2.1%, 2.3 times as much as structured claims and EHR data combined), decubitus ulcer (1.4%, 1.7 times as much), dementia (6.7%, 1.5 times as much), falls (23.6%, 3.2 times as much), malnutrition (2.5%, 18.0 times as much), lack of social support (29.8%, 455.9 times as much), urinary retention (4.2%, 3.9 times as much), vision impairment (6.2%, 7.4 times as much), weight loss (19.2%, 2.9 as much), and walking difficulty (36.34%, 3.4 as much). The geriatric syndrome rates extracted from structured data were substantially lower than published epidemiological rates, although adding the NLP results considerably closed this gap. CONCLUSION: Claims and structured EHR data give an incomplete picture of burden related to geriatric syndromes. Geriatric syndromes are likely to be missed if unstructured data are not analyzed. Pragmatic NLP algorithms can assist with identifying individuals at high risk of experiencing geriatric syndromes and improving coordination of care for older adults.",case identification | electronic health records | geriatric syndromes | natural language processing and text-mining | unstructured free-text data
"Routine patient data in electronic patient records are only partly structured, and an even smaller segment is coded, mainly for administrative purposes. Large parts are only available as free text. Transforming this content into a structured and semantically explicit form is a prerequisite for querying and information extraction. The core of the system architecture presented in this paper is based on SAP HANA in-memory database technology using the SAP Connected Health platform for data integration as well as for clinical data warehousing. A natural language processing pipeline analyses unstructured content and maps it to a standardized vocabulary within a well-defined information model. The resulting semantically standardized patient profiles are used for a broad range of clinical and research application scenarios.",Electronic Health Records | Natural Language Processing | Semantics
"BACKGROUND: The increasing adoption of electronic health records (EHRs) in clinical practice holds the promise of improving care and advancing research by serving as a rich source of data, but most EHRs allow clinicians to enter data in a text format without much structure. Natural language processing (NLP) may reduce reliance on manual abstraction of these text data by extracting clinical features directly from unstructured clinical digital text data and converting them into structured data. OBJECTIVE: This study aimed to assess the performance of a commercially available NLP tool for extracting clinical features from free-text consult notes. METHODS: We conducted a pilot, retrospective, cross-sectional study of the accuracy of NLP from dictated consult notes from our tuberculosis clinic with manual chart abstraction as the reference standard. Consult notes for 130 patients were extracted and processed using NLP. We extracted 15 clinical features from these consult notes and grouped them a priori into categories of simple, moderate, and complex for analysis. RESULTS: For the primary outcome of overall accuracy, NLP performed best for features classified as simple, achieving an overall accuracy of 96% (95% CI 94.3-97.6). Performance was slightly lower for features of moderate clinical and linguistic complexity at 93% (95% CI 91.1-94.4), and lowest for complex features at 91% (95% CI 87.3-93.1). CONCLUSIONS: The findings of this study support the use of NLP for extracting clinical features from dictated consult notes in the setting of a tuberculosis clinic. Further research is needed to fully establish the validity of NLP for this and other purposes.",electronic health record | natural language processing | tuberculosis
"OBJECTIVE: There is increasing interest in using electronic health records (EHRs) to identify subjects for genomic association studies, due in part to the availability of large amounts of clinical data and the expected cost efficiencies of subject identification. We describe the construction and validation of an EHR-based algorithm to identify subjects with age-related cataracts. MATERIALS AND METHODS: We used a multi-modal strategy consisting of structured database querying, natural language processing on free-text documents, and optical character recognition on scanned clinical images to identify cataract subjects and related cataract attributes. Extensive validation on 3657 subjects compared the multi-modal results to manual chart review. The algorithm was also implemented at participating electronic MEdical Records and GEnomics (eMERGE) institutions. RESULTS: An EHR-based cataract phenotyping algorithm was successfully developed and validated, resulting in positive predictive values (PPVs) >95%. The multi-modal approach increased the identification of cataract subject attributes by a factor of three compared to single-mode approaches while maintaining high PPV. Components of the cataract algorithm were successfully deployed at three other institutions with similar accuracy. DISCUSSION: A multi-modal strategy incorporating optical character recognition and natural language processing may increase the number of cases identified while maintaining similar PPVs. Such algorithms, however, require that the needed information be embedded within clinical documents. CONCLUSION: We have demonstrated that algorithms to identify and characterize cataracts can be developed utilizing data collected via the EHR. These algorithms provide a high level of accuracy even when implemented across multiple EHRs and institutional boundaries.",
"BACKGROUND: Electronic health records (EHRs) offer a wealth of observational data. Machine-learning (ML) methods are efficient at data extraction, capable of processing the information-rich free-text physician notes in EHRs. The clinical diagnosis contained therein represents physician expert opinion and is more consistently recorded than classification criteria components. OBJECTIVES: To investigate the overlap and differences between rheumatoid arthritis patients as identified either from EHR free-text through the extraction of the rheumatologist diagnosis using machine-learning (ML) or through manual chart-review applying the 1987 and 2010 RA classification criteria. METHODS: Since EHR initiation, 17,662 patients have visited the Leiden rheumatology outpatient clinic. For ML, we used a support vector machine (SVM) model to identify those who were diagnosed with RA by their rheumatologist. We trained and validated the model on a random selection of 2000 patients, balancing PPV and sensitivity to define a cutoff, and assessed performance on a separate 1000 patients. We then deployed the model on our entire patient selection (including the 3000). Of those, 1127 patients had both a 1987 and 2010 EULAR/ACR criteria status at 1 year after inclusion into the local prospective arthritis cohort. In these 1127 patients, we compared the patient characteristics of RA cases identified with ML and those fulfilling the classification criteria. RESULTS: The ML model performed very well in the independent test set (sensitivity=0.85, specificity=0.99, PPV=0.86, NPV=0.99). In our selection of patients with both EHR and classification information, 373 were recognized as RA by ML and 357 and 426 fulfilled the 1987 or 2010 criteria, respectively. Eighty percent of the ML-identified cases fulfilled at least one of the criteria sets. Both demographic and clinical parameters did not differ between the ML extracted cases and those identified with EULAR/ACR classification criteria. CONCLUSIONS: With ML methods, we enable fast patient extraction from the huge EHR resource. Our ML algorithm accurately identifies patients diagnosed with RA by their rheumatologist. This resulting group of RA patients had a strong overlap with patients identified using the 1987 or 2010 classification criteria and the baseline (disease) characteristics were comparable. ML-assisted case labeling enables high-throughput creation of inclusive patient selections for research purposes.",Artificial intelligence | Big data | Chart review | Classification criteria | EHR | Electronic health records | Machine learning algorithms | Observational research | Rheumatoid arthritis
"BACKGROUND: The secondary use of electronic health records (EHRs) promises to facilitate medical research. We reviewed general data requirements in observational studies and analyzed the feasibility of conducting observational studies with structured EHR data, in particular diagnosis and procedure codes. METHODS: After reviewing published observational studies from the University Hospital of Erlangen for general data requirements, we identified three different study populations for the feasibility analysis with eligibility criteria from three exemplary observational studies. For each study population, we evaluated the availability of relevant patient characteristics in our EHR, including outcome and exposure variables. To assess data quality, we computed distributions of relevant patient characteristics from the available structured EHR data and compared them to those of the original studies. We implemented computed phenotypes for patient characteristics where necessary. In random samples, we evaluated how well structured patient characteristics agreed with a gold standard from manually interpreted free texts. We categorized our findings using the four data quality dimensions ""completeness"", ""correctness"", ""currency"" and ""granularity"". RESULTS: Reviewing general data requirements, we found that some investigators supplement routine data with questionnaires, interviews and follow-up examinations. We included 847 subjects in the feasibility analysis (Study 1 n = 411, Study 2 n = 423, Study 3 n = 13). All eligibility criteria from two studies were available in structured data, while one study required computed phenotypes in eligibility criteria. In one study, we found that all necessary patient characteristics were documented at least once in either structured or unstructured data. In another study, all exposure and outcome variables were available in structured data, while in the other one unstructured data had to be consulted. The comparison of patient characteristics distributions, as computed from structured data, with those from the original study yielded similar distributions as well as indications of underreporting. We observed violations in all four data quality dimensions. CONCLUSIONS: While we found relevant patient characteristics available in structured EHR data, data quality problems may entail that it remains a case-by-case decision whether diagnosis and procedure codes are sufficient to underpin observational studies. Free-text data or subsequently supplementary study data may be important to complement a comprehensive patient history.",Availability | Completeness | Correctness | Currency | Data quality | Electronic health record | Granularity | Observational study | Retrospective study
"OBJECTIVE: A major source of information available in electronic health record (EHR) systems are the clinical free text notes documenting patient care. Managing this information is time-consuming for clinicians. Automatic text summarisation could assist clinicians in obtaining an overview of the free text information in ongoing care episodes, as well as in writing final discharge summaries. We present a study of automated text summarisation of clinical notes. It looks to identify which methods are best suited for this task and whether it is possible to automatically evaluate the quality differences of summaries produced by different methods in an efficient and reliable way. METHODS AND MATERIALS: The study is based on material consisting of 66,884 care episodes from EHRs of heart patients admitted to a university hospital in Finland between 2005 and 2009. We present novel extractive text summarisation methods for summarising the free text content of care episodes. Most of these methods rely on word space models constructed using distributional semantic modelling. The summarisation effectiveness is evaluated using an experimental automatic evaluation approach incorporating well-known ROUGE measures. We also developed a manual evaluation scheme to perform a meta-evaluation on the ROUGE measures to see if they reflect the opinions of health care professionals. RESULTS: The agreement between the human evaluators is good (ICC=0.74, p<0.001), demonstrating the stability of the proposed manual evaluation method. Furthermore, the correlation between the manual and automated evaluations are high (> 0.90 Spearman's rho). Three of the presented summarisation methods ('Composite', 'Case-Based' and 'Translate') significantly outperform the other methods for all ROUGE measures (p<0.05, Wilcoxon signed-rank test and Bonferroni correction). CONCLUSION: The results indicate the feasibility of the automated summarisation of care episodes. Moreover, the high correlation between manual and automated evaluations suggests that the less labour-intensive automated evaluations can be used as a proxy for human evaluations when developing summarisation methods. This is of significant practical value for summarisation method development, because manual evaluation cannot be afforded for every variation of the summarisation methods. Instead, one can resort to automatic evaluation during the method development process.",Automatic text summarisation | Clinical text processing | Distributional semantics | Electronic health records | Summarisation evaluation | Word space models
"BACKGROUND: Primary care databases are a major source of data for epidemiological and health services research. However, most studies are based on coded information, ignoring information stored in free text. Using the early presentation of rheumatoid arthritis (RA) as an exemplar, our objective was to estimate the extent of data hidden within free text, using a keyword search. METHODS: We examined the electronic health records (EHRs) of 6,387 patients from the UK, aged 30 years and older, with a first coded diagnosis of RA between 2005 and 2008. We listed indicators for RA which were present in coded format and ran keyword searches for similar information held in free text. The frequency of indicator code groups and keywords from one year before to 14 days after RA diagnosis were compared, and temporal relationships examined. RESULTS: One or more keyword for RA was found in the free text in 29% of patients prior to the RA diagnostic code. Keywords for inflammatory arthritis diagnoses were present for 14% of patients whereas only 11% had a diagnostic code. Codes for synovitis were found in 3% of patients, but keywords were identified in an additional 17%. In 13% of patients there was evidence of a positive rheumatoid factor test in text only, uncoded. No gender differences were found. Keywords generally occurred close in time to the coded diagnosis of rheumatoid arthritis. They were often found under codes indicating letters and communications. CONCLUSIONS: Potential cases may be missed or wrongly dated when coded data alone are used to identify patients with RA, as diagnostic suspicions are frequently confined to text. The use of EHRs to create disease registers or assess quality of care will be misleading if free text information is not taken into account. Methods to facilitate the automated processing of text need to be developed and implemented.",
"BACKGROUND: Information in Electronic Health Records is largely stored as unstructured free text. Natural language processing (NLP), or Medical Language Processing (MLP) in medicine, aims at extracting structured information from free text, and is less expensive and time-consuming than manual extraction. However, most algorithms in MLP are institution-specific or address only one clinical need, and thus cannot be broadly applied. In addition, most MLP systems do not detect concepts in misspelled text and cannot detect attribute relationships between concepts. The objective of this study was to develop and evaluate an MLP application that includes generic algorithms for the detection of (misspelled) concepts and of attribute relationships between them. METHODS: An implementation of the MLP system cTAKES, called DIRECT, was developed with generic SNOMED CT concept filter, concept relationship detection, and attribute relationship detection algorithms and a custom dictionary. Four implementations of cTAKES were evaluated by comparing 98 manually annotated oncology charts with the output of DIRECT. The F(1)-score was determined for named-entity recognition and attribute relationship detection for the concepts 'lung cancer', 'non-small cell lung cancer', and 'recurrence'. The performance of the four implementations was compared with a two-tailed permutation test. RESULTS: DIRECT detected lung cancer and non-small cell lung cancer concepts with F(1)-scores between 0.828 and 0.947 and between 0.862 and 0.933, respectively. The concept recurrence was detected with a significantly higher F(1)-score of 0.921, compared to the other implementations, and the relationship between recurrence and lung cancer with an F(1)-score of 0.857. The precision of the detection of lung cancer, non-small cell lung cancer, and recurrence concepts were 1.000, 0.966, and 0.879, compared to precisions of 0.943, 0.967, and 0.000 in the original implementation, respectively. CONCLUSION: DIRECT can detect oncology concepts and attribute relationships with high precision and can detect recurrence with significant increase in F(1)-score, compared to the original implementation of cTAKES, due to the usage of a custom dictionary and a generic concept relationship detection algorithm. These concepts and relationships can be used to encode clinical narratives, and can thus substantially reduce manual chart abstraction efforts, saving time for clinicians and researchers.",Algorithms | Chart abstraction | Electronic health records | Natural language processing | SNOMED CT
"BACKGROUND: Currently used prediction tools have limited ability to identify community-dwelling older people at high risk for falls. Prediction models utilizing Electronic Heath Records (EHR) provide opportunities but up to now showed limited clinical value as risk stratification tool; because of among others the underestimation of falls prevalence. The aim of this study was to develop a fall prediction model for community-dwelling older people using a combination of structured data and free text of primary care EHR and to internally validate its predictive performance. METHODS: EHR data of individuals aged 65 or over. Age, sex, history of falls, medications and medical conditions were included as potential predictors. Falls were ascertained from the free text. We employed the Bootstrap-enhanced penalized logistic regression with the least absolute shrinkage and selection operator to develop the prediction model. We used 10-fold cross-validation to internally validate the prediction strategy. Model performance was assessed in terms of discrimination and calibration. RESULTS: Data of 36,470 eligible participants were extracted from the dataset. The number of participants who fell at least once was 4,778 (13.1%). The final prediction model included age, sex, history of falls, two medications and five medical conditions. The model had a median area under the receiver operating curve of 0.705 (IQR 0.700-0.714) . CONCLUSIONS: Our prediction model to identify older people at high risk for falls achieved fair discrimination, and had reasonable calibration. It can be applied in clinical practice as it relies on routinely collected variables and does not require mobility assessment tests.",Accidental falls | fall prediction | fall prevention | free text | routinely collected data
"OBJECTIVE: Electronic Health Records (EHRs) increasingly include designated fields to capture social determinants of health (SDOH). We developed measures to characterize their use, and use of other SDOH data types, to optimize SDOH data integration. MATERIALS AND METHODS: We developed 3 measures that accommodate different EHR data types on an encounter or patient-year basis. We implemented these measures-documented during encounter (DDE) captures documentation occurring during the encounter; documented by discharge (DBD) includes DDE plus documentation occurring any time prior to admission; and reviewed during encounter (RDE) captures whether anyone reviewed documented data-for the newly available structured SDOH fields and 4 other comparator SDOH data types (problem list, inpatient nursing question, social history free text, and social work notes) on a hospital encounter basis (with patient-year metrics in the Supplementary Appendix). Our sample included all patients (n = 27 127) with at least one hospitalization at UCSF Health (a large, urban, tertiary medical center) over a 1-year period. RESULTS: We observed substantial variation in the use of different SDOH EHR data types. Notably, social history question fields (newly added at study period start) were rarely used (DDE: 0.03% of encounters, DBD: 0.26%, RDE: 0.03%). Free-text patient social history fields had higher use (DDE: 12.1%, DBD: 49.0%, RDE: 14.4%). DISCUSSION: Our measures of real-world SDOH data use can guide current efforts to capture and leverage these data. For our institution, measures revealed substantial variation across data types, suggesting the need to engage in efforts such as EHR-user education and targeted workflow integration. CONCLUSION: Measures revealed opportunities to optimize SDOH data documentation and review.",EHR | SDOH | social informatics
Dermatologists rely on skin biopsies to diagnose cutaneous tumors and rashes. Skin biopsy sites should be accurately identified with conventional anatomical site descriptors in the pathology request form. Reliance upon free-text entries to describe these biopsy sites is prone to user error and can cause medical misadventures such as wrong-site follow-up surgery. We sought to determine whether a smartphone application (RightSite) could improve the precision of biopsy site labeling. We conducted a prospective proof-of-concept study of 100 smartphone-assisted skin biopsy site identifiers with matched comparison to 100 historical controls. Student's t-test was used to identify significant differences in the precision of anatomic descriptors before and after adoption of the application. We found a 69% improvement in precision of anatomic site labeling with the RightSite smartphone application (P < 0.0001). These data show smartphone-assisted biopsy site labeling improves the precision of anatomic site descriptors. Integrating graphical user interfaces into the electronic health records system could improve health care by standardizing anatomic site nomenclature and site-specific descriptors.,biopsy site identification | dermatologic surgery | informatics | mobile application | smartphone
"BACKGROUND: A key challenge to mining electronic health records for mammography research is the preponderance of unstructured narrative text, which strikingly limits usable output. The imaging characteristics of breast cancer subtypes have been described previously, but without standardization of parameters for data mining. METHODS: The authors searched the enterprise-wide data warehouse at the Houston Methodist Hospital, the Methodist Environment for Translational Enhancement and Outcomes Research (METEOR), for patients with Breast Imaging Reporting and Data System (BI-RADS) category 5 mammogram readings performed between January 2006 and May 2015 and an available pathology report. The authors developed natural language processing (NLP) software algorithms to automatically extract mammographic and pathologic findings from free text mammogram and pathology reports. The correlation between mammographic imaging features and breast cancer subtype was analyzed using one-way analysis of variance and the Fisher exact test. RESULTS: The NLP algorithm was able to obtain key characteristics for 543 patients who met the inclusion criteria. Patients with estrogen receptor-positive tumors were more likely to have spiculated margins (P = .0008), and those with tumors that overexpressed human epidermal growth factor receptor 2 (HER2) were more likely to have heterogeneous and pleomorphic calcifications (P = .0078 and P = .0002, respectively). CONCLUSIONS: Mammographic imaging characteristics, obtained from an automated text search and the extraction of mammogram reports using NLP techniques, correlated with pathologic breast cancer subtype. The results of the current study validate previously reported trends assessed by manual data collection. Furthermore, NLP provides an automated means with which to scale up data extraction and analysis for clinical decision support. Cancer 2017;114-121. © 2016 American Cancer Society.",data mining | imaging characteristics | mammographic to pathologic correlation | natural language processing | subtypes of breast cancer
"INTRODUCTIONS: Digitization could be incorporated in rural areas of resource-poor countries because information gathered by nurses working on-site could be better used. BACKGROUND: For effective management of community health information, the usage and maintenance of digital records are important. Digitization of information provides essential information for informing health policy. AIM: To develop a sustainable database to effectively collect and manage community health information and nursing practice. METHODS: This study used a mixed method design. Phase 1 involved the development of a database system through repeated systematic focus group discussions with community health nurses. Phase 2 involved a practical trial examination of the developed system with both objective and subjective evaluations. RESULTS: A nursing database system was developed with templates designed for the major health problems of communities. The templates were composed of multiple-choice items and a free-text field that allowed records to be more detailed than handwritten records and maintained in standardized formats. This enables accumulation of data that were less likely to be influenced by the variance of ability in each nurse. DISCUSSION AND CONCLUSION: A multifaceted evaluation of the database system suggested that it could improve the efficiency of information management and contribute to the improvement of nursing care quality through standardization of the recording pattern. IMPLICATIONS FOR NURSING AND HEALTH POLICY: The nursing database will enable high-quality information storage that will potentially better inform health and healthcare policies as well as enable visualization of data concerning nursing care challenges and activities within the relevant communities. This information is essential for policy development and implementation in areas of human and fiscal resource allocations and meeting training/education needs.",Chronic Diseases | Community Health Nursing | Database System | Electronic Health Records | Information Technology | Resource-Poor Countries | Rural Areas | System Usability | Thailand
"OBJECTIVE: Accurate ascertainment of comorbidities is paramount in clinical research. While manual adjudication is labor-intensive and expensive, the adoption of electronic health records enables computational analysis of free-text documentation using natural language processing (NLP) tools. HYPOTHESIS: We sought to develop highly accurate NLP modules to assess for the presence of five key cardiovascular comorbidities in a large electronic health record system. METHODS: One-thousand clinical notes were randomly selected from a cardiovascular registry at Mass General Brigham. Trained physicians manually adjudicated these notes for the following five diagnostic comorbidities: hypertension, dyslipidemia, diabetes, coronary artery disease, and stroke/transient ischemic attack. Using the open-source Canary NLP system, five separate NLP modules were designed based on 800 ""training-set"" notes and validated on 200 ""test-set"" notes. RESULTS: Across the five NLP modules, the sentence-level and note-level sensitivity, specificity, and positive predictive value was always greater than 85% and was most often greater than 90%. Accuracy tended to be highest for conditions with greater diagnostic clarity (e.g. diabetes and hypertension) and slightly lower for conditions whose greater diagnostic challenges (e.g. myocardial infarction and embolic stroke) may lead to less definitive documentation. CONCLUSION: We designed five open-source and highly accurate NLP modules that can be used to assess for the presence of important cardiovascular comorbidities in free-text health records. These modules have been placed in the public domain and can be used for clinical research, trial recruitment and population management at any institution as well as serve as the basis for further development of cardiovascular NLP tools.",cardiovascular comorbidities | natural language processing
"BACKGROUND: Electronic health records are invaluable for medical research, but much information is stored as free text rather than in a coded form. For example, in the UK General Practice Research Database (GPRD), causes of death and test results are sometimes recorded only in free text. Free text can be difficult to use for research if it requires time-consuming manual review. Our aim was to develop an automated method for extracting coded information from free text in electronic patient records. METHODS: We reviewed the electronic patient records in GPRD of a random sample of 3310 patients who died in 2001, to identify the cause of death. We developed a computer program called the Freetext Matching Algorithm (FMA) to map diagnoses in text to the Read Clinical Terminology. The program uses lookup tables of synonyms and phrase patterns to identify diagnoses, dates and selected test results. We tested it on two random samples of free text from GPRD (1000 texts associated with death in 2001, and 1000 general texts from cases and controls in a coronary artery disease study), comparing the output to the U.S. National Library of Medicine's MetaMap program and the gold standard of manual review. RESULTS: Among 3310 patients registered in the GPRD who died in 2001, the cause of death was recorded in coded form in 38.1% of patients, and in the free text alone in 19.4%. On the 1000 texts associated with death, FMA coded 683 of the 735 positive diagnoses, with precision (positive predictive value) 98.4% (95% confidence interval (CI) 97.2, 99.2) and recall (sensitivity) 92.9% (95% CI 90.8, 94.7). On the general sample, FMA detected 346 of the 447 positive diagnoses, with precision 91.5% (95% CI 88.3, 94.1) and recall 77.4% (95% CI 73.2, 81.2), which was similar to MetaMap. CONCLUSIONS: We have developed an algorithm to extract coded information from free text in GP records with good precision. It may facilitate research using free text in electronic patient records, particularly for extracting the cause of death.",
"The widespread adoption of electronic health records (EHRs) and the growing wealth of digitized information sources about patients is ushering in an era of 'Big Data' that may revolutionize clinical research in oncology. Research will likely be more efficient and potentially more accurate than the current gold standard of manual chart review studies. However, EHRs as they exist today have significant limitations: important data elements are missing or are only captured in free text or PDF documents. Using two case studies, we illustrate the challenges of leveraging the data that are routinely collected by the healthcare system in EHRs (e.g., real-world data), specific challenges encountered in the cancer domain and opportunities that can be achieved when these are overcome.",clinical research | electronic health records | real-world data
"BACKGROUND: Variations in treatment choice, or late stage at first diagnosis, mean that, despite guideline recommendations, not all patients with hormone receptor (hr)-positive locally advanced or metastatic breast cancer (la/mbca) will have received endocrine therapy before disease progression. In the present study, we aimed to estimate the proportion of women with postmenopausal hr-positive la/mbca in the United States who are endocrine therapy-naïve. METHODS: Women in the Optum Electronic Health Record (ehr) database with a breast cancer (bca) diagnosis (January 2008-March 2015) were included. Patient and malignancy characteristics were identified using structured data fields and natural-language processing of free-text clinical notes. The proportion of women with postmenopausal hr-positive, human epidermal growth factor 2 (her2)-negative (or unknown) la/mbca who had not received prior endocrine therapy was determined. Results were extrapolated to the entire U.S. population using the U.S. National Cancer Institute's Surveillance, Epidemiology, and End Results database. Results are presented descriptively. RESULTS: In the ehr database, 11,831 women with bca had discernible information on postmenopausal status, hr status, and disease stage. Of those women, 1923 (16.3%) had postmenopausal hr-positive, her2-negative (or unknown) la/mbca, and 70.7% of those 1923 patients (n = 1360) had not received prior endocrine therapy, accounting for 11.5% of the overall population. Extrapolating those estimates nationally suggests an annual incidence of 14,784 cases, and a 5-year limited duration prevalence of 50,638 cases. CONCLUSIONS: A substantial proportion of women with postmenopausal hr-positive la/mbca in the United States could be endocrine therapy-naïve.","Breast cancer, advanced | breast cancer, metastatic | electronic health records | endocrine therapy | hormone receptor–positive disease"
"BACKGROUND: The electronic health record (EHR) contains a wealth of medical information. An organized EHR can greatly help doctors treat patients. In some cases, only limited patient information is collected to help doctors make treatment decisions. Because EHRs can serve as a reference for this limited information, doctors' treatment capabilities can be enhanced. Natural language processing and deep learning methods can help organize and translate EHR information into medical knowledge and experience. OBJECTIVE: In this study, we aimed to create a model to extract concept embeddings from EHRs for disease pattern retrieval and further classification tasks. METHODS: We collected 1,040,989 emergency department visits from the National Taiwan University Hospital Integrated Medical Database and 305,897 samples from the National Hospital and Ambulatory Medical Care Survey Emergency Department data. After data cleansing and preprocessing, the data sets were divided into training, validation, and test sets. We proposed a Transformer-based model to embed EHRs and used Bidirectional Encoder Representations from Transformers (BERT) to extract features from free text and concatenate features with structural data as input to our proposed model. Then, Deep InfoMax (DIM) and Simple Contrastive Learning of Visual Representations (SimCLR) were used for the unsupervised embedding of the disease concept. The pretrained disease concept-embedding model, named EDisease, was further finetuned to adapt to the critical care outcome prediction task. We evaluated the performance of embedding using t-distributed stochastic neighbor embedding (t-SNE) to perform dimension reduction for visualization. The performance of the finetuned predictive model was evaluated against published models using the area under the receiver operating characteristic (AUROC). RESULTS: The performance of our model on the outcome prediction had the highest AUROC of 0.876. In the ablation study, the use of a smaller data set or fewer unsupervised methods for pretraining deteriorated the prediction performance. The AUROCs were 0.857, 0.870, and 0.868 for the model without pretraining, the model pretrained by only SimCLR, and the model pretrained by only DIM, respectively. On the smaller finetuning set, the AUROC was 0.815 for the proposed model. CONCLUSIONS: Through contrastive learning methods, disease concepts can be embedded meaningfully. Moreover, these methods can be used for disease retrieval tasks to enhance clinical practice capabilities. The disease concept model is also suitable as a pretrained model for subsequent prediction tasks.",EHR | NLP | concept | deep learning | disease embedding | disease retrieval | electronic health record | emergency department | extraction | machine learning | natural language processing
"Free-text reports in electronic health records (EHRs) contain medically significant information - signs, symptoms, findings, diagnoses - recorded by clinicians during patient encounters. These reports contain rich clinical information which can be leveraged for surveillance of disease and occurrence of adverse events. In order to gain meaningful knowledge from these text reports to support surveillance efforts, information must first be converted into a structured, computable format. Traditional methods rely on manual review of charts, which can be costly and inefficient. Natural language processing (NLP) methods offer an efficient, alternative approach to extracting the information and can achieve a similar level of accuracy. We developed an NLP system to automatically identify mentions of surgical site infections in radiology reports and classify reports containing evidence of surgical site infections leveraging these mentions. We evaluated our system using a reference standard of reports annotated by domain experts, administrative data generated for each patient encounter, and a machine learning-based approach.",
"BACKGROUND: The ability to manage and leverage family history information in the electronic health record (EHR) is crucial to delivering high-quality clinical care. OBJECTIVES: We aimed to evaluate existing standards in representing relative information, examine this information documented in EHRs, and develop a natural language processing (NLP) application to extract relative information from free-text clinical documents. METHODS: We reviewed a random sample of 100 admission notes and 100 discharge summaries of 198 patients, and also reviewed the structured entries for these patients in an EHR system's family history module. We investigated the two standards used by Stage 2 of Meaningful Use (SNOMED CT and HL7 Family History Standard) and identified coverage gaps of each standard in coding relative information. Finally, we evaluated the performance of the MTERMS NLP system in identifying relative information from free-text documents. RESULTS: The structure and content of SNOMED CT and HL7 for representing relative information are different in several ways. Both terminologies have high coverage to represent local relative concepts built in an ambulatory EHR system, but gaps in key concept coverage were detected; coverage rates for relative information in free-text clinical documents were 95.2% and 98.6%, respectively. Compared to structured entries, richer family history information was only available in free-text documents. Using a comprehensive lexicon that included concepts and terms of relative information from different sources, we expanded the MTERMS NLP system to extract and encode relative information in clinical documents and achieved a corresponding precision of 100% and recall of 97.4%. CONCLUSIONS: Comprehensive assessment and user guidance are critical to adopting standards into EHR systems in a meaningful way. A significant portion of patients' family history information is only documented in free-text clinical documents and NLP can be used to extract this information.",HL7 | SNOMED | Terminology | electronic health records | family history | natural language processing
"BACKGROUND: There has been an increasing interest in learning low-dimensional vector representations of medical concepts from Electronic Health Records (EHRs). Vector representations of medical concepts facilitate exploratory analysis and predictive modeling of EHR data to gain insights about the patterns of care and health outcomes. EHRs contain structured data such as diagnostic codes and laboratory tests, as well as unstructured free text data in form of clinical notes, which provide more detail about condition and treatment of patients. METHODS: In this work, we propose a method that jointly learns vector representations of medical concepts and words. This is achieved by a novel learning scheme based on the word2vec model. Our model learns those relationships by integrating clinical notes and sets of accompanying medical codes and by defining joint contexts for each observed word and medical code. RESULTS: In our experiments, we learned joint representations using MIMIC-III data. Using the learned representations of words and medical codes, we evaluated phenotypes for 6 diseases discovered by our and baseline method. The experimental results show that for each of the 6 diseases our method finds highly relevant words. We also show that our representations can be very useful when predicting the reason for the next visit. CONCLUSIONS: The jointly learned representations of medical concepts and words capture not only similarity between codes or words themselves, but also similarity between codes and words. They can be used to extract phenotypes of different diseases. The representations learned by the joint model are also useful for construction of patient features.",Distributed representation | Electronic health records | Healthcare | Natural language processing
"Coreference resolution is the task of determining linguistic expressions that refer to the same real-world entity in natural language. Research on coreference resolution in the general English domain dates back to 1960s and 1970s. However, research on coreference resolution in the clinical free text has not seen major development. The recent US government initiatives that promote the use of electronic health records (EHRs) provide opportunities to mine patient notes as more and more health care institutions adopt EHR. Our goal was to review recent advances in general purpose coreference resolution to lay the foundation for methodologies in the clinical domain, facilitated by the availability of a shared lexical resource of gold standard coreference annotations, the Ontology Development and Information Extraction (ODIE) corpus.",
"OBJECTIVE: Our primary objective was to identify cognitive behavioural therapy (CBT) delivery for people with psychosis (CBTp) using an automated method in a large electronic health record database. We also examined what proportion of service users with a diagnosis of psychosis were recorded as having received CBTp within their episode of care during defined time periods provided by early intervention or promoting recovery community services for people with psychosis, compared with published audits and whether demographic characteristics differentially predicted the receipt of CBTp. METHODS: Both free text using natural language processing (NLP) techniques and structured methods of identifying CBTp were combined and evaluated for positive predictive value (PPV) and sensitivity. Using inclusion criteria from two published audits, we identified anonymised cross-sectional samples of 2579 and 2308 service users respectively with a case note diagnosis of schizophrenia or psychosis for further analysis. RESULTS: The method achieved PPV of 95% and sensitivity of 96%. Using the National Audit of Schizophrenia 2 criteria, 34.6% service users were identified as ever having received at least one session and 26.4% at least two sessions of CBTp; these are higher percentages than previously reported by manual audit of a sample from the same trust that returned 20.0%. In the fully adjusted analysis, CBTp receipt was significantly (p<0.05) more likely in younger patients, in white and other when compared with black ethnic groups and patients with a diagnosis of other schizophrenia spectrum and schizoaffective disorder when compared with schizophrenia. CONCLUSIONS: The methods presented here provided a potential method for evaluating delivery of CBTp on a large scale, providing more scope for routine monitoring, cross-site comparisons and the promotion of equitable access.",CBT | CBT for psychosis | CBTp | EHR | cognitive behavioural therapy | electronic Health records
"The concept of optimizing health care by understanding and generating knowledge from previous evidence, ie, the Learning Health-care System (LHS), has gained momentum and now has national prominence. Meanwhile, the rapid adoption of electronic health records (EHRs) enables the data collection required to form the basis for facilitating LHS. A prerequisite for using EHR data within the LHS is an infrastructure that enables access to EHR data longitudinally for health-care analytics and real time for knowledge delivery. Additionally, significant clinical information is embedded in the free text, making natural language processing (NLP) an essential component in implementing an LHS. Herein, we share our institutional implementation of a big data-empowered clinical NLP infrastructure, which not only enables health-care analytics but also has real-time NLP processing capability. The infrastructure has been utilized for multiple institutional projects including the MayoExpertAdvisor, an individualized care recommendation solution for clinical care. We compared the advantages of big data over two other environments. Big data infrastructure significantly outperformed other infrastructure in terms of computing speed, demonstrating its value in making the LHS a possibility in the near future.",big data | health-care analytics | learning health-care system | natural language processing
"INTRODUCTION: In the last few years much work has been conducted in creating systems that support clinical trials for example by utilizing electronic health record data. One of these endeavours is the Electronic Health Record for Clinical Research project (EHR4CR). An unanswered question that the project aims to answer is which data elements are most commonly required for patient recruitment. METHODS: Free text eligibility criteria from 40 studies were analysed, simplified and elements were extracted. These elements where then added to an existing inventory of data elements for protocol feasibility. RESULTS: We simplified and extracted data elements from 40 trials, which resulted in 1170 elements. From these we created an inventory of 150 unique data elements relevant for patient identification and recruitment with definitions and referenced codes to standard terminologies. DISCUSSION: Our list was created with expertise from pharmaceutical companies. Comparisons with related work shows that identified concepts are similar. An evaluation of the availability of these elements in electronic health records is still ongoing. Hospitals that want to engage in re-use of electronic health record data for research purposes, for example by joining networks like EHR4CR, can now prioritize their effort based on this list.",
"OBJECTIVES: To summarize key contributions to current research in the field of Clinical Research Informatics (CRI) and to select best papers published in 2020. METHOD: A bibliographic search using a combination of Medical Subject Headings (MeSH) descriptors and free-text terms on CRI was performed using PubMed, followed by a double-blind review in order to select a list of candidate best papers to be then peer-reviewed by external reviewers. After peer-review ranking, a consensus meeting between two section editors and the editorial team was organized to finally conclude on the selected four best papers. RESULTS: Among the 877 papers published in 2020 and returned by the search, there were four best papers selected. The first best paper describes a method for mining temporal sequences from clinical documents to infer disease trajectories and enhancing high-throughput phenotyping. The authors of the second best paper demonstrate that the generation of synthetic Electronic Health Record (EHR) data through Generative Adversarial Networks (GANs) could be substantially improved by more appropriate training and evaluation criteria. The third best paper offers an efficient advance on methods to detect adverse drug events by computer-assisting expert reviewers with annotated candidate mentions in clinical documents. The large-scale data quality assessment study reported by the fourth best paper has clinical research informatics implications, in terms of the trustworthiness of inferences made from analysing electronic health records. CONCLUSIONS: The most significant research efforts in the CRI field are currently focusing on data science with active research in the development and evaluation of Artificial Intelligence/Machine Learning (AI/ML) algorithms based on ever more intensive use of real-world data and especially EHR real or synthetic data. A major lesson that the coronavirus disease 2019 (COVID-19) pandemic has already taught the scientific CRI community is that timely international high-quality data-sharing and collaborative data analysis is absolutely vital to inform policy decisions.",
"This study sought to determine physician, specialty and practice factors influencing choice of method for electronic health record (EHR) documentation: direct typing (DT), electronic transcription (ET), human transcription (HT), and scribes. A survey assessing physician documentation practices was developed and distributed online. The primary outcome was the proportion of physicians using each method. Secondary outcomes were provider-rated accuracy, efficiency, and ease of navigation on a 1-5 Likert scale. Means were compared using linear mixed models with Bonferroni adjustment. The 818 respondents were mostly outpatient (46%) adult (79%) physicians, practiced for a mean 15.8 years, and used DT for EHR documentation (72%). Emergency physicians were more likely to use scribes (p < 0.0001). DT was rated less efficient than all other methods (p < 0.0001). ET was rated less accurate than DT (p < 0.001) and HT (p < 0.001). HT was rated less easy to navigate than DT (p = 0.002) and scribe (p < 0.001), and ET less than scribe (p = 0.002). Two hundred and forty-three respondents provided free-text comments that further described opinions. DT was the most commonly used EHR method but rated least efficient. Scribes were rated easy to navigate and efficient but infrequently used outside of emergency settings. Further innovation is needed to design systems responsive to all physician EHR needs.",documentation | electronic health record | medical scribe | voice recognition
"BACKGROUND: Using electronic health records (EHRs), in addition to claims, to systematically identify patients with factors associated with adverse outcomes (geriatric risk) among older adults can prove beneficial for population health management and clinical service delivery. OBJECTIVE: To define and compare geriatric risk factors derivable from claims, structured EHRs, and unstructured EHRs, and estimate the relationship between geriatric risk factors and health care utilization. RESEARCH DESIGN: We performed a retrospective cohort study of patients enrolled in a Medicare Advantage plan from 2011 to 2013 using both administrative claims and EHRs. We defined 10 individual geriatric risk factors and a summary geriatric risk index based on diagnosed conditions and pattern matching techniques applied to EHR free text. The prevalence of geriatric risk factors was estimated using claims, structured EHRs, and structured and unstructured EHRs combined. The association of geriatric risk index with any occurrence of hospitalizations, emergency department visits, and nursing home visits were estimated using logistic regression adjusted for demographic and comorbidity covariates. RESULTS: The prevalence of geriatric risk factors increased after adding unstructured EHR data to structured EHRs, compared with those derived from structured EHRs alone and claims alone. On the basis of claims, structured EHRs, and structured and unstructured EHRs combined, 12.9%, 15.0%, and 24.6% of the patients had 1 geriatric risk factor, respectively; 3.9%, 4.2%, and 15.8% had ≥2 geriatric risk factors, respectively. Statistically significant association between geriatric risk index and health care utilization was found independent of demographic and comorbidity covariates. For example, based on claims, estimated odds ratios for having 1 and ≥2 geriatric risk factors in year 1 were 1.49 (P<0.001) and 2.62 (P<0.001) in predicting any occurrence of hospitalizations in year 1, and 1.32 (P<0.001) and 1.34 (P=0.003) in predicting any occurrence of hospitalizations in year 2. CONCLUSIONS: The results demonstrate the feasibility and potential of using EHRs and claims for collecting new types of geriatric risk information that could augment the more commonly collected disease information to identify and move upstream the management of high-risk cases among older patients.",
"BACKGROUND: Clinician's asthma guideline adherence in asthma care is suboptimal. The effort for improving adherence can be enhanced by assessing and monitoring clinician's adherence to guidelines reflected in electronic health records (EHRs) which yet requires costly manual chart review since many care elements cannot be identified by structured data. OBJECTIVE: This study was designed to demonstrate the feasibility of an artificial intelligence (AI) tool using natural language processing (NLP) leveraging free text of EHRs of pediatric patients to extract key components of the 2007 National Asthma Education and Prevention Program guidelines. METHODS: This is a retrospective cross-sectional study using a birth cohort with asthma diagnosis at Mayo Clinic between 2003 and 2016. We used 1,039 clinical notes with an asthma diagnosis from a random sample of 300 patients. Rule-based NLP algorithms were developed to identify asthma guideline congruent elements by examining care description in EHR free text. RESULTS: NLP algorithms demonstrated a sensitivity (0.82 - 1.0), specificity (0.95 - 1.0), positive predictive value (0.86 -1.0), and negative protective value (0.92 - 1.0) against manual chart review for asthma guideline-congruent elements. Assessing medication compliance and inhaler technique assessment were the most challenging elements to assess due to the complexity and wide variety of descriptions. CONCLUSION: NLP technologies may enable automated assessment of clinician's documentation in EHRs regarding adherence to asthma guidelines and can be a useful population management and research tool for assessing and monitoring asthma care quality. Multi-site studies with a larger sample size are needed for assessing the generalizability of our NLP algorithms.",adherence to asthma guidelines | automated chart review | documentation variation | national asthma education and prevention program | natural language processing
"The electronic health record (EHR) provides an opportunity for improved use of clinical documentation including leveraging tobacco use information by clinicians and researchers. In this study, we investigated the content, consistency, and completeness of tobacco use data from structured and unstructured sources in the EHR. A natural language process (NLP) pipeline was utilized to extract details about tobacco use from clinical notes and free-text tobacco use comments within the social history module of an EHR system. We analyzed the consistency of tobacco use information within clinical notes, comments, and available structured fields for tobacco use. Our results indicate that structured fields for tobacco use alone may not be able to provide complete tobacco use information. While there was better consistency for some elements (e.g., status and type), inconsistencies were found particularly for temporal information. Further work is needed to improve tobacco use information integration from different parts of the EHR.",
"BACKGROUND: Legacy data and new structured data can be stored in a standardized format as XML-based EHRs on XML databases. Querying documents on these databases is crucial for answering research questions. Instead of using free text searches, that lead to false positive results, the precision can be increased by constraining the search to certain parts of documents. METHODS: A search ontology-based specification of queries on XML documents defines search concepts and relates them to parts in the XML document structure. Such query specification method is practically introduced and evaluated by applying concrete research questions formulated in natural language on a data collection for information retrieval purposes. The search is performed by search ontology-based XPath engineering that reuses ontologies and XML-related W3C standards. RESULTS: The key result is that the specification of research questions can be supported by the usage of search ontology-based XPath engineering. A deeper recognition of entities and a semantic understanding of the content is necessary for a further improvement of precision and recall. Key limitation is that the application of the introduced process requires skills in ontology and software development. In future, the time consuming ontology development could be overcome by implementing a new clinical role: the clinical ontologist. CONCLUSION: The introduced Search Ontology XML extension connects Search Terms to certain parts in XML documents and enables an ontology-based definition of queries. Search ontology-based XPath engineering can support research question answering by the specification of complex XPath expressions without deep syntax knowledge about XPaths.",EHR query | Electronic health records | Information retrieval | Medical informatics applications | Pathology electronic health records | Query engineering | Search ontology
"Electronic Health Record (EHR) use in India is generally poor, and structured clinical information is mostly lacking. This work is the first attempt aimed at evaluating unstructured text mining for extracting relevant clinical information from Indian clinical records. We annotated a corpus of 250 discharge summaries from an Intensive Care Unit (ICU) in India, with markups for diseases, procedures, and lab parameters, their attributes, as well as key demographic information and administrative variables such as patient outcomes. In this process, we have constructed guidelines for an annotation scheme useful to clinicians in the Indian context. We evaluated the performance of an NLP engine, Cocoa, on a cohort of these Indian clinical records. We have produced an annotated corpus of roughly 90 thousand words, which to our knowledge is the first tagged clinical corpus from India. Cocoa was evaluated on a test corpus of 50 documents. The overlap F-scores across the major categories, namely disease/symptoms, procedures, laboratory parameters and outcomes, are 0.856, 0.834, 0.961 and 0.872 respectively. These results are competitive with results from recent shared tasks based on US records. The annotated corpus and associated results from the Cocoa engine indicate that unstructured text mining is a viable method for cohort analysis in the Indian clinical context, where structured EHR records are largely absent.",Biomedical text extraction | Data mining | Discharge summary | Natural language processing | Text annotation
"Drug safety is an important aspect in healthcare, resulting in a number of inadvertent events, which may harm the patients. IT based Clinical Decision Support (CDS), integrated in electronic-prescription or Electronic Health Records (EHR) systems, can provide a means for checking prescriptions for errors. This requires expressing prescription guidelines in a way that can be interpreted by IT systems. The paper uses Natural Language Processing (NLP), to interpret drug guidelines by the UK NICE BNF offered in free text. The employed NLP component, MetaMap, identifies the concepts in the instructions and interprets their semantic meaning. The UMLS semantic types that correspond to these concepts are then processed, in order to understand the concepts that are needed to be implemented in software engineering for a CDS engine.",CDS | NLP | Pharmacovigilance | drug safety
"Free-text clinical notes in electronic health records are more difficult for data mining while the structured diagnostic codes can be missing or erroneous. To improve the quality of diagnostic codes, this work extracts diagnostic codes from free-text notes: five old and new word vectorization methods were used to vectorize Stanford progress notes and predict eight ICD-10 codes of common cardiovascular diseases with logistic regression. The models showed good performance, with TF-IDF as the best vectorization model showing the highest AUROC (0.9499-0.9915) and AUPRC (0.2956-0.8072). The models also showed transferability when tested on MIMIC-III data with AUROC from 0.7952 to 0.9790 and AUPRC from 0.2353 to 0.8084. Model interpretability was shown by the important words with clinical meanings matching each disease. This study shows the feasibility of accurately extracting structured diagnostic codes, imputing missing codes, and correcting erroneous codes from free-text clinical notes for information retrieval and downstream machine-learning applications.",ICD-10 codes | cardiovascular disease | clinical notes | interpretability | natural language processing
"The secondary use of electronic health records opens up new perspectives. They provide researchers with structured data and unstructured data, including free text reports. Many applications been developed to leverage knowledge from free-text reports, but manual review of documents is still a complex process. We developed FASTVISU a web-based application to assist clinicians in reviewing documents. We used FASTVISU to review a set of 6340 documents from 741 patients suffering from the celiac disease. A first automated selection pruned the original set to 847 documents from 276 patients' records. The records were reviewed by two trained physicians to identify the presence of 15 auto-immune diseases. It took respectively two hours and two hours and a half to evaluate the entire corpus. Inter-annotator agreement was high (Cohen's kappa at 0.89). FASTVISU is a user-friendly modular solution to validate entities extracted by NLP methods from free-text documents stored in clinical data warehouses.",
"BACKGROUND: Electronic health records are being increasingly used by nurses with up to 80% of the health data recorded as free text. However, only a few studies have developed nursing-relevant tools that help busy clinicians to identify information they need at the point of care. OBJECTIVE: This study developed and validated one of the first automated natural language processing applications to extract wound information (wound type, pressure ulcer stage, wound size, anatomic location, and wound treatment) from free text clinical notes. METHODS AND DESIGN: First, two human annotators manually reviewed a purposeful training sample (n=360) and random test sample (n=1100) of clinical notes (including 50% discharge summaries and 50% outpatient notes), identified wound cases, and created a gold standard dataset. We then trained and tested our natural language processing system (known as MTERMS) to process the wound information. Finally, we assessed our automated approach by comparing system-generated findings against the gold standard. We also compared the prevalence of wound cases identified from free-text data with coded diagnoses in the structured data. RESULTS: The testing dataset included 101 notes (9.2%) with wound information. The overall system performance was good (F-measure is a compiled measure of system's accuracy=92.7%), with best results for wound treatment (F-measure=95.7%) and poorest results for wound size (F-measure=81.9%). Only 46.5% of wound notes had a structured code for a wound diagnosis. CONCLUSIONS: The natural language processing system achieved good performance on a subset of randomly selected discharge summaries and outpatient notes. In more than half of the wound notes, there were no coded wound diagnoses, which highlight the significance of using natural language processing to enrich clinical decision making. Our future steps will include expansion of the application's information coverage to other relevant wound factors and validation of the model with external data.",Electronic health records | Medical informatics | Natural language processing | Nursing informatics | Pressure ulcer | Wound
"BACKGROUND: Electronic health records (EHRs) are proliferating, and financial incentives encourage their use. Applying Fair Information Practice principles to EHRs necessitates balancing patients' rights to control their personal information with providers' data needs to deliver safe, high-quality care. We describe the technical and organizational challenges faced in capturing patients' preferences for patient-controlled EHR access and applying those preferences to an existing EHR. METHODS: We established an online system for capturing patients' preferences for who could view their EHRs (listing all participating clinic providers individually and categorically-physicians, nurses, other staff) and what data to redact (none, all, or by specific categories of sensitive data or patient age). We then modified existing data-viewing software serving a state-wide health information exchange and a large urban health system and its primary care clinics to allow patients' preferences to guide data displays to providers. RESULTS: Patients could allow or restrict data displays to all clinicians and staff in a demonstration primary care clinic, categories of providers (physicians, nurses, others), or individual providers. They could also restrict access to all EHR data or any or all of five categories of sensitive data (mental and reproductive health, sexually transmitted diseases, HIV/AIDS, and substance abuse) and for specific patient ages. The EHR viewer displayed data via reports, data flowsheets, and coded and free text data displayed by Google-like searches. Unless patients recorded restrictions, by default all requested data were displayed to all providers. Data patients wanted restricted were not displayed, with no indication they were redacted. Technical barriers prevented redacting restricted information in free textnotes. The program allowed providers to hit a ""Break the Glass"" button to override patients' restrictions, recording the date, time, and next screen viewed. Establishing patient-control over EHR data displays was complex and required ethical, clinical, database, and programming expertise and difficult choices to overcome technical and health system constraints. CONCLUSIONS: Assessing patients' preferences for access to their EHRs and applying them in clinical practice requires wide-ranging technical, clinical, and bioethical expertise, to make tough choices to overcome significant technical and organization challenges.",
"BACKGROUND: Recent research has suggested that using electronic health records (EHRs) can negatively impact clinical reasoning (CR) and interprofessional collaborative practices (ICPs). Understanding the benefits and obstacles that EHR use introduces into clinical activities is essential for improving medical documentation, while also supporting CR and ICP. METHODS: This qualitative study was a longitudinal pre/post investigation of the impact of EHR implementation on CR and ICP at a large pediatric hospital. We collected data via observations, interviews, document analysis, and think-aloud/-after sessions. Using constructivist Grounded Theory's iterative cycles of data collection and analysis, we identified and explored an emerging theme that clinicians described as central to their CR and ICP activities: building the patient's story. We studied how building the patient's story was impacted by the introduction and implementation of an EHR. RESULTS: Clinicians described the patient's story as a cognitive awareness and overview understanding of the patient's (1) current status, (2) relevant history, (3) data patterns that emerged during care, and (4) the future-oriented care plan. Constructed by consolidating and interpreting a wide array of patient data, building the patient's story was described as a vitally important skill that was required to provide patient-centered care, within an interprofessional team, that safeguards patient safety and clinicians' professional credibility. Our data revealed that EHR use obstructed clinicians' ability to build the patient's story by fragmenting data interconnections. Further, the EHR limited the number and size of free-text spaces available for narrative notes. This constraint inhibited clinicians' ability to read the why and how interpretations of clinical activities from other team members. This resulted in the loss of shared interprofessional understanding of the patient's story, and the increased time required to build the patient's story. CONCLUSIONS: We discuss these findings in relation to research on the role of narratives for enabling CR and ICP. We conclude that EHRs have yet to truly fulfill their promise to support clinicians in their patient care activities, including the essential work of building the patient's story.",Clinical reasoning | Electronic health records | Interprofessional team collaboration | Narrative | Patient story
"BACKGROUND: The Mortality Probability Model (MPM) is used in research and quality improvement to adjust for severity of illness and can also inform triage decisions. However, a limitation for its automated use or application is that it includes the variable ""intracranial mass effect"" (IME), which requires human engagement with the electronic health record (EHR). We developed and tested a natural language processing (NLP) algorithm to identify IME from CT head reports. METHODS: We obtained initial CT head reports from adult patients who were admitted to the ICU from our ED between 10/2013 and 9/2016. Each head CT head report was labeled yes/no IME by at least two of five independent labelers. The reports were then randomly divided 80/20 into training and test sets. All reports were preprocessed to remove linguistic and style variability, and a dictionary was created to map similar common terms. We tested three vectorization strategies: Term Frequency-Inverse Document frequency (TF-IDF), Word2Vec, and Universal Sentence Encoder to convert the report text to a numerical vector. This vector served as the input to a classification-tree-based ensemble machine learning algorithm (XGBoost). After training, model performance was assessed in the test set using the area under the receiver operating characteristic curve (AUROC). We also divided the continuous range of scores into positive/inconclusive/negative categories for IME. RESULTS: Of the 1202 CT reports in the training set, 308 (25.6%) reports were manually labeled as ""yes"" for IME. Of the 355 reports in the test set, 108 (30.4%) were labeled as ""yes"" for IME. The TF-IDF vectorization strategy as an input for the XGBoost model had the best AUROC:-- 0.9625 (95% CI 0.9443-0.9807). TF-IDF score categories were defined and had the following likelihood ratios: ""positive"" (TF-IDF score > 0.5) LR = 24.59; ""inconclusive"" (TF-IDF 0.05-0.5) LR = 0.99; and ""negative"" (TF-IDF < 0.05) LR = 0.05. 82% of reports were classified as either ""positive"" or ""negative"". In the test set, only 4 of 199 (2.0%) reports with a ""negative"" classification were false negatives and only 8 of 93 (8.6%) reports classified as ""positive"" were false positives. CONCLUSION: NLP can accurately identify IME from free-text reports of head CTs in approximately 80% of records, adequate to allow automatic calculation of MPM based on EHR data for many applications.",Artificial intelligence | Emergency critical care | Hospital mortality | Natural language processing
"BACKGROUND: Anti-VEGF drugs are currently used to treat macular diseases. This has led to a wealth of additional data, which could help understand and predict treatment courses; however, this information is usually only available in free text form. OBJECTIVE: A retrospective study was designed to analyze how far interpretable information can be obtained from clinical texts by automated extraction. The aim was to assess the suitability of a text mining method that was customized for this purpose. MATERIAL AND METHODS: Data on 3683 patients were available, including 40,485 discharge letters. Some of the data of interest, e.g. visual acuity (VA), intraocular pressure (IOP) and accompanying diagnoses, were not only recorded textually but also entered in a database and could thus serve as a gold standard for text analysis. The text was analyzed using the Averbis Health Discovery text mining platform. To optimize the extraction task, rule knowledge and a German language technical vocabulary linked to the international medical terminology standard systematized nomenclature of medicine (SNOMED CT) was manually added. RESULTS: The correspondence between extracted data and the structured database entries is described by the F1 value. There was agreement of 94.7% for VA, 98.3% for IOP and 94.7% for the accompanying diagnoses. Manual analysis of noncorresponding cases showed that in 50% text content did not match the database content for various reasons. After an adjustment, F1 values 1-3% above the previously determined values were obtained. CONCLUSION: Text mining procedures are very well suited for the considered discharge letter corpus and the problem described in order to extract contents from clinical texts in a structured manner for further evaluation.",Decision support systems | Electronic health records | Macular degeneration | Natural language processing | Systematized nomenclature of medicine
"The detection of Adverse Medical Events (AMEs) plays an important role in disease management in ensuring efficient treatment delivery and quality improvement of health services. Recently, with the rapid development of hospital information systems, a large volume of Electronic Health Records (EHRs) have been produced, in which AMEs are regularly documented in a free-text manner. In this study, we are concerned with the problem of AME detection by utilizing a large volume of unstructured EHR data. To address this challenge, we propose a neural attention network-based model to incorporate the contextual information of words into AME detection. Specifically, we develop a context-aware attention mechanism to locate salient words with respect to the target AMEs in patient medical records. And then we combine the proposed context attention mechanism with the deep learning tactic to boost the performance of AME detection. We validate our proposed model on a real clinical dataset that consists of 8845 medical records of patients with cardiovascular diseases. The experimental results show that our proposed model advances state-of-the-art models and achieves competitive performance in terms of AME detection.",Adverse medical event | Cardiovascular disease | Deep learning | Electronic health record | Neural attention network
"OBJECTIVE: This paper describes the University of Michigan's nine-year experience in developing and using a full-text search engine designed to facilitate information retrieval (IR) from narrative documents stored in electronic health records (EHRs). The system, called the Electronic Medical Record Search Engine (EMERSE), functions similar to Google but is equipped with special functionalities for handling challenges unique to retrieving information from medical text. MATERIALS AND METHODS: Key features that distinguish EMERSE from general-purpose search engines are discussed, with an emphasis on functions crucial to (1) improving medical IR performance and (2) assuring search quality and results consistency regardless of users' medical background, stage of training, or level of technical expertise. RESULTS: Since its initial deployment, EMERSE has been enthusiastically embraced by clinicians, administrators, and clinical and translational researchers. To date, the system has been used in supporting more than 750 research projects yielding 80 peer-reviewed publications. In several evaluation studies, EMERSE demonstrated very high levels of sensitivity and specificity in addition to greatly improved chart review efficiency. DISCUSSION: Increased availability of electronic data in healthcare does not automatically warrant increased availability of information. The success of EMERSE at our institution illustrates that free-text EHR search engines can be a valuable tool to help practitioners and researchers retrieve information from EHRs more effectively and efficiently, enabling critical tasks such as patient case synthesis and research data abstraction. CONCLUSION: EMERSE, available free of charge for academic use, represents a state-of-the-art medical IR tool with proven effectiveness and user acceptance.",Electronic health records (E05.318.308.940.968.625.500) | Information storage and retrieval (L01.470) | Search engine (L01.470.875)
"Background: Cognitive impairments are a neglected aspect of schizophrenia despite being a major factor of poor functional outcome. They are usually measured using various rating scales, however, these necessitate trained practitioners and are rarely routinely applied in clinical settings. Recent advances in natural language processing techniques allow us to extract such information from unstructured portions of text at a large scale and in a cost effective manner. We aimed to identify cognitive problems in the clinical records of a large sample of patients with schizophrenia, and assess their association with clinical outcomes. Methods: We developed a natural language processing based application identifying cognitive dysfunctions from the free text of medical records, and assessed its performance against a rating scale widely used in the United Kingdom, the cognitive component of the Health of the Nation Outcome Scales (HoNOS). Furthermore, we analyzed cognitive trajectories over the course of patient treatment, and evaluated their relationship with various socio-demographic factors and clinical outcomes. Results: We found a high prevalence of cognitive impairments in patients with schizophrenia, and a strong correlation with several socio-demographic factors (gender, education, ethnicity, marital status, and employment) as well as adverse clinical outcomes. Results obtained from the free text were broadly in line with those obtained using the HoNOS subscale, and shed light on additional associations, notably related to attention and social impairments for patients with higher education. Conclusions: Our findings demonstrate that cognitive problems are common in patients with schizophrenia, can be reliably extracted from clinical records using natural language processing, and are associated with adverse clinical outcomes. Harvesting the free text from medical records provides a larger coverage in contrast to neurocognitive batteries or rating scales, and access to additional socio-demographic and clinical variables. Text mining tools can therefore facilitate large scale patient screening and early symptoms detection, and ultimately help inform clinical decisions.",cognition | data mining | electronic health records | natural language processing | schizophrenia
"INTRODUCTION: The use of clinically derived data from electronic health records (EHRs) and other electronic clinical systems can greatly facilitate clinical research as well as operational and quality initiatives. One approach for making these data available is to incorporate data from different sources into a joint data warehouse. When using such a data warehouse, it is important to understand the quality of the data. The primary objective of this study was to determine the completeness and concordance of common types of clinical data available in the Knowledge Program (KP) joint data warehouse, which contains feeds from several electronic systems including the EHR. METHODS: A manual review was performed of specific data elements for 250 patients from an EHR, and these were compared with corresponding elements in the KP data warehouse. Completeness and concordance were calculated for five categories of data including demographics, vital signs, laboratory results, diagnoses, and medications. RESULTS: In general, data elements for demographics, vital signs, diagnoses, and laboratory results were present in more cases in the source EHR compared to the KP. When data elements were available in both sources, there was a high concordance. In contrast, the KP data warehouse documented a higher prevalence of deaths and medications compared to the EHR. DISCUSSION: Several factors contributed to the discrepancies between data in the KP and the EHR-including the start date and frequency of data feeds updates into the KP, inability to transfer data located in nonstructured formats (e.g., free text or scanned documents), as well as incomplete and missing data variables in the source EHR. CONCLUSION: When evaluating the quality of a data warehouse with multiple data sources, assessing completeness and concordance between data set and source data may be better than designating one to be a gold standard. This will allow the user to optimize the method and timing of data transfer in order to capture data with better accuracy.",Data use and quality | health information technology | quality
"BACKGROUND: Workload from electronic health record (EHR) inbox notifications leads to information overload and contributes to job dissatisfaction and physician burnout. Better understanding of physicians' inbox requirements and workflows could optimize inbox designs, enhance efficiency, and reduce safety risks from information overload. DESIGN: We conducted a mixed-methods study to identify strategies to enhance EHR inbox design and workflow. First, we performed a secondary analysis of national survey data of all Department of Veterans Affairs (VA) primary care practitioners (PCP) to identify major themes in responses to a free-text question soliciting suggestions to improve EHR inbox design and workflows. We then conducted expert interviews of clinicians at five health care systems (1 VA and 4 non-VA settings using 4 different EHRs) to understand existing optimal strategies to improve efficiency and situational awareness related to EHR inbox use. Themes from survey data were cross-validated with interview findings. RESULTS: We analyzed responses from 2104 PCPs who completed the free-text inbox question (of 5001 PCPs who responded to survey) and used an inductive approach to identify five themes: (1) Inbox notification content should be actionable for patient care and relevant to recipient clinician, (2) Inboxes should reduce risk of losing messages, (3) Inbox functionality should be optimized to improve efficiency of processing notifications, (4) Team support should be leveraged to help with EHR inbox notification burden, (5) Sufficient time should be provided to all clinicians to process EHR inbox notifications. We subsequently interviewed 15 VA and non-VA clinicians and identified 11 unique strategies, each corresponding directly with one of these five themes. CONCLUSION: Feedback from practicing end-user clinicians provides robust evidence to improve content and design of the EHR inbox and related clinical workflows and organizational policies. Several strategies we identified could improve clinicians' EHR efficiency and satisfaction as well as empower them to work with their local administrators, health IT personnel, and EHR developers to improve these systems.",burnout | efficiency | electronic health records | health information technology | medical informatics | situational awareness
"Electronic health records (EHR) contain a large amount of structured data and free text. Exploring and sharing clinical data can improve healthcare and facilitate the development of medical software. However, revealing confidential information is against ethical principles and laws. We de-identified a Danish EHR database with 437,164 patients. The goal was to generate a version with real medical records, but related to artificial persons. We developed a de-identification algorithm that uses lists of named entities, simple language analysis, and special rules. Our algorithm consists of 3 steps: collect lists of identifiers from the database and external resources, define a replacement for each identifier, and replace identifiers in structured data and free text. Some patient records could not be safely de-identified, so the de-identified database has 323,122 patient records with an acceptable degree of anonymity, readability and correctness (F-measure of 95%). The algorithm has to be adjusted for each culture, language and database.",
"Chaplaincy care is different for every patient; a growing challenge is to ensure that electronic health records function to support personalized care. While ICU health care teams have advanced clinical practice guidelines to identify and integrate relevant aspects of the patient's story into whole person care, recommendations for documentation are rare. This qualitative study of over 400 free-text EHR notes offers unique insight into current use of free-text documentation in ICU by six chaplains integrated into the healthcare team. Our research provides insight into the phenomena chaplains record in the electronic record. Content analysis shows recurrent report of patient and family practices, beliefs, coping mechanisms, concerns, emotional resources and needs, family and faith support, medical decision making and medical communications. These findings are important for health care team discussions of factors deemed essential to whole person care in ICUs, and, by extension have the potential to support the development of EHR designs that aim to advance personalized care.",Chaplaincy care | documentation | electronic health record | spiritual care | team communication
"CONTEXT: Clinicians document cancer patients' symptoms in free-text format within electronic health record visit notes. Although symptoms are critically important to quality of life and often herald clinical status changes, computational methods to assess the trajectory of symptoms over time are woefully underdeveloped. OBJECTIVES: To create machine learning algorithms capable of extracting patient-reported symptoms from free-text electronic health record notes. METHODS: The data set included 103,564 sentences obtained from the electronic clinical notes of 2695 breast cancer patients receiving paclitaxel-containing chemotherapy at two academic cancer centers between May 1996 and May 2015. We manually annotated 10,000 sentences and trained a conditional random field model to predict words indicating an active symptom (positive label), absence of a symptom (negative label), or no symptom at all (neutral label). Sentences labeled by human coder were divided into training, validation, and test data sets. Final model performance was determined on 20% test data unused in model development or tuning. RESULTS: The final model achieved precision of 0.82, 0.86, and 0.99 and recall of 0.56, 0.69, and 1.00 for positive, negative, and neutral symptom labels, respectively. The most common positive symptoms were pain, fatigue, and nausea. Machine-based labeling of 103,564 sentences took two minutes. CONCLUSION: We demonstrate the potential of machine learning to gather, track, and analyze symptoms experienced by cancer patients during chemotherapy. Although our initial model requires further optimization to improve the performance, further model building may yield machine learning methods suitable to be deployed in routine clinical care, quality improvement, and research applications.",Machine learning | breast cancer | electronic health record | natural language processing | palliative care | patient-reported symptoms
"Chronic fatigue syndrome (CFS) is a long-term illness with a wide range of symptoms and condition trajectories. To improve the understanding of these, automated analysis of large amounts of patient data holds promise. Routinely documented assessments are useful for large-scale analysis, however relevant information is mainly in free text. As a first step to extract symptom and condition trajectories, natural language processing (NLP) methods are useful to identify important textual content and relevant information. In this paper, we propose an agnostic NLP method of extracting segments of patients' clinical histories in CFS assessments. Moreover, we present initial results on the advantage of using these segments to quantify and analyse the presence of certain clinically relevant concepts.",Chronic Fatigue Syndrome | Clinical Informatics | Electronic Health Records | Natural Language Processing
"BACKGROUND: Personal electronic health records (PHR) are considered instrumental in improving health care quality and efficiency, enhancing communication between all parties involved and strengthening the patient's role. Technical architectures, data privacy, and applicability issues have been discussed for many years. Nevertheless, nationwide implementation of a PHR is still pending in Germany despite legal regulations provided by the eHealth Act passed in 2015. Within the information technology for patient-oriented care project funded by the Federal Ministry of Education and Research (2012-2017), a Web-based personal electronic health record prototype (PEPA) was developed enabling patient-controlled information exchange across different care settings. Gastrointestinal cancer patients and general practitioners utilized PEPA during a 3-month trial period. Both patients and physicians authorized by them could view PEPA content online and upload or download files. OBJECTIVE: This paper aims to outline findings of the posttrial qualitative study carried out to evaluate user-reported experiences, perceptions, and perspectives, focusing on their interpretation of PEPA beyond technical usability and views on a future nationwide implementation. METHODS: Data were collected through semistructured guide-based interviews with 11 patients and 3 physicians (N=14). Participants were asked to share experiences, views of perceived implications, and perspectives towards nationwide implementation. Further data were generated through free-text fields in a subsequent study-specific patient questionnaire and researcher's notes. Data were pseudonymized, audiotaped, and transcribed verbatim. Content analysis was performed through the Framework Analysis approach. All qualitative data were systemized by using MAXQDA Analytics PRO 12 (Rel.12.3.1). Additionally, participant characteristics were analyzed descriptively using IBM SPSS Statistics Version 24. RESULTS: Users interpreted PEPA as a central medium containing digital chronological health-related documentation that simplifies information sharing across care settings. While patients consider the implementation of PEPA in Germany in the near future, physicians are more hesitant. Both groups believe in PEPA's concept, but share awareness of concerns about data privacy and older or impaired people's abilities to manage online records. Patients perceive benefits for involvement in treatment processes and continuity of care but worry about financing and the implementation of functionally reduced versions. Physicians consider integration into primary systems critical for interoperability but anticipate technical challenges, as well as resistance from older patients and colleagues. They omit clear positioning regarding PEPA's potential incremental value for health care organizations or the provider-patient relationship. CONCLUSIONS: Digitalization in German health care will continue to bring change, both organizational and in the physician-patient relationship. Patients endorse and expect a nationwide PEPA implementation, anticipating various benefits. Decision makers and providers need to contribute to closing modernization gaps by committing to new concepts and by invigorating transformed roles.",continuity of care | eHealth | nationwide implementation | personal patient-controlled electronic health record
"The important information about a patient is often stored in a free-form text to describe the events in the patient's medical history. In this work, we propose and evaluate a hybrid approach based on rules and syntactical analysis to normalise temporal expressions and assess uncertainty depending on the remoteness of the event. A dataset of 500 sentences was manually labelled to measure the accuracy. On this dataset, the accuracy of extracting temporal expressions is 95,5%, and the accuracy of normalization is 94%. The event extraction accuracy is 74.80%. The essential advantage of this work is the implementation of the considered approach for the non-English language where NLP tools are limited.",clinical text mining | corpus | machine learning | normalization | syntactical parsing | time expression extraction
"OBJECTIVE: There are concerns that structured electronic documentation systems can limit expressivity and encourage long and unreadable notes. We assessed the impact of an electronic clinical documentation system on the quality of admission notes for patients admitted to a general medical unit. METHODS: This was a prospective randomized crossover study comparing handwritten paper notes to electronic notes on different patients by the same author, generated using a semistructured electronic admission documentation system over a 2-month period in 2014. The setting was a 4-team, 80-bed general internal medicine clinical teaching unit at a large urban academic hospital. The quality of clinical documentation was assessed using the QNOTE instrument (best possible score = 100), and word counts were assessed for free-text sections of notes. RESULTS: Twenty-one electronic-paper note pairs (42 notes) written by 21 authors were randomly drawn from a pool of 303 eligible notes. Overall note quality was significantly higher in electronic vs paper notes (mean 90 vs 69, P < .0001). The quality of free-text subsections (History of Present Illness and Impression and Plan) was significantly higher in the electronic vs paper notes (mean 93 vs 78, P < .0001; and 89 vs 77, P = .001, respectively). The History of Present Illness subsection was significantly longer in electronic vs paper notes (mean 172.4 vs 92.4 words, P = .0001). CONCLUSIONS: An electronic admission documentation system improved both the quality of free-text content and the overall quality of admission notes. Authors wrote more in the free-text sections of electronic documents as compared to paper versions.",QNOTE | clinical documentation | electronic health records | health care quality | note quality
"OBJECTIVE: Although the value of collecting occupational data is well-established, these data are not systematically collected in clinical practice. We assessed the availability of electronic health record (EHR)-based occupation data within a large integrated health care system to determine the feasibility of its use in research. MATERIALS AND METHODS: We used a mixed-methods approach to extract EHR data and define employment status, employer, and employment industry of 1107 colorectal cancer survivors. This was a secondary analysis of a subset of the Patient Outcomes Research to Advance Learning (PORTAL) colorectal cancer cohort. RESULTS: We categorized the employment industry for 46% of the cohort. Employment status was available for 58% of the cohort. The employer was missing for over 95% of the cohort. CONCLUSION: By combining data from structured and free-text EHR fields, we identified employment status and industry for approximately half of our sample. Findings demonstrate limitations of EHR data and underscore the need for systematic collection of occupation data in clinical practice.",cancer survivors | electronic health record | employment status | occupation
"BACKGROUND: Data in medical records have in part been recorded in structured and coded forms for some decades. However, the patient history is as yet largely recorded in an uncoded format. There is a need to consider the optimal balance of use of free text and coded data in the patient history. This review protocol summarises our plans to identify, critically appraise and synthesise evidence relating to approaches taken to introduce structure and coding within patient histories in electronic health records, and the empirically demonstrated benefits and risks of structuring and coding of patient histories in health records. OBJECTIVES: To determine how structured and coded data are being introduced for the recording of patient histories, the benefits observed where structuring and coding have been introduced and the risks encountered when structuring and coding are introduced. METHODS: We will search the following databases for evidence of published and unpublished material: CINAHL; EMBASE; Google Scholar; IndMED; LILACS; MEDLINE; NIHR; Paklit and PsycINFO. We will, depending on the study designs employed, use the Cochrane EPOC, Joanna Briggs Institute (JBI) and Newcastle-Ottawa instruments to critically appraise studies. Data synthesis is likely to be undertaken using a narrative approach, although meta-analysis will also be undertaken if appropriate and if the data allow this. RESULTS: This protocol should represent a reproducible approach to reviewing the literature regarding structuring and coding in patient histories. We anticipate that we will be able to report results in early 2011. CONCLUSION: The review should offer increased clarity and direction on the optimal balance between structuring/coding and free text recording of data relating to the patient history.",
"BACKGROUND: Health care systems rely on electronic patient data, yet access to breast tissue pathology results continues to depend on interpreting dictated free-text reports. OBJECTIVE: The objective was to develop a method to electronically search and categorize pathologic diagnoses of patients' breast tissue specimens from dictated free-text pathology reports in a large health system for multiple users including clinicians. DESIGN: A database integrating existing patient-level administrative and clinical information for breast cancer screening and diagnostic services and a web-based application for comprehensive searching of pathology reports were developed by a health system team led by pathologists. The Breast Pathology Assessment Tool and Hierarchy for Diagnosis (BPATH-Dx) provided search terms and guided electronic transcription of diagnoses from text fields on breast pathology clinical reports to standardized categories. APPROACH: Breast pathology encounters in the pathology database were matched with administrative data for 7332 women with breast tissue specimens obtained from an initial procedure in the health system from January 1, 2008 to December 31, 2011. Sequential queries of the pathology text based on BPATH-Dx categorized biopsies according to their worst pathological diagnosis, as is standard practice. Diagnoses ranged from invasive breast cancer (23.3%), carcinoma in situ (7.8%), atypical lesions (6.39%), proliferative lesions without atypia (27.9%), and nonproliferative lesions (34.7%), and were further classified into subcategories. A random sample of 5% of reports that were manually reviewed indicated 97.5% agreement. CONCLUSIONS: Sequential queries of free-text pathology reports guided by a standardized assessment tool in conjunction with a web-based search application provide an efficient and reproducible approach to accessing nonmalignant breast pathology diagnoses. This method advances the use of pathology data and electronic health records to improve health care quality, patient care, outcomes, and research.",Breast biopsy | breast pathology | electronic data systems
"BACKGROUND: The prescriber's directions to the patient (Sig) are one of the most quality-sensitive components of a prescription order. Owing to their free-text format, the Sig data that are transmitted in electronic prescriptions (e-prescriptions) have the potential to produce interpretation challenges at receiving pharmacies that may threaten patient safety and also negatively affect medication labeling and patient counseling. Ensuring that all data transmitted in the e-prescription are complete and unambiguous is essential for minimizing disruptions in workflow at prescribers' offices and receiving pharmacies and optimizing the safety and effectiveness of patient care. OBJECTIVES: To (a) assess the quality and variability of free-text Sig strings in ambulatory e-prescriptions and (b) propose best-practice recommendations to improve the use of this quality-sensitive field. METHODS: A retrospective qualitative analysis was performed on a nationally representative sample of 25,000 e-prescriptions issued by 22,152 community-based prescribers across the United States using 501 electronic health records (EHRs) or e-prescribing software applications. The content of Sig text strings in e-prescriptions was classified according to a Sig classification scheme developed with guidance from an expert advisory panel. The Sig text strings were also analyzed for quality-related events (QREs). For purposes of this analysis, QREs were defined as Sig text content that could impair accurate and unambiguous interpretation by staff at receiving pharmacies. RESULTS: A total of 3,797 unique Sig concepts were identified in the 25,000 Sig text strings analyzed; more than 50% of all Sigs could be categorized into 25 unique Sig concepts. Even Sig strings that expressed apparently simple and straightforward concepts displayed substantial variability; for example, the sample contained 832 permutations of words and phrases used to convey the Sig concept of ""Take 1 tablet by mouth once daily."" Approximately 10% of Sigs contained QREs that could pose patient safety risks or workflow disruptions that could necessitate pharmacist callbacks to prescribers for clarification or other manual interventions. CONCLUSIONS: The quality of free-text patient directions in e-prescriptions can vary dramatically. However, more than half of all patient directions sent in the ambulatory setting can be categorized into only 25 Sig concepts. This suggests an immediate, practical opportunity to improve patient safety and workflow efficiency for both prescribers and pharmacies. Recommendations include implementing enhancements to Sig creation tools in e-prescribing and EHR software applications, adoption of the Structured and Codified Sig format supported by the current national e-prescribing standard, and improved usability testing and end-user training for generating complete and unambiguous patient directions. Such quality improvements are essential for optimizing the safety and effectiveness of patient care as well as for minimizing workflow disruptions to both prescribers and pharmacies. DISCLOSURES: This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors. Yang, Ward-Charlerie, Dhavle, and Green are employed by Surescripts. Rupp reported receiving consulting fees from Surescripts during the conduct of this study. No other disclosures were reported. The content in this article is solely the responsibility of the authors and does not necessarily represent the official views of Surescripts and Midwestern University or any of the affiliated institutions of the authors. Study concept and design were contributed by all the authors. Yang and Ward-Charlerie collected the data, and data interpretion was performed by Yang, Ward-Charlerie and Dhavle. The manuscript was primarily written by Yang, along with Dhavle and Green, and revised by Yang, Dhavle, Rupp, and Green.",
"BACKGROUND: Documentation burden is a common problem with modern electronic health record (EHR) systems. To reduce this burden, various recording methods (eg, voice recorders or motion sensors) have been proposed. However, these solutions are in an early prototype phase and are unlikely to transition into practice in the near future. A more pragmatic alternative is to directly modify the implementation of the existing functionalities of an EHR system. OBJECTIVE: This study aims to assess the nature of free-text comments entered into EHR flowsheets that supplement quantitative vital sign values and examine opportunities to simplify functionality and reduce documentation burden. METHODS: We evaluated 209,055 vital sign comments in flowsheets that were generated in the Epic EHR system at the Vanderbilt University Medical Center in 2018. We applied topic modeling, as well as the natural language processing Clinical Language Annotation, Modeling, and Processing software system, to extract generally discussed topics and detailed medical terms (expressed as probability distribution) to investigate the stories communicated in these comments. RESULTS: Our analysis showed that 63.33% (6053/9557) of the users who entered vital signs made at least one free-text comment in vital sign flowsheet entries. The user roles that were most likely to compose comments were registered nurse, technician, and licensed nurse. The most frequently identified topics were the notification of a result to health care providers (0.347), the context of a measurement (0.307), and an inability to obtain a vital sign (0.224). There were 4187 unique medical terms that were extracted from 46,029 (0.220) comments, including many symptom-related terms such as ""pain,"" ""upset,"" ""dizziness,"" ""coughing,"" ""anxiety,"" ""distress,"" and ""fever"" and drug-related terms such as ""tylenol,"" ""anesthesia,"" ""cannula,"" ""oxygen,"" ""motrin,"" ""rituxan,"" and ""labetalol."" CONCLUSIONS: Considering that flowsheet comments are generally not displayed or automatically pulled into any clinical notes, our findings suggest that the flowsheet comment functionality can be simplified (eg, via structured response fields instead of a text input dialog) to reduce health care provider effort. Moreover, rich and clinically important medical terms such as medications and symptoms should be explicitly recorded in clinical notes for better visibility.",content analysis | documentation burden | electronic health system | flowsheets | free text | vital sign comments
"Chief complaints are important textual data that can serve to enrich diagnosis and symptom data in electronic health record (EHR) systems. In this study, a method is presented to preprocess chief complaints and assign corresponding ICD-10-CM codes using the MetaMap natural language processing (NLP) system and Unified Medical Language System (UMLS) Metathesaurus. An exploratory analysis was conducted using a set of 7,942 unique chief complaints from the statewide health information exchange containing EHR data from hospitals across Rhode Island. An evaluation of the proposed method was then performed using a set of 123,086 chief complaints with corresponding ICD-10-CM encounter diagnoses. With 87.82% of MetaMap-extracted concepts correctly assigned, the preliminary findings support the potential use of the method explored in this study for improving upon existing NLP techniques for enabling use of data captured within chief complaints to support clinical care, research, and public health surveillance.",
"PURPOSE: With increasing volumes of electronic health record data, algorithm-driven extraction may aid manual extraction. Visual acuity often is extracted manually in vision research. The total visual acuity extraction algorithm (TOVA) is presented and validated for automated extraction of visual acuity from free text, unstructured clinical notes. METHODS: Consecutive inpatient ophthalmology notes over an 8-year period from the University of Washington healthcare system in Seattle, WA were used for validation of TOVA. The total visual acuity extraction algorithm applied natural language processing to recognize Snellen visual acuity in free text notes and assign laterality. The best corrected measurement was determined for each eye and converted to logMAR. The algorithm was validated against manual extraction of a subset of notes. RESULTS: A total of 6266 clinical records were obtained giving 12,452 data points. In a subset of 644 validated notes, comparison of manually extracted data versus TOVA output showed 95% concordance. Interrater reliability testing gave κ statistics of 0.94 (95% confidence interval [CI], 0.89-0.99), 0.96 (95% CI, 0.94-0.98), 0.95 (95% CI, 0.92-0.98), and 0.94 (95% CI, 0.90-0.98) for acuity numerators, denominators, adjustments, and signs, respectively. Pearson correlation coefficient was 0.983. Linear regression showed an R(2) of 0.966 (P < 0.0001). CONCLUSIONS: The total visual acuity extraction algorithm is a novel tool for extraction of visual acuity from free text, unstructured clinical notes and provides an open source method of data extraction. TRANSLATIONAL RELEVANCE: Automated visual acuity extraction through natural language processing can be a valuable tool for data extraction from free text ophthalmology notes.",clinical research | data mining | electronic health records | natural language processing | visual acuity
"OBJECTIVE: An accurate computable representation of food and drug allergy is essential for safe healthcare. Our goal was to develop a high-performance, easily maintained algorithm to identify medication and food allergies and sensitivities from unstructured allergy entries in electronic health record (EHR) systems. MATERIALS AND METHODS: An algorithm was developed in Transact-SQL to identify ingredients to which patients had allergies in a perioperative information management system. The algorithm used RxNorm and natural language processing techniques developed on a training set of 24 599 entries from 9445 records. Accuracy, specificity, precision, recall, and F-measure were determined for the training dataset and repeated for the testing dataset (24 857 entries from 9430 records). RESULTS: Accuracy, precision, recall, and F-measure for medication allergy matches were all above 98% in the training dataset and above 97% in the testing dataset for all allergy entries. Corresponding values for food allergy matches were above 97% and above 93%, respectively. Specificities of the algorithm were 90.3% and 85.0% for drug matches and 100% and 88.9% for food matches in the training and testing datasets, respectively. DISCUSSION: The algorithm had high performance for identification of medication and food allergies. Maintenance is practical, as updates are managed through upload of new RxNorm versions and additions to companion database tables. However, direct entry of codified allergy information by providers (through autocompleters or drop lists) is still preferred to post-hoc encoding of the data. Data tables used in the algorithm are available for download. CONCLUSIONS: A high performing, easily maintained algorithm can successfully identify medication and food allergies from free text entries in EHR systems.",Allergies | Electronic health records | Electronic medical records | Hypersensitivity | Natural language processing | RxNorm
"CONTEXT: For patients with cancer, uncontrolled pain and other symptoms are the leading cause of unplanned hospitalizations. Early access to specialty palliative care (PC) is effective to reduce symptom burden, but more efficient approaches are needed for rapid identification and referral. Information on symptom burden largely exists in free-text notes, limiting its utility as a trigger for best practice alerts or automated referrals. OBJECTIVES: To evaluate whether natural language processing (NLP) can be used to identify uncontrolled symptoms (pain, dyspnea, or nausea/vomiting) in the electronic health record (EHR) among hospitalized cancer patients with advanced disease. METHODS: The dataset included 1,644 hospitalization encounters for cancer patients admitted from 1/2017 -6/2019. We randomly sampled 296 encounters, which included 15,580 clinical notes. We manually reviewed the notes and recorded symptom severity. The primary endpoint was an indicator for whether a symptom was labeled as ""controlled"" (none, mild, not reported) or as ""uncontrolled"" (moderate or severe). We randomly split the data into training and test sets and used the Random Forest algorithm to evaluate final model performance. RESULTS: Our models predicted presence of an uncontrolled symptom with the following performance: pain with 61% accuracy, 69% sensitivity, and 46% specificity (F1: 69.5); nausea/vomiting with 68% accuracy, 21% sensitivity, and 90% specificity (F1: 29.4); and dyspnea with 80% accuracy, 22% sensitivity, and 88% specificity (F1: 21.1). CONCLUSION: This study demonstrated initial feasibility of using NLP to identify hospitalized cancer patients with uncontrolled symptoms. Further model development is needed before these algorithms could be implemented to trigger early access to PC.",cancer symptoms | electronic health record | machine learning | natural language processing | palliative care
"OBJECTIVE: The aim of this study was to investigate (1) why ordering clinicians use free-text orders to communicate medication information; (2) what risks physicians and nurses perceive when free-text orders are used for communicating medication information; and (3) how electronic health records (EHRs) could be improved to encourage the safe communication of medication information. METHODS: We performed semi-structured, scenario-based interviews with eight physicians and eight nurses. Interview responses were analyzed and grouped into common themes. RESULTS: Participants described eight reasons why clinicians use free-text medication orders, five risks relating to the use of free-text medication orders, and five recommendations for improving EHR medication-related communication. Poor usability, including reduced efficiency and limited functionality associated with structured order entry, was the primary reason clinicians used free-text orders to communicate medication information. Common risks to using free-text orders for medication communication included the increased likelihood of missing orders and the increased workload on nurses responsible for executing orders. DISCUSSION: Clinicians' use of free-text orders is primarily due to limitations in the current structured order entry design. To encourage the safe communication of medication information between clinicians, the EHR's structured order entry must be redesigned to support clinicians' cognitive and workflow needs that are currently being addressed via the use of free-text orders. CONCLUSION: Clinicians' use of free-text orders as a workaround to insufficient structured order entry can create unintended patient safety risks. Thoughtful solutions designed to address these workarounds can improve the medication ordering process and the subsequent medication administration process.",
"BACKGROUND: Clinical natural language processing (cNLP) systems are of crucial importance due to their increasing capability in extracting clinically important information from free text contained in electronic health records (EHRs). The conversion of a nonstructured representation of a patient's clinical history into a structured format enables medical doctors to generate clinical knowledge at a level that was not possible before. Finally, the interpretation of the insights gained provided by cNLP systems has a great potential in driving decisions about clinical practice. However, carrying out robust evaluations of those cNLP systems is a complex task that is hindered by a lack of standard guidance on how to systematically approach them. OBJECTIVE: Our objective was to offer natural language processing (NLP) experts a methodology for the evaluation of cNLP systems to assist them in carrying out this task. By following the proposed phases, the robustness and representativeness of the performance metrics of their own cNLP systems can be assured. METHODS: The proposed evaluation methodology comprised five phases: (1) the definition of the target population, (2) the statistical document collection, (3) the design of the annotation guidelines and annotation project, (4) the external annotations, and (5) the cNLP system performance evaluation. We presented the application of all phases to evaluate the performance of a cNLP system called ""EHRead Technology"" (developed by Savana, an international medical company), applied in a study on patients with asthma. As part of the evaluation methodology, we introduced the Sample Size Calculator for Evaluations (SLiCE), a software tool that calculates the number of documents needed to achieve a statistically useful and resourceful gold standard. RESULTS: The application of the proposed evaluation methodology on a real use-case study of patients with asthma revealed the benefit of the different phases for cNLP system evaluations. By using SLiCE to adjust the number of documents needed, a meaningful and resourceful gold standard was created. In the presented use-case, using as little as 519 EHRs, it was possible to evaluate the performance of the cNLP system and obtain performance metrics for the primary variable within the expected CIs. CONCLUSIONS: We showed that our evaluation methodology can offer guidance to NLP experts on how to approach the evaluation of their cNLP systems. By following the five phases, NLP experts can assure the robustness of their evaluation and avoid unnecessary investment of human and financial resources. Besides the theoretical guidance, we offer SLiCE as an easy-to-use, open-source Python library.",clinical natural language processing | electronic health records | gold standard | natural language processing | reference standard | sample size
"Electronic Medical Records (EMR) contain a lot of valuable data about patients, which is however unstructured. There is a lack of labeled medical text data in Russian and there are no tools for automatic annotation. We present an unsupervised approach to medical data annotation. Morphological and syntactical analyses of initial sentences produce syntactic trees, from which similar subtrees are then grouped by Word2Vec and labeled using dictionaries and Wikidata categories. This method can be used to automatically label EMRs in Russian and proposed methodology can be applied to other languages, which lack resources for automatic labeling and domain vocabularies.",automatic text labeling | electronic health records | graph algorithms | natural language processing | node2vec | syntactical parsing
"We present findings on using natural language processing to classify tobacco-related entries from problem lists found within patient's electronic health records. Problem lists describe health-related issues recorded during a patient's medical visit; these problems are typically followed up upon during subsequent visits and are updated for relevance or accuracy. The mechanics of problem lists vary across different electronic health record systems. In general, they either manifest as pre-generated generic problems that may be selected from a master list or as text boxes where a healthcare professional may enter free text describing the problem. Using commonly-available natural language processing tools, we classified tobacco-related problems into three classes: active-user, former-user, and non-user; we further demonstrate that rule-based post-processing may significantly increase precision in identifying these classes (+32%, +22%, +35% respectively). We used these classes to generate tobacco time-spans that reconstruct a patient's tobacco-use history and better support secondary data analysis. We bundle this as an open-source toolkit with flow visualizations indicating how patient tobacco-related behavior changes longitudinally, which can also capture and visualize contradicting information such as smokers being flagged as having never smoked.",
"OBJECTIVE: To conduct a data validation study encompassing an accuracy assessment of the data extraction process for the Axon Registry®. METHODS: Data elements were abstracted from electronic health records (EHRs) by an external auditor (IQVIA) using virtual site visits at participating sites. IQVIA independently calculated Axon Registry quality measure performance rates based on American Academy of Neurology measure specifications and logic using Axon Registry data. Agreement between Axon Registry and IQVIA data elements and measure performance rates was calculated. Discordance was investigated to elucidate underlying systemic or idiosyncratic reasons for disagreement. RESULTS: Nine sites (n = 720 patients; n = 80 patients per site) with diversity among EHR vendor, practice settings, size, locations, and data transfer method were included. There was variable concordance between the data elements in the Axon Registry and those abstracted independently by IQVIA; high match rates (≥92%) were observed for discrete elements (e.g., demographics); lower match rates (<44%) were observed for elements with free text (e.g., plan of care). Across all measures, there was a 76% patient-level measure performance agreement between Axon Registry and IQVIA (κ = 0.53, p < 0.001). CONCLUSION: There was a range of concordance between data elements and quality measures in the Axon Registry and those independently abstracted and calculated by an independent vendor. Validation of data and processes is important for the Axon Registry as a clinical quality data registry that utilizes automated data extraction methods from the EHR. Implementation of remediation strategies to improve data accuracy will support the ability of the Axon Registry to perform accurate quality reporting.",
"BACKGROUND: Antimicrobial resistance is a growing, global public health crisis, due in large part to the overuse and misuse of antibiotics. Understanding medication allergy data and allergy reactions that are documented in electronic health records (EHRs) can help to identify opportunities to improve the quality of documentation of beta-lactam allergies, thus potentially reducing the prescribing of alternative antibiotics. METHODS: Medication allergies and allergy reactions recorded in the EHR for 319 051 patients seen across 32 community health centers were reviewed. Patients with a beta-lactam allergy recorded in their EHR were identified. Free text, as well as standardized allergy and allergy reaction fields, were analyzed. RESULTS: Among patients, 9.1% (n = 29 095) had evidence of a beta-lactam allergy recorded in their EHR. Women, white, and non-Hispanic patients were more likely to have a documented allergy compared to men, black, and Hispanic patients. Among all patients with a documented beta-lactam allergy, 36.2% had an empty or missing allergy reaction description in their EHR. CONCLUSIONS: Findings suggest that current EHR documentation practices among the health centers reviewed do not provide enough information on allergic reactions to allow providers to discern between true allergies and common, but anticipated, drug side effects. Improved EHR documentation guidance, training that reinforces the use of standardized data and more detailed recording of allergic reactions, combined with initiatives to address patient barriers including health literacy, may help to improve the accuracy of drug allergies in patients' records. These initiatives, combined with antimicrobial stewardship programs, can help to reduce inappropriate prescribing of alternative antibiotics when beta-lactam antibiotics are first-line and can be tolerated.",antibiotic resistance | beta-lactam antibiotics | community health centers | drug allergy | electronic health record
"INTRODUCTION: Applying Fair Information Practice principles to electronic health records (EHRs) requires allowing patient control over who views their data. METHODS: We designed a program that captures patients' preferences for provider access to an urban health system's EHR. Patients could allow or restrict providers' access to all data (diagnoses, medications, test results, reports, etc.) or only highly sensitive data (sexually transmitted infections, HIV/AIDS, drugs/alcohol, mental or reproductive health). Except for information in free-text reports, we redacted EHR data shown to providers according to patients' preferences. Providers could ""break the glass"" to display redacted information. We prospectively studied this system in one primary care clinic, noting redactions and when users ""broke the glass,"" and surveyed providers about their experiences and opinions. RESULTS: Eight of nine eligible clinic physicians and all 23 clinic staff participated. All 105 patients who enrolled completed the preference program. Providers did not know which of their patients were enrolled, nor their preferences for accessing their EHRs. During the 6-month prospective study, 92 study patients (88 %) returned 261 times, during which providers viewed their EHRs 126 times (48 %). Providers ""broke the glass"" 102 times, 92 times for patients not in the study and ten times for six returning study patients, all of whom had restricted EHR access. Providers ""broke the glass"" for six (14 %) of 43 returning study patients with redacted data vs. zero among 49 study patients without redactions (p = 0.01). Although 54 % of providers agreed that patients should have control over who sees their EHR information, 58 % believed restricting EHR access could harm provider-patient relationships and 71 % felt quality of care would suffer. CONCLUSIONS: Patients frequently preferred restricting provider access to their EHRs. Providers infrequently overrode patients' preferences to view hidden data. Providers believed that restricting EHR access would adversely impact patient care. Applying Fair Information Practice principles to EHRs will require balancing patient preferences, providers' needs, and health care quality.",
"Electronic Health Records (EHRs) have made patient information widely available, allowing health professionals to provide better care. However, information confidentiality is an issue that continually needs to be taken into account. The objective of this study is to describe the implementation of rule-based access permissions to an EHR system. The rules that were implemented were based on a qualitative study. Every time users did not meet the specified requirements, they had to justify access through a pop up window with predetermined options, including a free text option (""other justification""). A secondary analysis of a deidentified database was performed. From a total of 20,540,708 hits on the electronic medical record database, 85% of accesses to the EHR system did not require justification. Content analysis of the ""Other Justification"" option allowed the identification of new types of access. At the time to justify, however, users may choose the faster or less clicks option to access to EHR, associating the justification of access to the EHR as a barrier.",
"The free text in electronic health records (EHRs) conveys a huge amount of clinical information about health state and patient history. Despite a rapidly growing literature on the use of machine learning techniques for extracting this information, little effort has been invested toward feature selection and the features' corresponding medical interpretation. In this study, we focus on the task of early detection of anastomosis leakage (AL), a severe complication after elective surgery for colorectal cancer (CRC) surgery, using free text extracted from EHRs. We use a bag-of-words model to investigate the potential for feature selection strategies. The purpose is earlier detection of AL and prediction of AL with data generated in the EHR before the actual complication occur. Due to the high dimensionality of the data, we derive feature selection strategies using the robust support vector machine linear maximum margin classifier, by investigating: 1) a simple statistical criterion (leave-one-out-based test); 2) an intensive-computation statistical criterion (Bootstrap resampling); and 3) an advanced statistical criterion (kernel entropy). Results reveal a discriminatory power for early detection of complications after CRC (sensitivity 100%; specificity 72%). These results can be used to develop prediction models, based on EHR data, that can support surgeons and patients in the preoperative decision making phase.",
"BACKGROUND: In France, recent developments in healthcare system organization have aimed at strengthening decision-making and action in public health at the regional level. Firstly, the 2004 Public Health Act, by setting 100 national and regional public health targets, introduced an evaluative approach to public health programs at the national and regional levels. Meanwhile, the implementation of regional platforms for managing electronic health records (EHRs) has also been under assessment to coordinate the deployment of this important instrument of care within each geographic area. In this context, the development and implementation of a regional approach to epidemiological data extracted from EHRs are an opportunity that must be seized as soon as possible. Our article addresses certain design and organizational aspects so that the technical requirements for such use are integrated into regional platforms in France. The article will base itself on organization of the Rhône-Alpes regional health platform. DISCUSSION: Different tools being deployed in France allow us to consider the potential of these regional platforms for epidemiology and public health (implementation of a national health identification number and a national information system interoperability framework). The deployment of the Rhône-Alpes regional health platform began in the 2000s in France. By August 2011, 2.6 million patients were identified in this platform. A new development step is emerging because regional decision-makers need to measure healthcare efficiency. To pool heterogeneous information contained in various independent databases, the format, norm and content of the metadata have been defined. Two types of databases will be created according to the nature of the data processed, one for extracting structured data, and the second for extracting non-structured and de-identified free-text documents. SUMMARY: Regional platforms for managing EHRs could constitute an important data source for epidemiological surveillance in the context of epidemic alerts, but also in monitoring a number of indicators of infectious and chronic diseases for which no data are yet available in France.",
"The present-day health data ecosystem comprises a wide array of complex heterogeneous data sources. A wide range of clinical, health care, social and other clinically relevant information are stored in these data sources. These data exist either as structured data or as free-text. These data are generally individual person-based records, but social care data are generally case based and less formal data sources may be shared by groups. The structured data may be organised in a proprietary way or be coded using one-of-many coding, classification or terminologies that have often evolved in isolation and designed to meet the needs of the context that they have been developed. This has resulted in a wide range of semantic interoperability issues that make the integration of data held on these different systems changing. We present semantic interoperability challenges and describe a classification of these. We propose a four-step process and a toolkit for those wishing to work more ontologically, progressing from the identification and specification of concepts to validating a final ontology. The four steps are: (1) the identification and specification of data sources; (2) the conceptualisation of semantic meaning; (3) defining to what extent routine data can be used as a measure of the process or outcome of care required in a particular study or audit and (4) the formalisation and validation of the final ontology. The toolkit is an extension of a previous schema created to formalise the development of ontologies related to chronic disease management. The extensions are focused on facilitating rapid building of ontologies for time-critical research studies.",data integration | electronic health records | interoperability | ontology | semantic
"Natural language processing (NLP) technologies provide an opportunity to extract key patient data from free text documents within the electronic health record (EHR). We are developing a series of components from which to construct NLP pipelines. These pipelines typically begin with a component whose goal is to label sections within medical documents with codes indicating the anticipated semantics of their content. This Clinical Section Labeler prepares the document for further, focused information extraction. Below we describe the evaluation of six algorithms designed for use in a Clinical Section Labeler. These algorithms are trained with N-gram-based feature sets extracted from document sections and the document types. In the evaluation, 6 different Bayesian models were trained and used to assign one of 27 different topics to each section. A tree-augmented Bayesian network using the document type and N-grams derived from section headers proved most accurate in assigning individual sections appropriate section topics.",
"Clinical and pathological stage are defining parameters in oncology, which direct a patient's treatment options and prognosis. Pathology reports contain a wealth of staging information that is not stored in structured form in most electronic health records (EHRs). Therefore, we evaluated three supervised machine learning methods (Support Vector Machine, Decision Trees, Gradient Boosting) to classify free-text pathology reports for prostate cancer into T, N and M stage groups.",Natural Language Processing | Neoplasm Staging | Prostate Cancer
"BACKGROUND: Examination of clinical data routinely recorded in general practice provides significant opportunities for identifying and quantifying medicine-related adverse events not captured by spontaneous adverse reaction reporting systems. Robust pharmacovigilance methods for detecting and monitoring adverse events due to treatment with new and existing medicines are required to estimate the true extent of adverse events experienced by primary care patients. OBJECTIVES: The aim of the study was to examine evidence of adverse events contained in general practice electronic records and to study observed events related to selective serotonin reuptake inhibitors (SSRIs) as an example of drug-specific pharmaceutical surveillance achievable with these data. METHODS: Electronic clinical records for a cohort of 338 931 patients consulting from 2002 to 2007 were extracted from the patient management systems of 30 primary care clinics in New Zealand. Medical warnings files, prescription records and free text consultation notes were used to identify physician-recorded treatment cautions, including adverse events and medicines they were associated with. A structured chronological analysis of prescriptions, consultation notes and adverse events relating to patients prescribed the SSRI citalopram was undertaken, and included investigating reasons for switching treatment to another SSRI (fluoxetine or paroxetine) as a method for detecting evidence of drug safety signals. We compared the number of adverse events identified for patients at one practice with the number spontaneously reported to New Zealand's Centre for Adverse Reactions Monitoring (CARM). RESULTS: During the 6-year study period, 173 478 patients received 4 811 561 prescriptions. There were 37 397 allergies, adverse events and other warnings recorded for 24 994 patients (7.4%); adverse events relating to 65 different types of drug were reported. Medicines most frequently implicated in adverse event reports were antibacterials, analgesics, antihypertensive medicines, lipid-modifying agents and skin preparations. Citalopram was prescribed for 5612 patients, and 701 adverse events relating to citalopram were identified in the electronic health records of 473 (8.4%) patients. A total of 713 (12.7%) patients changed treatment from citalopram to another SSRI, and 164 reasons for the switch were identified: suspected adverse drug effects for 129 (78.7%), lack of effect for 29 (17.7%) and patient preference for 6 (3.7%). The most common adverse events preceding the switch were anxiety, nausea and headaches. Of the 725 adverse events and medical warnings recorded at one practice, 21 (2.9%) were spontaneously reported to the CARM. CONCLUSIONS: Routinely recorded general practice data provide a wealth of opportunities for monitoring drug safety signals and for other patient safety issues. Medical warning records and consultation notes contain a wealth of information on adverse events but structured search methodologies are often required to identify these.",
"The objective of the work is to extract healthcare process quality indicators from the literature, and to evaluate which of them could be automatically computed using routinely collected data from electronic health records (EHRs). A minimal set of data commonly available in EHRs is first defined. The initial bibliographic query enables to identify 8,744 papers, among which 126 papers describe 440 process indicators. 22.3% of indicators can be automatically computed. The computation of the indicators mostly require diagnoses (99%), drug prescriptions (59%), medical procedures (48%), administrative data (30%), laboratory results (20%), free-text reports with basic keyword research (19%), linkage with the patient's previous stays (11%) and dependence assessment (3%). 77.7% of indicators cannot be automatically computed, mostly because they require a linkage with outpatient data (61%), structured data that are usually not available (43%), unstructured data (26%) or the trace of an information that was given to the patient (8%).",
"Information extraction (IE), a natural language processing (NLP) task that automatically extracts structured or semi-structured information from free text, has become popular in the clinical domain for supporting automated systems at point-of-care and enabling secondary use of electronic health records (EHRs) for clinical and translational research. However, a high performance IE system can be very challenging to construct due to the complexity and dynamic nature of human language. In this paper, we report an IE framework for cohort identification using EHRs that is a knowledge-driven framework developed under the Unstructured Information Management Architecture (UIMA). A system to extract specific information can be developed by subject matter experts through expert knowledge engineering of the externalized knowledge resources used in the framework.",
"OBJECTIVE: The accuracy of bone metastases diagnostic coding based on International Classification of Diseases, ninth revision (ICD-9) is unknown for most large databases used for epidemiologic research in the US. Electronic health records (EHR) are the preferred source of data, but often clinically relevant data occur only as unstructured free text. We examined the validity of bone metastases ICD-9 coding in structured EHR and administrative claims relative to the complete (structured and unstructured) patient chart obtained through technology-enabled chart abstraction. PATIENTS AND METHODS: Female patients with breast cancer with ≥1 visit after November 2010 were identified from three community oncology practices in the US. We calculated sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) of bone metastases ICD-9 code 198.5. The technology-enabled abstraction displays portions of the chart to clinically trained abstractors for targeted review, thereby maximizing efficiency. We evaluated effects of misclassification of patients developing skeletal complications or treated with bone-targeting agents (BTAs), and timing of BTA. RESULTS: Among 8,796 patients with breast cancer, 524 had confirmed bone metastases using chart abstraction. Sensitivity was 0.67 (95% confidence interval [CI] =0.63-0.71) based on structured EHR, and specificity was high at 0.98 (95% CI =0.98-0.99) with corresponding PPV of 0.71 (95% CI =0.67-0.75) and NPV of 0.98 (95% CI =0.98-0.98). From claims, sensitivity was 0.78 (95% CI =0.74-0.81), and specificity was 0.98 (95% CI =0.98-0.98) with PPV of 0.72 (95% CI =0.68-0.76) and NPV of 0.99 (95% CI =0.98-0.99). Structured data and claims missed 17% of bone metastases (89 of 524). False negatives were associated with measurable overestimation of the proportion treated with BTA or with a skeletal complication. Median date of diagnosis was delayed in structured data (32 days) and claims (43 days) compared with technology-assisted EHR. CONCLUSION: Technology-enabled chart abstraction of unstructured EHR greatly improves data quality, minimizing false negatives when identifying patients with bone metastases that may lead to inaccurate conclusions that can affect delivery of care.",EHR | ICD-9 | US | breast cancer | electronic medical records | unstructured data
"Free text in electronic health records resists large-scale analysis. Text records facts of interest not found in encoded data, and text mining enables their retrieval and quantification. The U.S. Department of Veterans Affairs (VA) clinical data repository affords an opportunity to apply text-mining methodology to study clinical questions in large populations. To assess the feasibility of text mining, investigation of the relationship between exposure to adverse childhood experiences (ACEs) and recorded diagnoses was conducted among all VA-treated Gulf war veterans, utilizing all progress notes recorded from 2000-2011. Text processing extracted ACE exposures recorded among 44.7 million clinical notes belonging to 243,973 veterans. The relationship of ACE exposure to adult illnesses was analyzed using logistic regression. Bias considerations were assessed. ACE score was strongly associated with suicide attempts and serious mental disorders (ORs = 1.84 to 1.97), and less so with behaviorally mediated and somatic conditions (ORs = 1.02 to 1.36) per unit. Bias adjustments did not remove persistent associations between ACE score and most illnesses. Text mining to detect ACE exposure in a large population was feasible. Analysis of the relationship between ACE score and adult health conditions yielded patterns of association consistent with prior research.",
"BACKGROUND: Despite considerable financial incentives for adoption, there is little evidence available about providers' use and satisfaction with key functions of electronic health records (EHRs) that meet ""meaningful use"" criteria. METHODS: We surveyed primary care providers (PCPs) in 11 general internal medicine and family medicine practices affiliated with 3 health systems in Texas about their use and satisfaction with performing common tasks (documentation, medication prescribing, preventive services, problem list) in the Epic EHR, a common commercial system. Most practices had greater than 5 years of experience with the Epic EHR. We used multivariate logistic regression to model predictors of being a structured documenter, defined as using electronic templates or prepopulated dot phrases to document at least two of the three note sections (history, physical, assessment and plan). RESULTS: 146 PCPs responded (70%). The majority used free text to document the history (51%) and assessment and plan (54%) and electronic templates to document the physical exam (57%). Half of PCPs were structured documenters (55%) with family medicine specialty (adjusted OR 3.3, 95% CI, 1.4-7.8) and years since graduation (nonlinear relationship with youngest and oldest having lowest probabilities) being significant predictors. Nearly half (43%) reported spending at least one extra hour beyond each scheduled half-day clinic completing EHR documentation. Three-quarters were satisfied with documenting completion of pneumococcal vaccinations and half were satisfied with documenting cancer screening (57% for breast, 45% for colorectal, and 46% for cervical). Fewer were satisfied with reminders for overdue pneumococcal vaccination (48%) and cancer screening (38% for breast, 37% for colorectal, and 31% for cervical). While most believed the problem list was helpful (70%) and kept an up-to-date list for their patients (68%), half thought they were unreliable and inaccurate (51%). CONCLUSIONS: Dissatisfaction with and suboptimal use of key functions of the EHR may mitigate the potential for EHR use to improve preventive health and chronic disease management. Future work should optimize use of key functions and improve providers' time efficiency.",
"BACKGROUND: Family history information (FHI) described in unstructured electronic health records (EHRs) is a valuable information source for patient care and scientific researches. Since FHI is usually described in the format of free text, the entire process of FHI extraction consists of various steps including section segmentation, family member and clinical observation extraction, and relation discovery between the extracted members and their observations. The extraction step involves the recognition of FHI concepts along with their properties such as the family side attribute of the family member concept. METHODS: This study focuses on the extraction step and formulates it as a sequence labeling problem. We employed a neural sequence labeling model along with different tag schemes to distinguish family members and their observations. Corresponding to different tag schemes, the identified entities were aggregated and processed by different algorithms to determine the required properties. RESULTS: We studied the effectiveness of encoding required properties in the tag schemes by evaluating their performance on the dataset released by the BioCreative/OHNLP challenge 2018. It was observed that the proposed side scheme along with the developed features and neural network architecture can achieve an overall F1-score of 0.849 on the test set, which ranked second in the FHI entity recognition subtask. CONCLUSIONS: By comparing with the performance of conditional random fields models, the developed neural network-based models performed significantly better. However, our error analysis revealed two challenging issues of the current approach. One is that some properties required cross-sentence inferences. The other is that the current model is not able to distinguish between the narratives describing the family members of the patient and those specifying the relatives of the patient's family members.",Family history information extraction | Named entity recognition | Neural sequence labeling modeling
"We describe here the extraction of country-of-origin, an acculturation variable relevant for gene-environment studies, in a biorepository linked to de-identified electronic health records (EHRs) assessed by the Epidemiologic Architecture for Genes Linked to Environment (EAGLE), a study site of the Population Architecture using Genomics and Epidemiology (PAGE) I study. We extracted country-of-origin from the unstructured clinical free text using regular expressions within the MySQL relational database system in a cohort of 15,863 subjects of mostly non-European descent (including 11,519 African Americans, 1,702 Hispanics, and 1,118 Asians). We performed searches for 231 world countries (including independent sovereign states, dependent areas, and disputed territories) and common misspellings in >14 gigabytes of data including >13 billion characters of clinical text. Manual review of a fraction of the initial country-of-origin assignments established rules for data cleaning and quality control to achieve final country-of-origin status for each subject. After data cleaning, a total of 1,911/15,893 (12.02%) subjects were assigned to a country-of-origin outside of the United States. Mexico was the most commonly assigned country outside of the United States (264 subjects; 13.8% of subjects with a foreign country-of-origin assignment). The distribution of the countries assigned followed expectations based on known migration patterns to the United States with an emphasis on the southeastern region. These data suggest country-of-origin can be successfully extracted from unstructured clinical text for downstream genetic association studies.",
"Discordance between data stored in Electronic Health Records (EHR) may have a harmful effect on patient care. Automatic identification of such situations is an important yet challenging task, especially when the discordance involves information stored in free text fields. Here we present a method to automatically detect inconsistencies between data stored in free text and related coded fields. Using EHR data we train an ensemble of classifiers to predict the value of coded fields from the free text fields. Cases in which the classifiers predict with high confidence a code different from the clinicians' choice are marked as potential inconsistencies. Experimental results over discharge letters of sarcoma patients, verified by a domain expert, demonstrate the validity of our method.",
"Controlled clinical trials are usually supported with an in-front data aggregation system, which supports the storage of relevant information according to the trial context within a highly structured environment. In contrast to the documentation of clinical trials, daily routine documentation has many characteristics that influence data quality. One such characteristic is the use of non-standardized text, which is an indispensable part of information representation in clinical information systems. Based on a cohort study we highlight challenges for mining electronic health records targeting free text entry fields within semi-structured data sources. Our prototypical information extraction system achieved an F-measure of 0.91 (precision=0.90, recall=0.93) for the training set and an F-measure of 0.90 (precision=0.89, recall=0.92) for the test set. We analyze the obtained results in detail and highlight challenges and future directions for the secondary use of routine data in general.",Clinical narrative | Information extraction | Secondary use
"Background: Deep Phenotyping is the precise and comprehensive analysis of phenotypic features in which the individual components of the phenotype are observed and described. In UK mental health clinical practice, most clinically relevant information is recorded as free text in the Electronic Health Record, and offers a granularity of information beyond what is expressed in most medical knowledge bases. The SNOMED CT nomenclature potentially offers the means to model such information at scale, yet given a sufficiently large body of clinical text collected over many years, it is difficult to identify the language that clinicians favour to express concepts. Methods: By utilising a large corpus of healthcare data, we sought to make use of semantic modelling and clustering techniques to represent the relationship between the clinical vocabulary of internationally recognised SMI symptoms and the preferred language used by clinicians within a care setting. We explore how such models can be used for discovering novel vocabulary relevant to the task of phenotyping Serious Mental Illness (SMI) with only a small amount of prior knowledge.  Results: 20 403 terms were derived and curated via a two stage methodology. The list was reduced to 557 putative concepts based on eliminating redundant information content. These were then organised into 9 distinct categories pertaining to different aspects of psychiatric assessment. 235 concepts were found to be expressions of putative clinical significance. Of these, 53 were identified having novel synonymy with existing SNOMED CT concepts. 106 had no mapping to SNOMED CT. Conclusions: We demonstrate a scalable approach to discovering new concepts of SMI symptomatology based on real-world clinical observation. Such approaches may offer the opportunity to consider broader manifestations of SMI symptomatology than is typically assessed via current diagnostic frameworks, and create the potential for enhancing nomenclatures such as SNOMED CT based on real-world expressions.",electronic health records | natural language processing | schizophrenia | serious mental illness | word2vec
"OBJECTIVES: Much research with electronic health records (EHRs) uses coded or structured data only; important information captured in the free text remains unused. One dimension of EHR data quality assessment is 'currency' or timeliness, that is, data are representative of the patient state at the time of measurement. We explored the use of free text in UK general practice patient records to evaluate delays in recording of rheumatoid arthritis (RA) diagnosis. We also aimed to locate and quantify disease and diagnostic information recorded only in text. SETTING: UK general practice patient records from the Clinical Practice Research Datalink. PARTICIPANTS: 294 individuals with incident diagnosis of RA between 2005 and 2008; 204 women and 85 men, median age 63 years. PRIMARY AND SECONDARY OUTCOME MEASURES: Assessment of (1) quantity and timing of text entries for disease-modifying antirheumatic drugs (DMARDs) as a proxy for the RA disease code, and (2) quantity, location and timing of free text information relating to RA onset and diagnosis. RESULTS: Inflammatory markers, pain and DMARDs were the most common categories of disease information in text prior to RA diagnostic code; 10-37% of patients had such information only in text. Read codes associated with RA-related text included correspondence, general consultation and arthritis codes. 64 patients (22%) had DMARD text entries >14 days prior to RA code; these patients had more and earlier referrals to rheumatology, tests, swelling, pain and DMARD prescriptions, suggestive of an earlier implicit diagnosis than was recorded by the diagnostic code. CONCLUSIONS: RA-related symptoms, tests, referrals and prescriptions were recorded in free text with 22% of patients showing strong evidence of delay in coding of diagnosis. Researchers using EHRs may need to mitigate for delayed codes by incorporating text into their case-ascertainment strategies. Natural language processing techniques have the capability to do this at scale.",Rheumatoid arthritis | data quality | electronic health records | free text | general practice
"BACKGROUND: New nursing homes (NH) data warehouses fed from residents' medical records allow monitoring the health of elderly population on a daily basis. Elsewhere, syndromic surveillance has already shown that professional data can be used for public health (PH) surveillance but not during a long-term follow-up of the same cohort. OBJECTIVE: This study aimed to build and assess a national ecological NH PH surveillance system (SS). METHODS: Using a national network of 126 NH, we built a residents' cohort, extracted medical and personal data from their electronic health records, and transmitted them through the internet to a national server almost in real time. After recording sociodemographic, autonomic and syndromic information, a set of 26 syndromes was defined using pattern matching with the standard query language-LIKE operator and a Delphi-like technique, between November 2010 and June 2016. We used early aberration reporting system (EARS) and Bayes surveillance algorithms of the R surveillance package (Höhle) to assess our influenza and acute gastroenteritis (AGE) syndromic data against the Sentinelles network data, French epidemics gold standard, following Centers for Disease Control and Prevention surveillance system assessment guidelines. RESULTS: By extracting all sociodemographic residents' data, a cohort of 41,061 senior citizens was built. EARS_C3 algorithm on NH influenza and AGE syndromic data gave sensitivities of 0.482 and 0.539 and specificities of 0.844 and 0.952, respectively, over a 6-year period, forecasting the last influenza outbreak by catching early flu signals. In addition, assessment of influenza and AGE syndromic data quality showed precisions of 0.98 and 0.96 during last season epidemic weeks' peaks (weeks 03-2017 and 01-2017) and precisions of 0.95 and 0.92 during last summer epidemic weeks' low (week 33-2016). CONCLUSIONS: This study confirmed that using syndromic information gives a good opportunity to develop a genuine French national PH SS dedicated to senior citizens. Access to senior citizens' free-text validated health data on influenza and AGE responds to a PH issue for the surveillance of this fragile population. This database will also make possible new ecological research on other subjects that will improve prevention, care, and rapid response when facing health threats.",Centers for Disease Control and Prevention | Delphi technique | nursing homes | pattern recognition | sentinel surveillance | syndromic surveillance
"The rapid adoption of electronic health records (EHRs) systems has made clinical data available in electronic format for research and for many downstream applications. Electronic screening of potentially eligible patients using these clinical databases for clinical trials is a critical need to improve trial recruitment efficiency. Nevertheless, manually translating free-text eligibility criteria into database queries is labor intensive and inefficient. To facilitate automated screening, free-text eligibility criteria must be structured and coded into a computable format using controlled vocabularies. Named entity recognition (NER) is thus an important first step. In this study, we evaluate 4 state-of-the-art transformer-based NER models on two publicly available annotated corpora of eligibility criteria released by Columbia University (i.e., the Chia data) and Facebook Research (i.e.the FRD data). Four transformer-based models (i.e., BERT, ALBERT, RoBERTa, and ELECTRA) pretrained with general English domain corpora vs. those pretrained with PubMed citations, clinical notes from the MIMIC-III dataset and eligibility criteria extracted from all the clinical trials on ClinicalTrials.gov were compared. Experimental results show that RoBERTa pretrained with MIMIC-III clinical notes and eligibility criteria yielded the highest strict and relaxed F-scores in both the Chia data (i.e., 0.658/0.798) and the FRD data (i.e., 0.785/0.916). With promising NER results, further investigations on building a reliable natural language processing (NLP)-assisted pipeline for automated electronic screening are needed.",Clinical Trial | Computing methodologies → Information extraction | Eligibility Criteria Parsing | Named Entity Recognition | Transformer-Based Model
"OBJECTIVES: Health systems are adopting electronic health records (EHRs). There are few studies on the effects of EHR implementation on graduate medical education. The authors sought to longitudinally assess perceptions of the impact of EHRs on graduate medical education during implementation and 2 years after implementation. METHODS: A survey was distributed to faculty and trainees during the first year (2013) of adoption of the EHR system. A follow-up survey was distributed 2 years later (2015). The χ(2) test was used to compare the quantitative responses, and factor analysis was conducted to identify correlations between items. Free text responses were analyzed qualitatively. RESULTS: The initial survey (in 2013) included 290 faculty and 106 trainees; the follow-up survey (in 2015) included 353 faculty and 226 trainees. In 2013, respondents had a positive impression of EHRs. During the implementation phase, participants believed that face-to-face teaching was negatively affected (P = 0.001). Faculty believed EHRs had a negative effect on trainees' ability to take a history/conduct physical examinations (P = 0.002) and to formulate a differential diagnosis/plan independently (P = 0.003). In 2015, faculty opinions of the impact of the EHR remained unchanged; trainee responses were more positive than in 2013 in some areas. Qualitative analysis showed that the most frequent strategies to enhance the educational process were the development of EHR skills and improved chart access and note assistance. CONCLUSIONS: Respondents remain positive about the EHR 2 years after implementation. Faculty remain concerned about its effect on the educational process, whereas residents appear more positive regarding the potential for EHRs to enhance their education.",
"Electronic Health Records (EHRs) have become the primary form of medical data-keeping across the United States. Federal law restricts the sharing of any EHR data that contains protected health information (PHI). De-identification, the process of identifying and removing all PHI, is crucial for making EHR data publicly available for scientific research. This project explores several deep learning-based named entity recognition (NER) methods to determine which method(s) perform better on the de-identification task. We trained and tested our models on the i2b2 training dataset, and qualitatively assessed their performance using EHR data collected from a local hospital. We found that 1) Bi-LSTM-CRF represents the best-performing encoder/decoder combination, 2) character-embeddings tend to improve precision at the price of recall, and 3) transformers alone under-perform as context encoders. Future work focused on structuring medical text may improve the extraction of semantic and syntactic information for the purposes of EHR deidentification.",
"In Electronic Health Records (EHRs), much of valuable information regarding patients' conditions is embedded in free text format. Natural language processing (NLP) techniques have been developed to extract clinical information from free text. One challenge faced in clinical NLP is that the meaning of clinical entities is heavily affected by modifiers such as negation. A negation detection algorithm, NegEx, applies a simplistic approach that has been shown to be powerful in clinical NLP. However, due to the failure to consider the contextual relationship between words within a sentence, NegEx fails to correctly capture the negation status of concepts in complex sentences. Incorrect negation assignment could cause inaccurate diagnosis of patients' condition or contaminated study cohorts. We developed a negation algorithm called DEEPEN to decrease NegEx's false positives by taking into account the dependency relationship between negation words and concepts within a sentence using Stanford dependency parser. The system was developed and tested using EHR data from Indiana University (IU) and it was further evaluated on Mayo Clinic dataset to assess its generalizability. The evaluation results demonstrate DEEPEN, which incorporates dependency parsing into NegEx, can reduce the number of incorrect negation assignment for patients with positive findings, and therefore improve the identification of patients with the target clinical findings in EHRs.",Dependency parser | Natural language processing | Negation
"PURPOSE: While search engines have become nearly ubiquitous on the Web, electronic health records (EHRs) generally lack search functionality; furthermore, there is no knowledge on how and what healthcare providers search while using an EHR-based search utility. In this study, we sought to understand user needs as captured by their search queries. METHODS: This post-implementation study analyzed user search log files for 6 months from an EHR-based, free-text search utility at our large academic institution. The search logs were de-identified and then analyzed in two steps. First, two investigators classified all the unique queries as navigational, transactional, or informational searches. Second, three physician reviewers categorized a random sample of 357 informational searches into high-level semantic types derived from the Unified Medical Language System (UMLS). The reviewers were given overlapping data sets, such that two physicians reviewed each query. RESULTS: We analyzed 2207 queries performed by 436 unique users over a 6-month period. Of the 2207 queries, 980 were unique queries. Users of the search utility included clinicians, researchers and administrative staff. Across the whole user population, approximately 14.5% of the user searches were navigational searches and 85.1% were informational. Within informational searches, we found that users predominantly searched for laboratory results and specific diseases. CONCLUSIONS: A variety of user types, ranging from clinicians to administrative staff, took advantage of the EHR-based search utility. Though these users' search behavior differed, they predominantly performed informational searches related to laboratory results and specific diseases. Additionally, a number of queries were part of words, implying the need for a free-text module to be included in any future concept-based search algorithm.",
"BACKGROUND: With the rapid adoption of electronic medical records (EMRs), there is an ever-increasing opportunity to collect data and extract knowledge from EMRs to support patient-centered stroke management. OBJECTIVE: This study aims to compare the effectiveness of state-of-the-art automatic text classification methods in classifying data to support the prediction of clinical patient outcomes and the extraction of patient characteristics from EMRs. METHODS: Our study addressed the computational problems of information extraction and automatic text classification. We identified essential tasks to be considered in an ischemic stroke value-based program. The 30 selected tasks were classified (manually labeled by specialists) according to the following value agenda: tier 1 (achieved health care status), tier 2 (recovery process), care related (clinical management and risk scores), and baseline characteristics. The analyzed data set was retrospectively extracted from the EMRs of patients with stroke from a private Brazilian hospital between 2018 and 2019. A total of 44,206 sentences from free-text medical records in Portuguese were used to train and develop 10 supervised computational machine learning methods, including state-of-the-art neural and nonneural methods, along with ontological rules. As an experimental protocol, we used a 5-fold cross-validation procedure repeated 6 times, along with subject-wise sampling. A heatmap was used to display comparative result analyses according to the best algorithmic effectiveness (F1 score), supported by statistical significance tests. A feature importance analysis was conducted to provide insights into the results. RESULTS: The top-performing models were support vector machines trained with lexical and semantic textual features, showing the importance of dealing with noise in EMR textual representations. The support vector machine models produced statistically superior results in 71% (17/24) of tasks, with an F1 score >80% regarding care-related tasks (patient treatment location, fall risk, thrombolytic therapy, and pressure ulcer risk), the process of recovery (ability to feed orally or ambulate and communicate), health care status achieved (mortality), and baseline characteristics (diabetes, obesity, dyslipidemia, and smoking status). Neural methods were largely outperformed by more traditional nonneural methods, given the characteristics of the data set. Ontological rules were also effective in tasks such as baseline characteristics (alcoholism, atrial fibrillation, and coronary artery disease) and the Rankin scale. The complementarity in effectiveness among models suggests that a combination of models could enhance the results and cover more tasks in the future. CONCLUSIONS: Advances in information technology capacity are essential for scalability and agility in measuring health status outcomes. This study allowed us to measure effectiveness and identify opportunities for automating the classification of outcomes of specific tasks related to clinical conditions of stroke victims, and thus ultimately assess the possibility of proactively using these machine learning techniques in real-world situations.",EHR | data mining | electronic health records | electronic medical records | natural language processing | outcomes | patient outcomes | stroke | text classification | text processing
"Electronic Health Records (EHRs) represent a source of high value data which is often underutilized because exploiting the information contained therein requires specialized techniques unavailable to the end user i.e. the physician or the investigator. Here I describe four simple and practical avenues that will allow the standard EHR end user to identify patient cohorts: the use of diagnostic codes from different international catalogues; a search in reports from complementary tests (e.g. radiographs or lab tests) for any result of interest; a free text search; or a drug prescription search in the patient's electronic prescription record. This medical approach is acquiring great importance in the field of rare diseases, and here I demonstrate its application with X-linked hypophosphatemia. The use of these four EHR questioning approaches makes finding a cohort of patients of any condition or disease feasible and manageable, and once each case record is checked, a well-defined cohort can be assembled.",Cohort identification | XLH | electronic health records | methodological approach | patient search
"BACKGROUND: Drug prescriptions are often recorded in free-text clinical narratives; making this information available in a structured form is important to support many health-related tasks. Although several natural language processing (NLP) methods have been proposed to extract such information, many challenges remain. OBJECTIVE: This study evaluates the feasibility of using NLP and deep learning approaches for extracting and linking drug names and associated attributes identified in clinical free-text notes and presents an extensive error analysis of different methods. This study initiated with the participation in the 2018 National NLP Clinical Challenges (n2c2) shared task on adverse drug events and medication extraction. METHODS: The proposed system (DrugEx) consists of a named entity recognizer (NER) to identify drugs and associated attributes and a relation extraction (RE) method to identify the relations between them. For NER, we explored deep learning-based approaches (ie, bidirectional long-short term memory with conditional random fields [BiLSTM-CRFs]) with various embeddings (ie, word embedding, character embedding [CE], and semantic-feature embedding) to investigate how different embeddings influence the performance. A rule-based method was implemented for RE and compared with a context-aware long-short term memory (LSTM) model. The methods were trained and evaluated using the 2018 n2c2 shared task data. RESULTS: The experiments showed that the best model (BiLSTM-CRFs with pretrained word embeddings [PWE] and CE) achieved lenient micro F-scores of 0.921 for NER, 0.927 for RE, and 0.855 for the end-to-end system. NER, which relies on the pretrained word and semantic embeddings, performed better on most individual entity types, but NER with PWE and CE had the highest classification efficiency among the proposed approaches. Extracting relations using the rule-based method achieved higher accuracy than the context-aware LSTM for most relations. Interestingly, the LSTM model performed notably better in the reason-drug relations, the most challenging relation type. CONCLUSIONS: The proposed end-to-end system achieved encouraging results and demonstrated the feasibility of using deep learning methods to extract medication information from free-text data.",discharge summaries | electronic health records | information extraction | medication prescriptions | natural language processing
"OBJECTIVE: Routine primary care data may be used for the derivation of clinical prediction rules and risk scores. We sought to measure the impact of a decision support system (DSS) on data completeness and freedom from bias. MATERIALS AND METHODS: We used the clinical documentation of 34 UK general practitioners who took part in a previous study evaluating the DSS. They consulted with 12 standardized patients. In addition to suggesting diagnoses, the DSS facilitates data coding. We compared the documentation from consultations with the electronic health record (EHR) (baseline consultations) vs consultations with the EHR-integrated DSS (supported consultations). We measured the proportion of EHR data items related to the physician's final diagnosis. We expected that in baseline consultations, physicians would document only or predominantly observations related to their diagnosis, while in supported consultations, they would also document other observations as a result of exploring more diagnoses and/or ease of coding. RESULTS: Supported documentation contained significantly more codes (incidence rate ratio [IRR] = 5.76 [4.31, 7.70] P < .001) and less free text (IRR = 0.32 [0.27, 0.40] P < .001) than baseline documentation. As expected, the proportion of diagnosis-related data was significantly lower (b = -0.08 [-0.11, -0.05] P < .001) in the supported consultations, and this was the case for both codes and free text. CONCLUSIONS: We provide evidence that data entry in the EHR is incomplete and reflects physicians' cognitive biases. This has serious implications for epidemiological research that uses routine data. A DSS that facilitates and motivates data entry during the consultation can improve routine documentation.",bias | clinical | decision support systems | documentation | electronic health records | primary health care
"BACKGROUND: Phenotypic information locked away in unstructured narrative text presents significant barriers to information accessibility, both for clinical practitioners and for computerised applications used for clinical research purposes. Text mining (TM) techniques have previously been applied successfully to extract different types of information from text in the biomedical domain. They have the potential to be extended to allow the extraction of information relating to phenotypes from free text. METHODS: To stimulate the development of TM systems that are able to extract phenotypic information from text, we have created a new corpus (PhenoCHF) that is annotated by domain experts with several types of phenotypic information relating to congestive heart failure. To ensure that systems developed using the corpus are robust to multiple text types, it integrates text from heterogeneous sources, i.e., electronic health records (EHRs) and scientific articles from the literature. We have developed several different phenotype extraction methods to demonstrate the utility of the corpus, and tested these methods on a further corpus, i.e., ShARe/CLEF 2013. RESULTS: Evaluation of our automated methods showed that PhenoCHF can facilitate the training of reliable phenotype extraction systems, which are robust to variations in text type. These results have been reinforced by evaluating our trained systems on the ShARe/CLEF corpus, which contains clinical records of various types. Like other studies within the biomedical domain, we found that solutions based on conditional random fields produced the best results, when coupled with a rich feature set. CONCLUSIONS: PhenoCHF is the first annotated corpus aimed at encoding detailed phenotypic information. The unique heterogeneous composition of the corpus has been shown to be advantageous in the training of systems that can accurately extract phenotypic information from a range of different text types. Although the scope of our annotation is currently limited to a single disease, the promising results achieved can stimulate further work into the extraction of phenotypic information for other diseases. The PhenoCHF annotation guidelines and annotations are publicly available at https://code.google.com/p/phenochf-corpus.",
"OBJECTIVE: To evaluate the completeness of diagnosis recording in problem lists in a hospital electronic health record (EHR) system during the COVID-19 pandemic. DESIGN: Retrospective chart review with manual review of free text electronic case notes. SETTING: Major teaching hospital trust in London, one year after the launch of a comprehensive EHR system (Epic), during the first peak of the COVID-19 pandemic in the UK. PARTICIPANTS: 516 patients with suspected or confirmed COVID-19. MAIN OUTCOME MEASURES: Percentage of diagnoses already included in the structured problem list. RESULTS: Prior to review, these patients had a combined total of 2841 diagnoses recorded in their EHR problem lists. 1722 additional diagnoses were identified, increasing the mean number of recorded problems per patient from 5.51 to 8.84. The overall percentage of diagnoses originally included in the problem list was 62.3% (2841 / 4563, 95% confidence interval 60.8%, 63.7%). CONCLUSIONS: Diagnoses and other clinical information stored in a structured way in electronic health records is extremely useful for supporting clinical decisions, improving patient care and enabling better research. However, recording of medical diagnoses on the structured problem list for inpatients is incomplete, with almost 40% of important diagnoses mentioned only in the free text notes.",COVID-19 | Data completeness | EHR systems | Electronic health record | Health information technology | Problem lists
"AIMS AND OBJECTIVES: To explore the impact of implementing an electronic health record system on staff at a Scottish hospice. BACKGROUND: Electronic health records are broadly considered preferable to paper-based systems. However, changing from one system to the other is difficult. This study analysed the impact of this change in a Scottish hospice. DESIGN: Naturalistic prospective repeated-measures mixed-methods approach. METHODS: Data on the usability of the system, staff engagement and staff experience were obtained at four time points spanning 30 months from inception. Quantitative data were obtained from surveys, and qualitative from concurrent analysis of free-text comments and focus group. Participants were all 150 employees of a single hospice in Scotland. RESULTS: Both system usability and staff engagement scores decreased for the first two years before recovering at 30 months. Staff experience data pointed to two main challenges: (1) Technical issues, with subthemes of accessibility and usability. (2) Cultural issues, with subthemes of time, teamwork, care provision and perception of change. CONCLUSIONS: It took 30 months for system usability and staff engagement scores to rise, after falling significantly for the first two years. The unintended outcomes of implementation included challenges to the way the patient story was both recorded and communicated. Nevertheless, this process of change was found to be consistent with the 'J-curve' theory of organisational change, and as such, it is both predictable and manageable for other organisations. RELEVANCE TO CLINICAL PRACTICE: It is known that implementing an electronic health record system is complex. This paper puts parameters on this complexity by defining both the nature of the complexity ('J' curve) and the time taken for the organisation to begin recovery from the challenges (two years). Understanding these parameters will help health organisations across the world plan more strategically.",caring | computerised | health services research | implementation | nursing information systems | nursing workforce | organisational behaviour | palliative care | technology
"OBJECTIVE: We aimed to evaluate the validity of an algorithm to classify diagnoses according to the appropriateness of outpatient antibiotic use in the context of Chinese free text. SETTING AND PARTICIPANTS: A random sample of 10 000 outpatient visits was selected between January and April 2018 from a national database for monitoring rational use of drugs, which included data from 194 secondary and tertiary hospitals in China. RESEARCH DESIGN: Diagnoses for outpatient visits were classified as tier 1 if associated with at least one condition that 'always' justified antibiotic use; as tier 2 if associated with at least one condition that only 'sometimes' justified antibiotic use but no conditions that 'always' justified antibiotic use; or as tier 3 if associated with only conditions that never justified antibiotic use, using a tier-fashion method and regular expression (RE)-based algorithm. MEASURES: Sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV) of the classification algorithm, using classification made by chart review as the standard reference, were calculated. RESULTS: The sensitivities of the algorithm for classifying tier 1, tier 2 and tier 3 diagnoses were 98.2% (95% CI 96.4% to 99.3%), 98.4% (95% CI 97.6% to 99.1%) and 100.0% (95% CI 100.0% to 100.0%), respectively. The specificities were 100.0% (95% CI 100.0% to 100.0%), 100.0% (95% CI 99.9% to 100.0%) and 98.6% (95% CI 97.9% to 99.1%), respectively. The PPVs for classifying tier 1, tier 2 and tier 3 diagnoses were 100.0% (95% CI 99.1% to 100.0%), 99.7% (95% CI 99.2% to 99.9%) and 99.7% (95% CI 99.6% to 99.8%), respectively. The NPVs were 99.9% (95% CI 99.8% to 100.0%), 99.8% (95% CI 99.7% to 99.9%) and 100.0% (95% CI 99.8% to 100.0%), respectively. CONCLUSIONS: The RE-based classification algorithm in the context of Chinese free text had sufficiently high validity for further evaluating the appropriateness of outpatient antibiotic prescribing.",antibiotics | drug utilisation | electronic health records | prescriptions | validation
"Adoption of electronic health records (EHRs) has the potential to assist with clinical reasoning and streamline workflow; however, the data entry and review capabilities of most systems are suboptimal which may lead to workarounds. As an instance of a workaround, we examined nurses' use of optional free-text comments in EHR flowsheets to support clinical needs for data interpretation. This mixed-method study included: 1) Content analysis of comments, 2) Interviews with nurses. We performed a sub-analysis of flowsheet data for 201 patients that experienced a cardiac arrest and interviewed 5 acute care nurses. We found that nurses used workarounds in the EHR - despite the extra effort that they required - to convey clinically significant relationships and to communicate concerning events to physicians. EHRs should better support entry of clinical data that ""belongs together"" and enable messaging capabilities integrated with nurses' flowsheet documentation workflow.",
"PURPOSE: Cancer staging is critical for prognostication, treatment planning, and determining clinical trial eligibility. Electronic health records (EHRs) have structured staging modules, but physician use is inconsistent. Typically, stage is entered as unstructured free text in clinical notes and cannot easily be used for reporting. METHODS: We created an Epic Best Practice Advisory (BPA) decision support tool that requires physicians to enter cancer stage in a structured module. If certain conditions are met, the BPA is triggered as a hard stop, and the physician cannot chart until staging is complete or a reason for not staging is selected. We used Plan, Do, Study, Act methodology to inform the intervention and compared preexisting staging rates to rates at 4, 8, and 12 months postintervention. RESULTS: For 12 months before BPA implementation, 1,480 of 5,222 (28%) patients had cancer stage structured within the Epic problem list. From 1 to 4 months after the BPA 2,057 of 1,788 (115%) cases were staged in Epic. In the 5- to 8-month period after the BPA, 1,057 of 1,893 (56%) cases were staged, and 9 to 12 months after the BPA 1,082 of 1,817 (60%) were staged. CONCLUSION: Electronic decision support improves the rate of structured cancer staging at our institution. The staging rates between 56% and 60% for the 5- to 8-month and 9- to 12-month periods likely reflect accurate postintervention staging rates, whereas the initial 115% rate for 1 to 4 months is inflated by providers staging cancers diagnosed before the BPA.",
"The COVID-19 pandemic swept across the world rapidly, infecting millions of people. An efficient tool that can accurately recognize important clinical concepts of COVID-19 from free text in electronic health records (EHRs) will be valuable to accelerate COVID-19 clinical research. To this end, this study aims at adapting the existing CLAMP natural language processing tool to quickly build COVID-19 SignSym, which can extract COVID-19 signs/symptoms and their 8 attributes (body location, severity, temporal expression, subject, condition, uncertainty, negation, and course) from clinical text. The extracted information is also mapped to standard concepts in the Observational Medical Outcomes Partnership common data model. A hybrid approach of combining deep learning-based models, curated lexicons, and pattern-based rules was applied to quickly build the COVID-19 SignSym from CLAMP, with optimized performance. Our extensive evaluation using 3 external sites with clinical notes of COVID-19 patients, as well as the online medical dialogues of COVID-19, shows COVID-19 SignSym can achieve high performance across data sources. The workflow used for this study can be generalized to other use cases, where existing clinical natural language processing tools need to be customized for specific information needs within a short time. COVID-19 SignSym is freely accessible to the research community as a downloadable package (https://clamp.uth.edu/covid/nlp.php) and has been used by 16 healthcare organizations to support clinical research of COVID-19.",
"AIMS: Certain treatments for type 2 diabetes mellitus cause hypoglycaemia and weight gain, and thus might counteract the benefits of intensive glucose control. We quantify the association of cardiovascular disease (CVD) outcomes with hypoglycaemia and weight gain among patients with type 2 diabetes treated with sulfonylureas. MATERIALS AND METHODS: This cohort study included patients from January 2009 through December 2014 who were selected from within a deidentified nationwide electronic health records repository, including multiple provider networks and electronic medical records systems. Hypoglycaemia measures from structured data fields and free text clinical notes were categorized as serious or non-serious. Covariate adjusted Poisson regression analysis was used to assess the association between frequency of hypoglycaemia (by severity), or magnitude of weight change, and incidence of acute myocardial infarction (AMI), congestive heart failure (CHF) and stroke. RESULTS: Among 143 635 eligible patients, we observed 5669 cases of AMI, 14 109 incident cases of CHF and 7017 cases of stroke. Overall incidence rates were 1.53, 4.26 and 1.92 per 100 person-years for AMI, CHF and stroke, respectively. The associations between overall hypoglycaemia and each of the CVD outcomes were positive, with stronger associations observed for serious hypoglycaemia and attenuated or null associations observed for non-serious hypoglycaemia. Weight change exhibited a U-shaped association with increased risks associated with both weight loss and weight gain relative to stable weight. CONCLUSIONS: This study provides evidence of increased CVD risk associated with hypoglycaemia, especially with serious hypoglycaemia events. While associations were attenuated with non-serious hypoglycaemia, the results were suggestive of a potential increased risk.",body composition | cardiovascular disease | hypoglycaemia | observational study | pharmaco-epidemiology | sulphonylureas
"BACKGROUND: Secondary use of data collected in Electronic Health Records opens perspectives for increasing our knowledge of rare diseases. The clinical data warehouse (named Dr. Warehouse) at the Necker-Enfants Malades Children's Hospital contains data collected during normal care for thousands of patients. Dr. Warehouse is oriented toward the exploration of clinical narratives. In this study, we present our method to find phenotypes associated with diseases of interest. METHODS: We leveraged the frequency and TF-IDF to explore the association between clinical phenotypes and rare diseases. We applied our method in six use cases: phenotypes associated with the Rett, Lowe, Silver Russell, Bardet-Biedl syndromes, DOCK8 deficiency and Activated PI3-kinase Delta Syndrome (APDS). We asked domain experts to evaluate the relevance of the top-50 (for frequency and TF-IDF) phenotypes identified by Dr. Warehouse and computed the average precision and mean average precision. RESULTS: Experts concluded that between 16 and 39 phenotypes could be considered as relevant in the top-50 phenotypes ranked by descending frequency discovered by Dr. Warehouse (resp. between 11 and 41 for TF-IDF). Average precision ranges from 0.55 to 0.91 for frequency and 0.52 to 0.95 for TF-IDF. Mean average precision was 0.79. Our study suggests that phenotypes identified in clinical narratives stored in Electronic Health Record can provide rare disease specialists with candidate phenotypes that can be used in addition to the literature. CONCLUSIONS: Clinical Data Warehouses can be used to perform Next Generation Phenotyping, especially in the context of rare diseases. We have developed a method to detect phenotypes associated with a group of patients using medical concepts extracted from free-text clinical narratives.",Data mining | Data warehouse | Natural language processing | Next generation phenotyping | Rare diseases
"BACKGROUND: Electronic health record (EHR) users must regularly review large amounts of data in order to make informed clinical decisions, and such review is time-consuming and often overwhelming. Technologies like automated summarization tools, EHR search engines and natural language processing have been shown to help clinicians manage this information. OBJECTIVE: To develop a support vector machine (SVM)-based system for identifying EHR progress notes pertaining to diabetes, and to validate it at two institutions. MATERIALS AND METHODS: We retrieved 2000 EHR progress notes from patients with diabetes at the Brigham and Women's Hospital (1000 for training and 1000 for testing) and another 1000 notes from the University of Texas Physicians (for validation). We manually annotated all notes and trained a SVM using a bag of words approach. We then used the SVM on the testing and validation sets and evaluated its performance with the area under the curve (AUC) and F statistics. RESULTS: The model accurately identified diabetes-related notes in both the Brigham and Women's Hospital testing set (AUC=0.956, F=0.934) and the external University of Texas Faculty Physicians validation set (AUC=0.947, F=0.935). DISCUSSION: Overall, the model we developed was quite accurate. Furthermore, it generalized, without loss of accuracy, to another institution with a different EHR and a distinct patient and provider population. CONCLUSIONS: It is possible to use a SVM-based classifier to identify EHR progress notes pertaining to diabetes, and the model generalizes well.",electronic health record | natural language processing | search | support vector machine
"PURPOSE: To test the association between repeated clinical smoking cessation support and long-term cessation. DESIGN: Retrospective, observational cohort study using structured and free-text data from electronic health records. SETTING: Six diverse health systems in the United States. PARTICIPANTS: Patients aged ≥18 years who were smokers in 2007 and had ≥1 primary care visit in each of the following 4 years (N = 33 691). MEASURES: Primary exposure was a composite categorical variable (comprised of documentation of smoking cessation medication, counseling, or referral) classifying the proportions of visits for which patients received any cessation assistance (<25% (reference), 25%-49%, 50%-74%, and ≥75% of visits). The dependent variable was long-term quit (LTQ; yes/no), defined as no indication of being a current smoker for ≥365 days following a visit where nonsmoker or former smoker was indicated. ANALYSIS: Mixed effects logistic regression analysis adjusted for age, sex, race, and comorbidities, with robust standard error estimation to account for within site correlation. RESULTS: Overall, 20% of the cohort achieved LTQ status. Patients with ≥75% of visits with any assistance had almost 3 times the odds of achieving LTQ status compared to those with <25% visits with assistance (odds ratio = 2.84; 95% confidence interval: 1.50-5.37). Results were similar for specific assistance types. CONCLUSIONS: These findings provide support for the importance of repeated assistance at primary care visits to increase long-term smoking cessation.",electronic health records | long-term quit | primary care | smoking cessation
"PURPOSE: Case-finding for common mental disorders (CMD) in routine data unobtrusively identifies patients for mental health research. There is absence of a review of studies examining CMD-case-finding accuracy in routine primary care data. CMD-case definitions include diagnostic/prescription codes, signs/symptoms, and free text within electronic health records. This systematic review assesses evidence for case-finding accuracy of CMD-case definitions compared to reference standards. METHODS: PRISMA-DTA checklist guided review. Eligibility criteria were outlined prior to study search; studies compared CMD-case definitions in routine primary care data to diagnostic interviews, screening instruments, or clinician judgement. Studies were quality assessed using QUADAS-2. RESULTS: Fourteen studies were included, and most were at high risk of bias. Nine studies examined depressive disorders and seven utilised diagnostic interviews as reference standards. Receiver operating characteristic (ROC) planes illustrated overall variable case-finding accuracy across case definitions, quantified by Youden's index. Forest plots demonstrated most case definitions provide high specificity. CONCLUSION: Case definitions effectively identify cases in a population with good accuracy and few false positives. For 100 anxiety cases, identified using diagnostic codes, between 12 and 20 will be false positives; 0-47 cases will be missed. Sensitivity is more variable and specificity is higher in depressive cases; for 100 cases identified using diagnostic codes, between 0 and 87 will be false positives; 4-18 cases will be missed. Incorporating context to case definitions may improve overall case-finding accuracy. Further research is required for meta-analysis and robust conclusions.",Adults | Anxiety | Depression | Electronic health records | Systematic review
"There has been increasing recognition of the key role of social determinants like occupation on health. Given the relatively poor understanding of occupation information in electronic health records (EHRs), we sought to characterize occupation information within free-text clinical document sources. From six distinct clinical sources, 868 total occupation-related sentences were identified for the study corpus. Building off approaches from previous studies, refined annotation guidelines were created using the National Institute for Occupational Safety and Health Occupational Data for Health data model with elements added to increase granularity. Our corpus generated 2,005 total annotations representing 39 of 41 entity types from the enhanced data model. Highest frequency entities were: Occupation Description (17.7%); Employment Status - Not Specified (12.5%); Employer Name (11.0%); Subject (9.8%); Industry Description (6.2%). Our findings support the value of standardizing entry of EHR occupation information to improve data quality for improved patient care and secondary uses of this information.",Electronic Health Records | Occupations | Social Determinants of Health
"BACKGROUND: Trials often struggle to achieve their target sample size with only half doing so. Some researchers have turned to Electronic Health Records (EHRs), seeking a more efficient way of recruitment. The Scottish Health Research Register (SHARE) obtained patients' consent for their EHRs to be used as a searching base from which researchers can find potential participants. However, due to the fact that EHR data is not complete, sufficient or accurate, a database search strategy may not generate the best case-finding result. The current study aims to evaluate the performance of a case-based reasoning method in identifying participants for population-based clinical studies recruiting through SHARE, and assess the difference between its resultant cohort and the original one deriving from searching EHRs. METHODS: A case-based reasoning framework was applied to 119 participants in nine projects using two-fold cross-validation, with records from a further 86,292 individuals used for testing. A prediction score for study participation was derived from the diagnosis, procedure, pharmaceutical prescription, and laboratory test results attributes of each participant. Evaluation was conducted by calculating Area Under the ROC Curve and information retrieval metrics for the ranking list of the test set by prediction score. We compared the most likely participants as identified by searching a database to those ranked highest by our model. RESULTS: The average ROCAUC for nine projects was 81% indicating strong predictive ability for these data. However, the derived ranking lists showed lower predictive performance, with only 21% of the persons ranked within top 50 positions being the same as identified by searching databases. CONCLUSIONS: Case-based reasoning is may be more effective than a database search strategy for participant identification for clinical studies using population EHRs. The lower performance of ranking lists derived from case-based reasoning means that patients identified as highly suitable for study participation may still not be recruited. This suggests that further study is needed into improvements in the collection and curation of population EHRs, such as use of free text data to aid reliable identification of people more likely to be recruited to clinical trials.",Artificial intelligence | Clinical studies | Electronic health record | Machine learning
"BACKGROUND: There are significant variabilities in guideline-concordant documentation in asthma care. However, assessing clinician's documentation is not feasible using only structured data but requires labor-intensive chart review of electronic health records (EHRs). A certain guideline element in asthma control factors, such as review inhaler techniques, requires context understanding to correctly capture from EHR free text. METHODS: The study data consist of two sets: (1) manual chart reviewed data-1039 clinical notes of 300 patients with asthma diagnosis, and (2) weakly labeled data (distant supervision)-27,363 clinical notes from 800 patients with asthma diagnosis. A context-aware language model, Bidirectional Encoder Representations from Transformers (BERT) was developed to identify inhaler techniques in EHR free text. Both original BERT and clinical BioBERT (cBERT) were applied with a cost-sensitivity to deal with imbalanced data. The distant supervision using weak labels by rules was also incorporated to augment the training set and alleviate a costly manual labeling process in the development of a deep learning algorithm. A hybrid approach using post-hoc rules was also explored to fix BERT model errors. The performance of BERT with/without distant supervision, hybrid, and rule-based models were compared in precision, recall, F-score, and accuracy. RESULTS: The BERT models on the original data performed similar to a rule-based model in F1-score (0.837, 0.845, and 0.838 for rules, BERT, and cBERT, respectively). The BERT models with distant supervision produced higher performance (0.853 and 0.880 for BERT and cBERT, respectively) than without distant supervision and a rule-based model. The hybrid models performed best in F1-score of 0.877 and 0.904 over the distant supervision on BERT and cBERT. CONCLUSIONS: The proposed BERT models with distant supervision demonstrated its capability to identify inhaler techniques in EHR free text, and outperformed both the rule-based model and BERT models trained on the original data. With a distant supervision approach, we may alleviate costly manual chart review to generate the large training data required in most deep learning-based models. A hybrid model was able to fix BERT model errors and further improve the performance.",Adherence to asthma guidelines | Context-aware language model | Deep learning | Documentation variations | Inhaler technique | Natural language processing
"BACKGROUND: Automated disease code classification using free-text medical information is important for public health surveillance. However, traditional natural language processing (NLP) pipelines are limited, so we propose a method combining word embedding with a convolutional neural network (CNN). OBJECTIVE: Our objective was to compare the performance of traditional pipelines (NLP plus supervised machine learning models) with that of word embedding combined with a CNN in conducting a classification task identifying International Classification of Diseases, Tenth Revision, Clinical Modification (ICD-10-CM) diagnosis codes in discharge notes. METHODS: We used 2 classification methods: (1) extracting from discharge notes some features (terms, n-gram phrases, and SNOMED CT categories) that we used to train a set of supervised machine learning models (support vector machine, random forests, and gradient boosting machine), and (2) building a feature matrix, by a pretrained word embedding model, that we used to train a CNN. We used these methods to identify the chapter-level ICD-10-CM diagnosis codes in a set of discharge notes. We conducted the evaluation using 103,390 discharge notes covering patients hospitalized from June 1, 2015 to January 31, 2017 in the Tri-Service General Hospital in Taipei, Taiwan. We used the receiver operating characteristic curve as an evaluation measure, and calculated the area under the curve (AUC) and F-measure as the global measure of effectiveness. RESULTS: In 5-fold cross-validation tests, our method had a higher testing accuracy (mean AUC 0.9696; mean F-measure 0.9086) than traditional NLP-based approaches (mean AUC range 0.8183-0.9571; mean F-measure range 0.5050-0.8739). A real-world simulation that split the training sample and the testing sample by date verified this result (mean AUC 0.9645; mean F-measure 0.9003 using the proposed method). Further analysis showed that the convolutional layers of the CNN effectively identified a large number of keywords and automatically extracted enough concepts to predict the diagnosis codes. CONCLUSIONS: Word embedding combined with a CNN showed outstanding performance compared with traditional methods, needing very little data preprocessing. This shows that future studies will not be limited by incomplete dictionaries. A large amount of unstructured information from free-text medical writing will be extracted by automated approaches in the future, and we believe that the health care field is about to enter the age of big data.",convolutional neural network | data mining | electronic health records | electronic medical records | machine learning | natural language processing | neural networks (computer) | text mining | word embedding
"Family history is considered a core element of clinical care. In this study we assessed the quality of family history data captured in an established commercial electronic health record (EHR) at a large academic medical center. Because the EHR had no centralized location to store family history information, it was collected as part of clinical notes in structured or free-text format. We analyzed differences between 10,000 free-text and 9,121 structured family history observations. Each observation was classified according to disease presence/absence and family member affected (e.g., father, mother, etc.). The structured notes did not collect a complete family history as defined by standards endorsed by the U.S. Agency for Healthcare Research and Quality; the free-text notes contained more information than the structured notes, but still not enough to be considered ""complete."" Several barriers remain for collecting complete, useful family history data in electronic health records.",
"A significant part of medical knowledge is stored as unstructured free text. However, clinical narratives are known to contain duplicated sections due to clinicians' copy/paste parts of a former report into a new one. In this study, we aim at evaluating the duplications found within patient records in more than 650,000 French clinical narratives. We adapted a method to identify efficiently duplicated zones in a reasonable time. We evaluated the potential impact of duplications in two use cases: the presence of (i) treatments and/or (ii) relative dates. We identified an average rate of duplication of 33%. We found that 20% of the document contained drugs mentioned only in duplicated zones and that 1.45% of the document contained mentions of relative dates in duplicated zone, that could potentially lead to erroneous interpretation. We suggest the systematic identification and annotation of duplicated zones in clinical narratives for information extraction and temporal-oriented tasks.",Algorithms | Electronic Health Records | Natural Language Processing
"BACKGROUND: Research using electronic health records (EHRs) relies heavily on coded clinical data. Due to variation in coding practices, it can be difficult to aggregate the codes for a condition in order to define cases. This paper describes a methodology to develop 'indicator markers' found in patients with early rheumatoid arthritis (RA); these are a broader range of codes which may allow a probabilistic case definition to use in cases where no diagnostic code is yet recorded. METHODS: We examined EHRs of 5,843 patients in the General Practice Research Database, aged ≥ 30 y, with a first coded diagnosis of RA between 2005 and 2008. Lists of indicator markers for RA were developed initially by panels of clinicians drawing up code-lists and then modified based on scrutiny of available data. The prevalence of indicator markers, and their temporal relationship to RA codes, was examined in patients from 3 y before to 14 d after recorded RA diagnosis. FINDINGS: Indicator markers were common throughout EHRs of RA patients, with 83.5% having 2 or more markers. 34% of patients received a disease-specific prescription before RA was coded; 42% had a referral to rheumatology, and 63% had a test for rheumatoid factor. 65% had at least one joint symptom or sign recorded and in 44% this was at least 6-months before recorded RA diagnosis. CONCLUSION: Indicator markers of RA may be valuable for case definition in cases which do not yet have a diagnostic code. The clinical diagnosis of RA is likely to occur some months before it is coded, shown by markers frequently occurring ≥ 6 months before recorded diagnosis. It is difficult to differentiate delay in diagnosis from delay in recording. Information concealed in free text may be required for the accurate identification of patients and to assess the quality of care in general practice.",
"BACKGROUND: Numbers and numerical concepts appear frequently in free text clinical notes from electronic health records. Knowledge of the frequent lexical variations of these numerical concepts, and their accurate identification, is important for many information extraction tasks. This paper describes an analysis of the variation in how numbers and numerical concepts are represented in clinical notes. METHODS: We used an inverted index of approximately 100 million notes to obtain the frequency of various permutations of numbers and numerical concepts, including the use of Roman numerals, numbers spelled as English words, and invalid dates, among others. Overall, twelve types of lexical variants were analyzed. RESULTS: We found substantial variation in how these concepts were represented in the notes, including multiple data quality issues. We also demonstrate that not considering these variations could have substantial real-world implications for cohort identification tasks, with one case missing > 80% of potential patients. CONCLUSIONS: Numbering within clinical notes can be variable, and not taking these variations into account could result in missing or inaccurate information for natural language processing and information retrieval tasks.",Information retrieval | Lexical variation | Natural language processing
"PURPOSE: The aims of this study were to assess the fidelity of electronic health record documentation prompting premedication to iodinated contrast media and to determine the appropriateness of administered premedication on the basis of that documentation. METHODS: In this retrospective quality assurance cohort study, medication adverse events recorded in electronic health records between January 1, 2018, and August 31, 2019, to ""iodine,"" ""iodine-containing products,"" and ""iodinated contrast media"" were identified (N = 4,309); entries missing documentation (n = 1,651) and breakthrough reactions (n = 22) were excluded. Reaction description, severity, and free-text comments were used to categorize each entry as concordant (documentation matches recorded severity per the ACR Manual on Contrast Media version 10.3), discordant (description-severity mismatch, agent unrelated to iodinated contrast media, not a hypersensitivity reaction), or unclear. A subset of patients undergoing premedication was identified, and premedication was categorized as appropriate, inappropriate, or unsure on the basis of the index reaction using the aforementioned framework. Descriptive statistics were calculated. RESULTS: There were 2,636 adverse event entries in 2,441 patients: 59.9% (1,578 of 2,636) were classified as concordant, 30.2% (797 of 2,636) as discordant (n = 377 not a hypersensitivity reaction, n = 317 description-severity mismatch, and n = 103 unrelated agent), and 9.9% (n = 261) as unclear documentation. For the premedicated subset, concordance classification was feasible for 202 unique patients premedicated 335 times. Premedication was appropriate in 72% (240 of 335) and inappropriate in 22% (73 of 335); 17% of premedication events (56 of 335) were inappropriately administered for a prior physiologic reaction. CONCLUSIONS: Premedication prompts in the electronic health record are often erroneous because of inaccurate coding, incomplete data, and reaction misclassification. These errors result in inappropriate premedication for a substantial minority of patients.",Iodinated contrast | contrast allergy | medication allergy documentation | premedication
"BACKGROUND: Cardiovascular diseases (CVDs) are difficult to diagnose early and have risk factors that are easy to overlook. Early prediction and personalization of treatment through the use of artificial intelligence (AI) may help clinicians and patients manage CVDs more effectively. However, to apply AI approaches to CVDs data, it is necessary to establish and curate a specialized database based on electronic health records (EHRs) and include pre-processed unstructured data. METHODS: To build a suitable database (CardioNet) for CVDs that can utilize AI technology, contributing to the overall care of patients with CVDs. First, we collected the anonymized records of 748,474 patients who had visited the Asan Medical Center (AMC) or Ulsan University Hospital (UUH) because of CVDs. Second, we set clinically plausible criteria to remove errors and duplication. Third, we integrated unstructured data such as readings of medical examinations with structured data sourced from EHRs to create the CardioNet. We subsequently performed natural language processing to structuralize the significant variables associated with CVDs because most results of the principal CVD-related medical examinations are free-text readings. Additionally, to ensure interoperability for convergent multi-center research, we standardized the data using several codes that correspond to the common data model. Finally, we created the descriptive table (i.e., dictionary of the CardioNet) to simplify access and utilization of data for clinicians and engineers and continuously validated the data to ensure reliability. RESULTS: CardioNet is a comprehensive database that can serve as a training set for AI models and assist in all aspects of clinical management of CVDs. It comprises information extracted from EHRs and results of readings of CVD-related digital tests. It consists of 27 tables, a code-master table, and a descriptive table. CONCLUSIONS: CardioNet database specialized in CVDs was established, with continuing data collection. We are actively supporting multi-center research, which may require further data processing, depending on the subject of the study. CardioNet will serve as the fundamental database for future CVD-related research projects.",Artificial intelligence | Cardiovascular diseases | Database | Electronic health records
"BACKGROUND: Pressure injuries (PrI) are serious complications for many with spinal cord injury (SCI), significantly burdening health care systems, in particular the Veterans Health Administration. Clinical practice guidelines (CPG) provide recommendations. However, many risk factors span multiple domains. Effective prioritization of CPG recommendations has been identified as a need. Bioinformatics facilitates clinical decision support for complex challenges. The Veteran's Administration Informatics and Computing Infrastructure provides access to electronic health record (EHR) data for all Veterans Health Administration health care encounters. The overall study objective was to expand our prototype structural model of environmental, social, and clinical factors and develop the foundation for resource which will provide weighted systemic insight into PrI risk in veterans with SCI. METHODS: The SCI PrI Resource (SCI-PIR) includes three integrated modules: (1) the SCIPUDSphere multidomain database of veterans' EHR data extracted from October 2010 to September 2015 for ICD-9-CM coding consistency together with tissue health profiles, (2) the Spinal Cord Injury Pressure Ulcer and Deep Tissue Injury Ontology (SCIPUDO) developed from the cohort's free text clinical note (Text Integration Utility) notes, and (3) the clinical user interface for direct SCI-PIR query. RESULTS: The SCI-PIR contains relevant EHR data for a study cohort of 36,626 veterans with SCI, representing 10% to 14% of the U.S. population with SCI. Extracted datasets include SCI diagnostics, demographics, comorbidities, rurality, medications, and laboratory tests. Many terminology variations for non-coded input data were found. SCIPUDO facilitates robust information extraction from over six million Text Integration Utility notes annually for the study cohort. Visual widgets in the clinical user interface can be directly populated with SCIPUDO terms, allowing patient-specific query construction. CONCLUSION: The SCI-PIR contains valuable clinical data based on CPG-identified risk factors, providing a basis for personalized PrI risk management following SCI. Understanding the relative impact of risk factors supports PrI management for veterans with SCI. Personalized interactive programs can enhance best practices by decreasing both initial PrI formation and readmission rates due to PrI recurrence for veterans with SCI.",
"OBJECTIVE: In this work, we have developed a learning system capable of exploiting information conveyed by longitudinal Electronic Health Records (EHRs) for the prediction of a common postoperative complication, Anastomosis Leakage (AL), in a data-driven way and by fusing temporal population data from different and heterogeneous sources in the EHRs. MATERIAL AND METHODS: We used linear and non-linear kernel methods individually for each data source, and leveraging the powerful multiple kernels for their effective combination. To validate the system, we used data from the EHR of the gastrointestinal department at a university hospital. RESULTS: We first investigated the early prediction performance from each data source separately, by computing Area Under the Curve values for processed free text (0.83), blood tests (0.74), and vital signs (0.65), respectively. When exploiting the heterogeneous data sources combined using the composite kernel framework, the prediction capabilities increased considerably (0.92). Finally, posterior probabilities were evaluated for risk assessment of patients as an aid for clinicians to raise alertness at an early stage, in order to act promptly for avoiding AL complications. DISCUSSION: Machine-learning statistical model from EHR data can be useful to predict surgical complications. The combination of EHR extracted free text, blood samples values, and patient vital signs, improves the model performance. These results can be used as a framework for preoperative clinical decision support.",Clinical decision support | Colorectal cancer | Electronic health records | Feature selection | Heterogeneous clinical data | Kernel methods
"Disorders of sex development (DSD) represent a collection of rare diseases that generate substantial controversy regarding best practices for diagnosis and treatment. A significant barrier preventing a better understanding of how patients with these conditions should be evaluated and treated, especially from a psychological standpoint, is the lack of systematic and standardized approaches to identify cases for study inclusion. Common approaches include ""hand-picked"" subjects already known to the practice, which could introduce bias. We implemented an informatics-based approach to identify patients with DSD from electronic health records (EHRs) at three large, academic children's hospitals. The informatics approach involved comprehensively searching EHRs at each hospital using a combination of structured billing codes as an initial filtering strategy followed by keywords applied to the free text clinical documentation. The informatics approach was implemented to replicate the functionality of an EHR search engine (EMERSE) available at one of the hospitals. At the two hospitals that did not have EMERSE, we compared case ascertainment using the informatics method to traditional approaches employed for identifying subjects. Potential cases identified using all approaches were manually reviewed by experts in DSD to verify eligibility criteria. At the two institutions where both the informatics and traditional approaches were applied, the informatics approach identified substantially higher numbers of potential study subjects. The traditional approaches yielded 14 and 28 patients with DSD, respectively; the informatics approach yielded 226 and 77 patients, respectively. The informatics approach missed only a few cases that the traditional approaches identified, largely because those cases were known to the study team, but patient data were not in the particular children's hospital EHR. The use of informatics approaches to search electronic documentation can result in substantially larger numbers of subjects identified for studies of rare diseases such as DSD, and these approaches can be applied across hospitals.",
"BACKGROUND: Quality improvement requires using quality measures that can be implemented in a valid manner. Using guidelines set forth by the Meaningful Use portion of the Health Information Technology for Economic and Clinical Health Act, the authors assessed the feasibility and performance of an automated electronic Meaningful Use dental clinical quality measure to determine the percentage of children who received fluoride varnish. METHODS: The authors defined how to implement the automated measure queries in a dental electronic health record. Within records identified through automated query, the authors manually reviewed a subsample to assess the performance of the query. RESULTS: The automated query results revealed that 71.0% of patients had fluoride varnish compared with the manual chart review results that indicated 77.6% of patients had fluoride varnish. The automated quality measure performance results indicated 90.5% sensitivity, 90.8% specificity, 96.9% positive predictive value, and 75.2% negative predictive value. CONCLUSIONS: The authors' findings support the feasibility of using automated dental quality measure queries in the context of sufficient structured data. Information noted only in free text rather than in structured data would require using natural language processing approaches to effectively query electronic health records. PRACTICAL IMPLICATIONS: To participate in self-directed quality improvement, dental clinicians must embrace the accountability era. Commitment to quality will require enhanced documentation to support near-term automated calculation of quality measures.",Dental care for children | dental public health | informatics
"BACKGROUND: Surveillance for healthcare-associated infections such as healthcare-associated urinary tract infections (HA-UTI) is important for directing resources and evaluating interventions. However, traditional surveillance methods are resource-intensive and subject to bias. AIM: To develop and validate a fully automated surveillance algorithm for HA-UTI using electronic health record (EHR) data. METHODS: Five algorithms were developed using EHR data from 2979 admissions at Karolinska University Hospital from 2010 to 2011: (1) positive urine culture (UCx); (2) positive UCx + UTI codes (International Statistical Classification of Diseases and Related Health Problems, 10(th) revision); (3) positive UCx + UTI-specific antibiotics; (4) positive UCx + fever and/or UTI symptoms; (5) algorithm 4 with negation for fever without UTI symptoms. Natural language processing (NLP) was used for processing free-text medical notes. The algorithms were validated in 1258 potential UTI episodes from January to March 2012 and results extrapolated to all UTI episodes within this period (N = 16,712). The reference standard for HA-UTIs was manual record review according to the European Centre for Disease Prevention and Control (and US Centers for Disease Control and Prevention) definitions by trained healthcare personnel. FINDINGS: Of the 1258 UTI episodes, 163 fulfilled the ECDC HA-UTI definition and the algorithms classified 391, 150, 189, 194, and 153 UTI episodes, respectively, as HA-UTI. Algorithms 1, 2, and 3 had insufficient performances. Algorithm 4 achieved better performance and algorithm 5 performed best for surveillance purposes with sensitivity 0.667 (95% confidence interval: 0.594-0.733), specificity 0.997 (0.996-0.998), positive predictive value 0.719 (0.624-0.807) and negative predictive value 0.997 (0.996-0.997). CONCLUSION: A fully automated surveillance algorithm based on NLP to find UTI symptoms in free-text had acceptable performance to detect HA-UTI compared to manual record review. Algorithms based on administrative and microbiology data only were not sufficient.",Algorithms | Automated surveillance | Healthcare-associated infection | Natural language processing | Urinary tract infections
"BACKGROUND: Vital signs in our emergency department information system were entered into free-text fields for heart rate, respiratory rate, blood pressure, temperature and oxygen saturation. OBJECTIVE: We sought to convert these text entries into a more useful form, for research and QA purposes, upon entry into a data warehouse. METHODS: We derived a series of rules and assigned quality scores to the transformed values, conforming to physiologic parameters for vital signs across the age range and spectrum of illness seen in the emergency department. RESULTS: Validating these entries revealed that 98% of free-text data had perfect quality scores, conforming to established vital sign parameters. Average vital signs varied as expected by age. Degradations in quality scores were most commonly attributed logging temperature in Fahrenheit instead of Celsius; vital signs with this error could still be transformed for use. Errors occurred more frequently during periods of high triage, though error rates did not correlate with triage volume. CONCLUSIONS: In developing a method for importing free-text vital sign data from our emergency department information system, we now have a data warehouse with a broad array of quality-checked vital signs, permitting analysis and correlation with demographics and outcomes.",Data warehouse | electronic health records | emergency medicine | hospital information systems | text mining | user computer interface | vital signs.
"Patients experience various symptoms when they have either acute or chronic diseases or undergo some treatments for diseases. Symptoms are often indicators of the severity of the disease and the need for hospitalization. Symptoms are often described in free text written as clinical notes in the Electronic Health Records (EHR) and are not integrated with other clinical factors for disease prediction and healthcare outcome management. In this research, we propose a novel deep language model to extract patient-reported symptoms from clinical text. The deep language model integrates syntactic and semantic analysis for symptom extraction and identifies the actual symptoms reported by patients and conditional or negation symptoms. The deep language model can extract both complex and straightforward symptom expressions. We used a real-world clinical notes dataset to evaluate our model and demonstrated that our model achieves superior performance compared to three other state-of-the-art symptom extraction models. We extensively analyzed our model to illustrate its effectiveness by examining each components contribution to the model. Finally, we applied our model on a COVID-19 tweets data set to extract COVID-19 symptoms. The results show that our model can identify all the symptoms suggested by CDC ahead of their timeline and many rare symptoms.",
"OBJECTIVE: Depression is a prevalent disorder difficult to diagnose and treat. In particular, depressed patients exhibit largely unpredictable responses to treatment. Toward the goal of personalizing treatment for depression, we develop and evaluate computational models that use electronic health record (EHR) data for predicting the diagnosis and severity of depression, and response to treatment. MATERIALS AND METHODS: We develop regression-based models for predicting depression, its severity, and response to treatment from EHR data, using structured diagnosis and medication codes as well as free-text clinical reports. We used two datasets: 35,000 patients (5000 depressed) from the Palo Alto Medical Foundation and 5651 patients treated for depression from the Group Health Research Institute. RESULTS: Our models are able to predict a future diagnosis of depression up to 12 months in advance (area under the receiver operating characteristic curve (AUC) 0.70-0.80). We can differentiate patients with severe baseline depression from those with minimal or mild baseline depression (AUC 0.72). Baseline depression severity was the strongest predictor of treatment response for medication and psychotherapy. CONCLUSIONS: It is possible to use EHR data to predict a diagnosis of depression up to 12 months in advance and to differentiate between extreme baseline levels of depression. The models use commonly available data on diagnosis, medication, and clinical progress notes, making them easily portable. The ability to automatically determine severity can facilitate assembly of large patient cohorts with similar severity from multiple sites, which may enable elucidation of the moderators of treatment response in the future.",data mining | depression | electronic health records | ontology | personalized medicine
"BACKGROUND: Clinical information is often stored as free text, e.g. in discharge summaries or pathology reports. These documents are semi-structured using section headers, numbered lists, items and classification strings. However, it is still challenging to retrieve relevant documents since keyword searches applied on complete unstructured documents result in many false positive retrieval results. OBJECTIVES: We are concentrating on the processing of pathology reports as an example for unstructured clinical documents. The objective is to transform reports semi-automatically into an information structure that enables an improved access and retrieval of relevant data. The data is expected to be stored in a standardized, structured way to make it accessible for queries that are applied to specific sections of a document (section-sensitive queries) and for information reuse. METHODS: Our processing pipeline comprises information modelling, section boundary detection and section-sensitive queries. For enabling a focused search in unstructured data, documents are automatically structured and transformed into a patient information model specified through openEHR archetypes. The resulting XML-based pathology electronic health records (PEHRs) are queried by XQuery and visualized by XSLT in HTML. RESULTS: Pathology reports (PRs) can be reliably structured into sections by a keyword-based approach. The information modelling using openEHR allows saving time in the modelling process since many archetypes can be reused. The resulting standardized, structured PEHRs allow accessing relevant data by retrieving data matching user queries. CONCLUSIONS: Mapping unstructured reports into a standardized information model is a practical solution for a better access to data. Archetype-based XML enables section-sensitive retrieval and visualisation by well-established XML techniques. Focussing the retrieval to particular sections has the potential of saving retrieval time and improving the accuracy of the retrieval.",Standardized electronic health record | electronic health record system | information retrieval | medical informatics applications | openEHR | section boundary detection
"Rapid growth in electronic health records (EHRs) use has led to an unprecedented expansion of available clinical data in electronic formats. However, much of the important healthcare information is locked in the narrative documents. Therefore Natural Language Processing (NLP) technologies, e.g., Named Entity Recognition that identifies boundaries and types of entities, has been extensively studied to unlock important clinical information in free text. In this study, we investigated a novel deep learning method to recognize clinical entities in Chinese clinical documents using the minimal feature engineering approach. We developed a deep neural network (DNN) to generate word embeddings from a large unlabeled corpus through unsupervised learning and another DNN for the NER task. The experiment results showed that the DNN with word embeddings trained from the large unlabeled corpus outperformed the state-of-the-art CRF's model in the minimal feature engineering setting, achieving the highest F1-score of 0.9280. Further analysis showed that word embeddings derived through unsupervised learning from large unlabeled corpus remarkably improved the DNN with randomized embedding, denoting the usefulness of unsupervised feature learning.",
"BACKGROUND: In developing countries such as Iran, international standards offer good sources to survey and use for appropriate planning in the domain of electronic health records (EHRs). Therefore, in this study, HL7 and ASTM standards were considered as the main sources from which to extract EHR data. OBJECTIVE: The objective of this study was to propose a hospital data set for a national EHR consisting of data classes and data elements by adjusting data sets extracted from the standards and paper-based records. METHOD: This comparative study was carried out in 2017 by studying the contents of the paper-based records approved by the health ministry in Iran and the international ASTM and HL7 standards in order to extract a minimum hospital data set for a national EHR. RESULTS: As a result of studying the standards and paper-based records, a total of 526 data elements in 174 classes were extracted. An examination of the data indicated that the highest number of extracted data came from the free text elements, both in the paper-based records and in the standards related to the administrative data. The major sources of data extracted from ASTM and HL7 were the E1384 and Hl7V.x standards, respectively. In the paper-based records, data were extracted from 19 forms sporadically. DISCUSSION: By declaring the confidentiality of information, the ASTM standards acknowledge the issue of confidentiality of information as one of the main challenges of EHR development, and propose new types of admission, such as teleconference, tele-video, and home visit, which are inevitable with the advent of new technology for providing healthcare and treating diseases. Data related to finance and insurance, which were scattered in different categories by three organizations, emerged as the financial category. Documenting the role and responsibility of the provider by adding the authenticator/signature data element was deemed essential. CONCLUSION: Not only using well-defined and standardized data, but also adapting EHR systems to the local facilities and the existing social and cultural conditions, will facilitate the development of structured data sets.",comparative study | data set | electronic health record | hospital data
"BACKGROUND: Persistent somatic symptoms (PSS) are common in primary care and often accompanied by an increasing disease burden for both the patient and healthcare. In medical practice, PSS is historically considered a diagnosis by exclusion or primarily seen as psychological. Besides, registration of PSS in electronic health records (EHR) is unambiguous and possibly does not reflect classification adequately. The present study explores how general practitioners (GPs) currently register PSS, and their view regarding the need for improvements in classification, registration, and consultations. METHOD: Dutch GPs were invited by email to participate in a national cross-sectional online survey. The survey addressed ICPC-codes used by GPs to register PSS, PSS-related terminology added to free text areas, usage of PSS-related syndrome codes, and GPs' need for improvement of PSS classification, registration and care. RESULTS: GPs (n = 259) were most likely to use codes specific to the symptom presented (89.3%). PSS-related terminology in free-text areas was used sparsely. PSS-related syndrome codes were reportedly used by 91.5% of GPs, but this was primarily the case for the code for irritable bowel syndrome. The ambiguous registration of PSS is reported as problematic by 47.9% of GPs. Over 56.7% of GPs reported needing additional training, tools or other support for PSS classification and consultation. GPs also reported needing other referral options and better guidelines. CONCLUSIONS: Registration of PSS in primary care is currently ambiguous. Approximately half of GPs felt a need for more options for registration of PSS and reported a need for further support. In order to improve classification, registration and care for patients with PSS, there is a need for a more appropriate coding scheme and additional training.",Classification of Disease | Clinical Coding | Electronic Health Records | General Practitioners | Medically Unexplained Symptoms | Persistent Somatic Symptoms | Primary Health Care
"BACKGROUND: In recent years electronic health records (EHRs) have been introduced on a large scale into mental health care. EHRs have a great number of advantages, one of the main ones being readability. However, very little attention seems to have been paid to the potential disadvantages and risks associated with EHRs. AIM: To point to some of the disadvantages and risks of EHRs, in their present form, particularly in relation to the care of patients with severe mental illness (SMI). METHOD: On the basis of clinical experience and relevant literature, we discuss some of the disadvantages and risks associated with EHRs in their current form. RESULTS: In long-term, multidisciplinary and complex treatments of patients with SMI, EHRs in their current form fail to provide the psychiatrist with an adequate overview of the treatment process. This is largely due to the way they are designed: an ever-increasing quantity of information about complex treatment stored in separate files that can only be accessed individually and that contain free text. In mental health care the introduction of new technology, unlike the introduction of new drugs, seems to occur without structured surveillance of the disadvantages and risks involved. CONCLUSION: EHRs need to be re-designed at the earliest opportunity.",
"BACKGROUND: Since the creation of the problem-oriented medical record, the building of problem lists has been the focus of many studies. To date, this issue is not well resolved, and building an appropriate contextualized problem list is still a challenge. OBJECTIVE: This paper aims to present the process of building a shared multipurpose common problem list at the Geneva University Hospitals. This list aims to bridge the gap between clinicians' language expressed in free text and secondary uses requiring structured information. METHODS: We focused on the needs of clinicians by building a list of uniquely identified expressions to support their daily activities. In the second stage, these expressions were connected to additional information to build a complex graph of information. A list of 45,946 expressions manually extracted from clinical documents was manually curated and encoded in multiple semantic dimensions, such as International Classification of Diseases, 10th revision; International Classification of Primary Care 2nd edition; Systematized Nomenclature of Medicine Clinical Terms; or dimensions dictated by specific usages, such as identifying expressions specific to a domain, a gender, or an intervention. The list was progressively deployed for clinicians with an iterative process of quality control, maintenance, and improvements, including the addition of new expressions or dimensions for specific needs. The problem management of the electronic health record allowed the measurement and correction of encoding based on real-world use. RESULTS: The list was deployed in production in January 2017 and was regularly updated and deployed in new divisions of the hospital. Over 4 years, 684,102 problems were created using the list. The proportion of free-text entries decreased progressively from 37.47% (8321/22,206) in December 2017 to 18.38% (4547/24,738) in December 2020. In the last version of the list, over 14 dimensions were mapped to expressions, among which 5 were international classifications and 8 were other classifications for specific uses. The list became a central axis in the electronic health record, being used for many different purposes linked to care, such as surgical planning or emergency wards, or in research, for various predictions using machine learning techniques. CONCLUSIONS: This study breaks with common approaches primarily by focusing on real clinicians' language when expressing patients' problems and secondarily by mapping whatever is required, including controlled vocabularies to answer specific needs. This approach improves the quality of the expression of patients' problems while allowing the building of as many structured dimensions as needed to convey semantics according to specific contexts. The method is shown to be scalable, sustainable, and efficient at hiding the complexity of semantics or the burden of constraint-structured problem list entry for clinicians. Ongoing work is analyzing the impact of this approach on how clinicians express patients' problems.",electronic health records | medical records | problem-oriented | semantics
"Observational research using data from electronic health records (EHR) is a rapidly growing area, which promises both increased sample size and data richness - therefore unprecedented study power. However, in many medical domains, large amounts of potentially valuable data are contained within the free text clinical narrative. Manually reviewing free text to obtain desired information is an inefficient use of researcher time and skill. Previous work has demonstrated the feasibility of applying Natural Language Processing (NLP) to extract information. However, in real world research environments, the demand for NLP skills outweighs supply, creating a bottleneck in the secondary exploitation of the EHR. To address this, we present TextHunter, a tool for the creation of training data, construction of concept extraction machine learning models and their application to documents. Using confidence thresholds to ensure high precision (>90%), we achieved recall measurements as high as 99% in real world use cases.",
"Nowadays most hospitals use electronic health records as part of their hospital information systems. However, these systems are more suitable for hospital management than for physicians. The health record is not sufficiently structured; it includes a lot of free-text information, and the set of collected attributes is fixed and practically impossible to extend. Physicians, gathering information for the purpose of medical studies, often use varied proprietary solutions based on MS Access databases or MS Excel Sheets. The EuroMISE Centre - Cardio is developing an electronic health record (EHR) application called MUDRLite, which could easily fill the gap between existing EHRs. MUDRLite is the result of applied research in the field of EHR design, which is based on experience gathered during cooperation in the TripleC project. MUDRLite development is an extra branch in the MUDR (MUltimedia Distributed Record) development; it simplifies both the MUDR architecture and the MUDR data storage principles.MUDRLite itself is an empty body, which has to be filled in with an XML configuration file. This file completely describes the visual aspects and the behavior of the EHR application. It includes simple 4GL-like constructs written in the MUDRLite Language (MLL). This enables - using the event-oriented programming principles - to program various handling procedures for a range of actions, e.g. clicking a button fills a form with the result of an SQL statement. MUDRLite can be tailored to particular needs of any healthcare provider. This makes the MUDRLite application easy to use in specific environments. In the first instance, we are testing it at the Neurovascular Department of the Central Military Hospital in Prague.",
"This research extracted patient-reported symptoms from free-text EHR notes of colorectal and breast cancer patients and studied the correlation of the symptoms with comorbid type 2 diabetes, race, and smoking status. An NLP framework was developed first to use UMLS MetaMap to extract all symptom terms from the 366,398 EHR clinical notes of 1694 colorectal cancer (CRC) patients and 3458 breast cancer (BC) patients. Semantic analysis and clustering algorithms were then developed to categorize all the relevant symptoms into eight symptom clusters defined by seed terms. After all the relevant symptoms were extracted from the EHR clinical notes, the frequency of the symptoms reported from colorectal cancer (CRC) and breast cancer (BC) patients over three time-periods post-chemotherapy was calculated. Logistic regression (LR) was performed with each symptom cluster as the response variable while controlling for diabetes, race, and smoking status. The results show that the CRC and BC patients with Type 2 Diabetes (T2D) were more likely to report symptoms than CRC and BC without T2D over three time-periods in the cancer trajectory. We also found that current smokers were more likely to report anxiety (CRC, BC), neuropathic symptoms (CRC, BC), anxiety (BC), and depression (BC) than non-smokers.",clinical decision-making | data mining | electronic health records | machine learning | text mining
"BACKGROUND: Narrative data entry pervades computerized health information systems and serves as a key component in collecting patient-related information in electronic health records and patient safety event reporting systems. The quality and efficiency of clinical data entry are critical in arriving at an optimal diagnosis and treatment. The application of text prediction holds potential for enhancing human performance of data entry in reporting patient safety events. OBJECTIVE: This study examined two functions of text prediction intended for increasing efficiency and data quality of text data entry reporting patient safety events. METHODS: The study employed a two-group randomized design with 52 nurses. The nurses were randomly assigned into a treatment group or a control group with a task of reporting five patient fall cases in Chinese using a web-based test system, with or without the prediction functions. T-test, Chi-square and linear regression model were applied to evaluating the outcome differences in free-text data entry between the groups. RESULTS: While both groups of participants exhibited a good capacity for accomplishing the assigned task of reporting patient falls, the results from the treatment group showed an overall increase of 70.5% in text generation rate, an increase of 34.1% in reporting comprehensiveness score and a reduction of 14.5% in the non-adherence of the comment fields. The treatment group also showed an increasing text generation rate over time, whereas no such an effect was observed in the control group. CONCLUSION: As an attempt investigating the effectiveness of text prediction functions in reporting patient safety events, the study findings proved an effective strategy for assisting reporters in generating complementary free text when reporting a patient safety event. The application of the strategy may be effective in other clinical areas when free text entries are required.",Data entry | Incident reporting | Patient safety | Text prediction | Two-group randomized design | Usability evaluation
"Efficient prediction of cancer recurrence in advance may help to recruit high risk breast cancer patients for clinical trial on-time and can guide a proper treatment plan. Several machine learning approaches have been developed for recurrence prediction in previous studies, but most of them use only structured electronic health records and only a small training dataset, with limited success in clinical application. While free-text clinic notes may offer the greatest nuance and detail about a patient's clinical status, they are largely excluded in previous predictive models due to the increase in processing complexity and need for a complex modeling framework. In this study, we developed a weak-supervision framework for breast cancer recurrence prediction in which we trained a deep learning model on a large sample of free-text clinic notes by utilizing a combination of manually curated labels and NLP-generated non-perfect recurrence labels. The model was trained jointly on manually curated data from 670 patients and NLP-curated data of 8062 patients. It was validated on manually annotated data from 224 patients with recurrence and achieved 0.94 AUROC. This weak supervision approach allowed us to learn from a larger dataset using imperfect labels and ultimately provided greater accuracy compared to a smaller hand-curated dataset, with less manual effort invested in curation.",
"Coding accuracy in case-mix databases enables efficient funding of health facilities and accurate epidemiological statistics based on patients' stays information. We assume that the data collected in the electronic health record, especially drug prescriptions and medical reports are relevant for checking the consistency of the coding of diagnoses. We evaluated a new coding control tool, ""TOLBIAC control"", embedded in the Web100T coding assistant. This tool interacts with the Vidal Application Programming Interface and the electronic health record of the University Hospital of Saint-Etienne. The micro-average F-measure was 0.76 for drug prescriptions and 0.55 for free text medical reports. This initial evaluation has revealed that drug prescriptions in EHRs can successfully be used to develop an automated ICD-10 code-control tool. Nevertheless the ""TOLBIAC control"" tool is not yet fully effective for widespread use because of its limited performance in text analysis, a feature that is currently undergoing improvements.",Clinical Coding | Diagnosis Related Groups | Drug Prescriptions | Electronic Health Records | International Classification of Diseases
"BACKGROUND: Nationally endorsed, clinical performance measures are available that allow for quality reporting using electronic health records (EHRs). To our knowledge, how well they reflect actual quality of care has not been studied. We sought to evaluate the validity of performance measures for coronary artery disease (CAD) using an ambulatory EHR. METHODS: We performed a retrospective electronic medical chart review comparing automated measurement with a 2-step process of automated measurement supplemented by review of free-text notes for apparent quality failures for all patients with CAD from a large internal medicine practice using a commercial EHR. The 7 performance measures included the following: antiplatelet drug, lipid-lowering drug, beta-blocker following myocardial infarction, blood pressure measurement, lipid measurement, low-density lipoprotein cholesterol control, and angiotensin-converting enzyme inhibitor or angiotensin receptor blocker for patients with diabetes mellitus or left ventricular systolic dysfunction. RESULTS: Performance varied from 81.6% for lipid measurement to 97.6% for blood pressure measurement based on automated measurement. A review of free-text notes for cases failing an automated measure revealed that misclassification was common and that 15% to 81% of apparent quality failures either satisfied the performance measure or met valid exclusion criteria. After including free-text data, the adherence rate ranged from 87.5% for lipid measurement and low-density lipoprotein cholesterol control to 99.2% for blood pressure measurement. CONCLUSIONS: Profiling the quality of outpatient CAD care using data from an EHR has significant limitations. Changes in how data are routinely recorded in an EHR are needed to improve the accuracy of this type of quality measurement. Validity testing in different settings is required.",
"BACKGROUND: Physician and nurses have worked together for generations; however, their language and training are vastly different; comparing and contrasting their work and their joint impact on patient outcomes is difficult in light of this difference. At the same time, the EHR only includes the physician perspective via the physician-authored discharge summary, but not nurse documentation. Prior research in this area has focused on collaboration and the usage of similar terminology. OBJECTIVE: The objective of the study is to gain insight into interprofessional care by developing a computational metric to identify similarities, related concepts and differences in physician and nurse work. METHODS: 58 physician discharge summaries and the corresponding nurse plans of care were transformed into Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). MedLEE, a Natural Language Processing (NLP) program, extracted ""physician terms"" from free-text physician summaries. The nursing plans of care were constructed using the HANDS(©) nursing documentation software. HANDS(©) utilizes structured terminologies: nursing diagnosis (NANDA-I), outcomes (NOC), and interventions (NIC) to create ""nursing terms"". The physician's and nurse's terms were compared using the UMLS network for relatedness, overlaying the physician and nurse terms for comparison. Our overarching goal is to provide insight into the care, by innovatively applying graph algorithms to the UMLS network. We reveal the relationships between the care provided by each professional that is specific to the patient level. RESULTS: We found that only 26% of patients had synonyms (identical UMLS CUIs) between the two professions' documentation. On average, physicians' discharge summaries contain 27 terms and nurses' documentation, 18. Traversing the UMLS network, we found an average of 4 terms related (distance less than 2) between the professions, leaving most concepts as unrelated between nurse and physician care. CONCLUSION: Our hypothesis that physician's and nurse's practice domains are markedly different is supported by the preliminary, quantitative evidence we found. Leveraging the UMLS network and graph traversal algorithms, allows us to compare and contrast nursing and physician care on a single patient, enabling a more complete picture of patient care. We can differentiate professional contributions to patient outcomes and related and divergent concepts by each profession.",Health informatics | Nursing documentation | Physician documentation | Professional discontinuity | UMLS
"Supporting data entry by clinicians is considered one of the greatest challenges in implementing electronic health records. In this paper we describe a formative evaluation study using three different methodologies through which we identified obstacles to point-of-care data entry for eye care and then used the formative process to develop and test solutions to overcome these obstacles. The greatest obstacles were supporting free text annotation of clinical observations and accommodating the creation of detailed diagrams in multiple colors. To support free text entry, we arrived at an approach that captures an image of a free text note and associates this image with related data elements in an encounter note. The detailed diagrams included a color pallet that allowed changing pen color with a single stroke and also captured the diagrams as an image associated with related data elements. During observed sessions with simulated patients, these approaches satisfied the clinicians' documentation needs by capturing the full range of clinical complexity that arises in practice.",
"OBJECTIVES: Comparative effectiveness research (CER) requires the capture and analysis of data from disparate sources, often from a variety of institutions with diverse electronic health record (EHR) implementations. In this paper we describe the CER Hub, a web-based informatics platform for developing and conducting research studies that combine comprehensive electronic clinical data from multiple health care organizations. METHODS: The CER Hub platform implements a data processing pipeline that employs informatics standards for data representation and web-based tools for developing study-specific data processing applications, providing standardized access to the patient-centric electronic health record (EHR) across organizations. RESULTS: The CER Hub is being used to conduct two CER studies utilizing data from six geographically distributed and demographically diverse health systems. These foundational studies address the effectiveness of medications for controlling asthma and the effectiveness of smoking cessation services delivered in primary care. DISCUSSION: The CER Hub includes four key capabilities: the ability to process and analyze both free-text and coded clinical data in the EHR; a data processing environment supported by distributed data and study governance processes; a clinical data-interchange format for facilitating standardized extraction of clinical data from EHRs; and a library of shareable clinical data processing applications. CONCLUSION: CER requires coordinated and scalable methods for extracting, aggregating, and analyzing complex, multi-institutional clinical data. By offering a range of informatics tools integrated into a framework for conducting studies using EHR data, the CER Hub provides a solution to the challenges of multi-institutional research using electronic medical record data.",Comparative effectiveness research | Electronic health records | Natural language processing
"CONTEXT: Patient reported outcomes (PROs) are one means of systematically gathering meaningful subjective information for patient care, population health, and patient centered outcomes research. However, optimal data management for effective PRO applications is unclear. CASE DESCRIPTION: Delivery systems associated with the Health Care Systems Research Network (HCSRN) have implemented PRO data collection as part of the Medicare annual Health Risk Assessment (HRA). A questionnaire assessed data content, collection, storage, and extractability in HCSRN delivery systems. FINDINGS: Responses were received from 15 (83.3 percent) of 18 sites. The proportion of Medicare beneficiaries completing an HRA ranged from less than 10 to 42 percent. Most sites collected core HRA elements and 10 collected information on additional domains such as social support. Measures for core domains varied across sites. Data were collected at and prior to visits. Modes included paper, clinician entry, patient portals, and interactive voice response. Data were stored in the electronic health record (EHR) in scanned documents, free text, and discrete fields, and in summary databases. MAJOR THEMES: PRO implementation requires effectively collecting, storing, extracting, and applying patient-reported data. Standardizing PRO measures and storing data in extractable formats can facilitate multi-site uses for PRO data, while access to individual PROs in the EHR may be sufficient for use at the point of care. CONCLUSION: Collecting comparable PRO data elements, storing data in extractable fields, and collecting data from a higher proportion of eligible respondents represents an optimal approach to support multi-site applications of PRO information.",data collection | electronic health records | patient reported outcomes | patient-centered care
"Linking clinical narratives to standardized vocabularies and coding systems is a key component of unlocking the information in medical text for analysis. However, many domains of medical concepts, such as functional outcomes and social determinants of health, lack well-developed terminologies that can support effective coding of medical text. We present a framework for developing natural language processing (NLP) technologies for automated coding of medical information in under-studied domains, and demonstrate its applicability through a case study on physical mobility function. Mobility function is a component of many health measures, from post-acute care and surgical outcomes to chronic frailty and disability, and is represented as one domain of human activity in the International Classification of Functioning, Disability, and Health (ICF). However, mobility and other types of functional activity remain under-studied in the medical informatics literature, and neither the ICF nor commonly-used medical terminologies capture functional status terminology in practice. We investigated two data-driven paradigms, classification and candidate selection, to link narrative observations of mobility status to standardized ICF codes, using a dataset of clinical narratives from physical therapy encounters. Recent advances in language modeling and word embedding were used as features for established machine learning models and a novel deep learning approach, achieving a macro-averaged F-1 score of 84% on linking mobility activity reports to ICF codes. Both classification and candidate selection approaches present distinct strengths for automated coding in under-studied domains, and we highlight that the combination of (i) a small annotated data set; (ii) expert definitions of codes of interest; and (iii) a representative text corpus is sufficient to produce high-performing automated coding systems. This research has implications for continued development of language technologies to analyze functional status information, and the ongoing growth of NLP tools for a variety of specialized applications in clinical care and research.",Disability | EHR data | Free text | ICF | Machine learning | Natural language processing | Physical function | Rehabilitation
"By automating collection of data elements, electronic health records may simplify the process of measuring the quality of medical care. Using data from a quality improvement initiative in primary care medical groups, we sought to determine whether the quality of care for falls and fear of falling in outpatients aged 75 and older could be accurately measured solely from codable (non-free-text) data in a structured visit note. A traditional medical record review by trained abstractors served as the criterion standard. Among 215 patient records reviewed, we found a structured visit note in 54% of charts within 3 mo of the date patients had been identified as having falls or fear of falling. The reliability of an algorithm based on codable data was at least good (kappa of at least 0.61) compared with full medical record review for three care processes recommended for patients with two falls or one fall with injury in the past year: orthostatic vital signs, vision test/eye examination, and home safety evaluation. However, the automated algorithm routinely underestimated quality of care. Performance standards based on automated measurement of quality of care from electronic health records need to account for documentation occurring in nonstructured form.",
"Problems in the structure, consistency, and completeness of electronic health record data are barriers to outcomes research, quality improvement, and practice redesign. This nonexperimental retrospective study examines the utility of importing de-identified electronic health record data into an external system to identify patients with and at risk for essential hypertension. We find a statistically significant increase in cases based on combined use of diagnostic and free-text coding (mean = 1,256.1, 95% CI 1,232.3-1,279.7) compared to diagnostic coding alone (mean = 1,174.5, 95% CI 1,150.5-1,198.3). While it is not surprising that significantly more patients are identified when broadening search criteria, the implications are critical for quality of care, the movement toward the National Committee for Quality Assurance's Patient-Centered Medical Home program, and meaningful use of electronic health records. Further, we find a statistically significant increase in potential cases based on the last two or more blood pressure readings greater than or equal to 140/90 mm Hg (mean = 1,353.9, 95% CI 1,329.9-1,377.9).",data quality | electronic health record | hypertension | outcomes research | quality care | registry
"The deployment of sophisticated software tools and electronic health records offers many new opportunities and challenges to support care delivery. One of the key opportunities is to enhance the quality of care with evidence-based medicine (EBM). One of the key challenges is to embed EBM in tools that directly facilitate the process of documentation and care delivery. Since clinicians typically have the option of using free text for most of their documentation, the tools that provide embedded EBM must be at least as efficient as free text. There are many requirements that must be met in order to effectively embed EBM within clinical content tools and enhance both the usability and the actual use of such tools and clinical content: (1) Facilitate the documentation process; (2) Facilitate the care delivery process, e.g. make order entry faster; (3) Contain recommendations that are highly relevant to the clinical context of an encounter; (4) Aid in the capture of discrete coded data. Support for local variation is often key to meeting these objectives and becomes a central factor in helping clinicians shift from unstructured free text, to the use of these tools, which support the delivery of EBM. This document describes the central tension between the objective of national standardization and delivery of EBM and the need for regional localization of clinical content. This tension must be thoughtfully managed to maximize the quality of care delivery and associated workflow practices. The key elements of legitimate local variation that must be recognized in order to achieve these goals are described in this document, and the key principles for managing the tensions between generalization and localization are identified.",
"BACKGROUND: Disparities research is often limited by incomplete accounting for differences in health status by populations. In the U.S., hysterectomy shows marked variation by race and geography, but it is difficult to understand what factors cause these variations without accounting for differences in the severity of gynecologic symptoms that drive hysterectomy decision-making. OBJECTIVE: Our objective is to demonstrate a method for using electronic health record (EHR)-derived data to create composite symptom severity indices to more fully capture relevant markers that influence the decision for hysterectomy. STUDY DESIGN: This was a retrospective cohort study of 1,993 people who underwent hysterectomy between April 4, 2014 and December 31, 2017 from ten hospitals and over 100 outpatient clinics in North Carolina. EHR data including billing, pharmacy, laboratory data, and free text notes, were used to identify markers of 3 common indications for hysterectomy: bulk symptoms (pressure from uterine enlargement), vaginal bleeding, and pelvic pain. To develop weighted symptom indices, we finalized a scoring algorithm based on the relationship of each marker to an objective measure, in combination with clinical expertise, with the goal of composite symptom severity indices that had sufficient variation to be useful in comparing different patient groups and allow discrimination between more or less severe symptoms of bulk, bleeding, or pain. RESULTS: Ranges of symptom severity scores varied across the three indices; including composite bulk score (0 to 14), vaginal bleeding score (0 to 44) and pain score (0 to 30). Mean values of each composite symptom severity index were greater for those who had diagnostic codes for vaginal bleeding, bulk symptoms, or pelvic pain, respectively. However, each index also demonstrated variation across the entire group of hysterectomy cases and identified symptoms that ranged in severity among those with and without the target diagnostic codes. CONCLUSIONS: Leveraging multisource data to create composite symptom severity indices provided greater discriminatory power to assess common gynecologic indications for hysterectomy. These methods can advance understanding in healthcare use in the setting of long-standing inequities and be applied across populations to account for previously unexplained variation across race, geography, and other social indicators.",Electronic Health Records | Health Equity | Hysterectomy | Leiomyoma | Quality of Life
"BACKGROUND: National health surveys indicate that chronic kidney disease (CKD) is an increasingly prevalent condition in Australia, placing a significant burden on the health budget and on the affected individuals themselves. Yet, there are relatively limited data on the prevalence of CKD within Australian general practice patients. In part, this could be due to variation in the terminology used by general practitioners (GPs) to identify and document a diagnosis of CKD. This project sought to investigate the variation in terms used when recording a diagnosis of CKD in general practice. METHODS: A search of routinely collected de-identified Australian general practice patient data (NPS MedicineWise MedicineInsight from January 1, 2013, to June 1, 2016; collected from 329 general practices) was conducted to determine the terms used. Manual searches were conducted on coded and on ""free-text"" or narrative information in the medical history, reason for encounter, and reason for prescription data fields. RESULTS: From this data set, 61 102 patients were potentially diagnosable with CKD on the basis of pathology results, but only 14 172 (23.2%) of these had a term representing CKD in their electronic record. Younger patients with pathology evidence of CKD were more likely to have documented CKD compared with older patients. There were a total of 2090 unique recorded documentation terms used by the GPs for CKD. The most commonly used terms tended to be those included as ""pick-list"" options within the various general practice software packages' standard ""classifications,"" accounting for 84% of use. CONCLUSIONS: A diagnosis of CKD was often not documented and, when recorded, it was in a variety of ways. While recording CKD with various terms and in free-text fields may allow GPs to flexibly document disease qualifiers and enter patient specific information, it might inadvertently decrease the quality of data collected from general practice records for clinical audit or research purposes.",chronic kidney disease | classification | coding | documentation | electronic health records | epidemiology | general practice | terminology
"Data mining is a powerful tool to reduce costs and mitigate errors in the diagnostic analysis and repair of complex engineered system, but it has yet to be applied systematically to the most complex and socially expensive system - the human body. The currently available approaches of knowledge-based and pattern-based artificial intelligence are unsuited to the iterative and often subjective nature of clinician-patient interactions. Furthermore, current electronic health records generally have poor design and low quality for such data mining. Bayesian methods have been developed to suggest multiple possible diagnoses given a set of clinical findings, but the larger problem is advising the physician on useful next steps. A new approach based on inverting Bayesian inference allows identification of the diagnostic actions that are most likely to disambiguate a differential diagnosis at each point in a patient's work-up. This can be combined with personalized cost information to suggest a cost-effective path to the clinician. Because the software is tracking the clinician's decision-making process, it can provide salient suggestions for both diagnoses and diagnostic tests in standard, coded formats that need only to be selected. This would reduce the need to type in free text, which is prone to ambiguities, omissions and errors. As the database of high-quality records grows, the scope, utility and acceptance of the system should also grow automatically, without requiring expert updating or correction.",Artificial intelligence | Bayesian | Cost-effectiveness | Diagnostic tests | Electronic health records | Learning health system
"OBJECTIVE: We present the Clinical Trial Knowledge Base, a regularly updated knowledge base of discrete clinical trial eligibility criteria equipped with a web-based user interface for querying and aggregate analysis of common eligibility criteria. MATERIALS AND METHODS: We used a natural language processing (NLP) tool named Criteria2Query (Yuan et al., 2019) to transform free text clinical trial eligibility criteria from ClinicalTrials.gov into discrete criteria concepts and attributes encoded using the widely adopted Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM) and stored in a relational SQL database. A web application accessible via RESTful APIs was implemented to enable queries and visual aggregate analyses. We demonstrate CTKB's potential role in EHR phenotype knowledge engineering using ten validated phenotyping algorithms. RESULTS: At the time of writing, CTKB contained 87,504 distinctive OMOP CDM standard concepts, including Condition (47.82%), Drug (23.01%), Procedure (13.73%), Measurement (24.70%) and Observation (5.28%), with 34.78% for inclusion criteria and 65.22% for exclusion criteria, extracted from 352,110 clinical trials. The average hit rate of criteria concepts in eMERGE phenotype algorithms is 77.56%. CONCLUSION: CTKB is a novel comprehensive knowledge base of discrete eligibility criteria concepts with the potential to enable knowledge engineering for clinical trial cohort definition, clinical trial population representativeness assessment, electronical phenotyping, and data gap analyses for using electronic health records to support clinical trial recruitment.",Clinical Trial | Eligibility Criteria | Knowledge Base | Natural Language Processing
"Nationally, nearly 40 percent of community-dwelling adults age 65 and older fall at least once a year, making unintentional falls the leading cause of both fatal and nonfatal injuries among this age group. Addressing this public health problem in primary care offers promise. However, challenges in incorporating fall risk screening into primary care result in a problem of missed opportunities for screening, counseling, intervention, and ultimately prevention. Given these barriers, this study examines the potential for the innovative use of routinely collected electronic health record data to provide enhanced clinical decision support in busy, often resource-thin primary care environments. Using de-identified data from a sample of West Virginia primary care centers, we find that it is both feasible and worthwhile to repurpose routinely collected data for the purpose of identification of older adults at risk of falls. Searching of both free-text and semistructured data was particularly valuable.",West Virginia | electronic health records | older adults | unintentional falls
"BACKGROUND: Sharing personal health information positively impacts quality of care across several domains, and particularly, safety and patient-centeredness. Patients may identify and flag up inconsistencies in their electronic health records (EHRs), leading to improved information quality and patient safety. However, in order to identify potential errors, patients need to be able to understand the information contained in their EHRs. OBJECTIVE: The aim of this study was to assess patients' perceptions of their ability to understand the information contained in their EHRs and to analyze the main barriers to their understanding. Additionally, the main types of patient-reported errors were characterized. METHODS: A cross-sectional web-based survey was undertaken between March 2017 and September 2017. A total of 682 registered users of the Care Information Exchange, a patient portal, with at least one access during the time of the study were invited to complete the survey containing both structured (multiple choice) and unstructured (free text) questions. The survey contained questions on patients' perceived ability to understand their EHR information and therefore, to identify errors. Free-text questions allowed respondents to expand on the reasoning for their structured responses and provide more detail about their perceptions of EHRs and identifying errors within them. Qualitative data were systematically reviewed by 2 independent researchers using the framework analysis method in order to identify emerging themes. RESULTS: A total of 210 responses were obtained. The majority of the responses (123/210, 58.6%) reported understanding of the information. The main barriers identified were information-related (medical terminology and knowledge and interpretation of test results) and technology-related (user-friendliness of the portal, information display). Inconsistencies relating to incomplete and incorrect information were reported in 12.4% (26/210) of the responses. CONCLUSIONS: While the majority of the responses affirmed the understanding of the information contained within the EHRs, both technology and information-based barriers persist. There is a potential to improve the system design to better support opportunities for patients to identify errors. This is with the aim of improving the accuracy, quality, and timeliness of the information held in the EHRs and a mechanism to further engage patients in their health care.",electronic health records | medical errors | patient participation | patient portals | patient safety
"BACKGROUND: Psychosocial information informs clinical decisions by providing crucial context for patients' barriers to recommended self-care; this is especially important in outpatient diabetes care because outcomes are largely dependent upon self-care behavior. Little is known about provider perceptions of use of psychosocial information. Further, while EHRs have dramatically changed how providers interact with patient health information, the EHRs' role in collection and retrieval of psychosocial information is not understood. METHODS: We designed a qualitative study. We used semi-structured interviews to investigate physicians' (N = 17) perspectives on the impact of EHR for psychosocial information use for outpatient Type II diabetes care decisions. We selected the constant comparative method to analyze the data. FINDINGS: Psychosocial information is perceived as dissimilar from other clinical information such as HbA1c and prescribed medications. Its narrative form conveys the patient's story, which elucidates barriers to following self-care recommendations. The narrative is abstract, and requires interpretation of patterns. Psychosocial information is also circumstantial; hence, the patients' context determines influence on self-care. Furthermore, EHRs can impair the collection of psychosocial information because the designs of EHR tools make it difficult to document, search for, and retrieve it. Templates do not enable users from collecting the patient's 'story', and using free text fields is time consuming. Providers therefore had low use of, and confidence in, the accuracy of psychosocial information in the EHR. PRINCIPAL CONCLUSIONS: Workflows and EHR tools should be re-designed to better support psychosocial information collection and retrieval. Tools should enable recording and summarization of the patient's story, and the rationale for treatment decisions.",Diabetes | Electronic health records | Health informatics | Outpatient care | Social determinants
"Electronic health records are increasingly used for research. The definition of cases or endpoints often relies on the use of coded diagnostic data, using a pre-selected group of codes. Validation of these cases, as 'true' cases of the disease, is crucial. There are, however, ambiguities in what is meant by validation in the context of electronic records. Validation usually implies comparison of a definition against a gold standard of diagnosis and the ability to identify false negatives ('true' cases which were not detected) as well as false positives (detected cases which did not have the condition). We argue that two separate concepts of validation are often conflated in existing studies. Firstly, whether the GP thought the patient was suffering from a particular condition (which we term confirmation or internal validation) and secondly, whether the patient really had the condition (external validation). Few studies have the ability to detect false negatives who have not received a diagnostic code. Natural language processing is likely to open up the use of free text within the electronic record which will facilitate both the validation of the coded diagnosis and searching for false negatives.",
"Meaningful use is a multidimensional concept that incorporates complex processes; workflow; interoperability; decision support; performance evaluation; and quality improvement. Meaningful use is congruent with the overall vision for information management in New Zealand. Health practitioners interface with patient information at many levels, and are pivotal to meaningful use at the interface between service providers, patients, and the electronic health record. Advancing towards meaningful use depends on implementing a meaningful interface terminology within the electronic health record. The Omaha System is an interface terminology that is integrated within Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT(®)), and has the capacity to disseminate and capture information at the point of care because its codes are simple defined terms. Two community nursing and allied health providers who are considering using the Omaha System in clinical systems for gathering intervention and outcomes data within the personal EHR include Nurse Maude and the Royal New Zealand Plunket Society. Help4U is investigating using the Omaha System as a way to standardise health terminology for consumer use. The Omaha System is also a good fit with the Midwifery and Maternity Providers Organisation (MMPO) existing clinical information system to describe and capture data about interventions currently recorded as free text. As a country that promotes access to affordable primary care and free hospital care, within an environment constrained by resource limitations, maximizing the use of data is key to demonstrating health outcomes for the population.",Electronic health records and systems | clinical data management | clinical documentation and communication | evaluation | standards
"AIMS: The diagnosis of heart failure (HF) is an important problem in primary care. We previously demonstrated a 74% increase in registered HF diagnoses in primary care electronic health records (EHRs) following an extended audit procedure. What remains unclear is the accuracy of registered HF pre-audit and which EHR variables are most important in the extended audit strategy. This study aims to describe the diagnostic HF classification sequence at different stages, assess general practitioner (GP) HF misclassification, and test the predictive performance of an optimized audit. METHODS AND RESULTS: This is a secondary analysis of the OSCAR-HF study, a prospective observational trial including 51 participating GPs. OSCAR used an extended audit based on typical HF risk factors, signs, symptoms, and medications in GPs' EHR. This resulted in a list of possible HF patients, which participating GPs had to classify as HF or non-HF. We compared registered HF diagnoses before and after GPs' assessment. For our analysis of audit performance, we used GPs' assessment of HF as primary outcome and audit queries as dichotomous predictor variables for a gradient boosted machine (GBM) decision tree algorithm and logistic regression model. Of the 18 011 patients eligible for the audit intervention, 4678 (26.0%) were identified as possible HF patients and submitted for GPs' assessment in the audit stage. There were 310 patients with registered HF before GP assessment, of whom 146 (47.1%) were judged not to have HF by their GP (over-registration). There were 538 patients with registered HF after GP assessment, of whom 374 (69.5%) did not have registered HF before GP assessment (under-registration). The GBM and logistic regression model had a comparable predictive performance (area under the curve of 0.70 [95% confidence interval 0.65-0.77] and 0.69 [95% confidence interval 0.64-0.75], respectively). This was not significantly impacted by reducing the set of predictor variables to the 10 most important variables identified in the GBM model (free-text and coded cardiomyopathy, ischaemic heart disease and atrial fibrillation, digoxin, mineralocorticoid receptor antagonists, and combinations of renin-angiotensin system inhibitors and beta-blockers with diuretics). This optimized query set was enough to identify 86% (n = 461/538) of GPs' self-assessed HF population with a 33% reduction (n = 1537/4678) in screening caseload. CONCLUSIONS: Diagnostic coding of HF in primary care health records is inaccurate with a high degree of under-registration and over-registration. An optimized query set enabled identification of more than 80% of GPs' self-assessed HF population.",Audit and feedback | Chronic heart failure | Electronic health records | Primary care | Screening
"OBJECTIVE: This study focuses on the task of automatically assigning standardized (topical) subject headings to free-text sentences in clinical nursing notes. The underlying motivation is to support nurses when they document patient care by developing a computer system that can assist in incorporating suitable subject headings that reflect the documented topics. Central in this study is performance evaluation of several text classification methods to assess the feasibility of developing such a system. MATERIALS AND METHODS: Seven text classification methods are evaluated using a corpus of approximately 0.5 million nursing notes (5.5 million sentences) with 676 unique headings extracted from a Finnish university hospital. Several of these methods are based on artificial neural networks. Evaluation is first done in an automatic manner for all methods, then a manual error analysis is done on a sample. RESULTS: We find that a method based on a bidirectional long short-term memory network performs best with an average recall of 0.5435 when allowed to suggest 1 subject heading per sentence and 0.8954 when allowed to suggest 10 subject headings per sentence. However, other methods achieve comparable results. The manual analysis indicates that the predictions are better than what the automatic evaluation suggests. CONCLUSIONS: The results indicate that several of the tested methods perform well in suggesting the most appropriate subject headings on sentence level. Thus, we find it feasible to develop a text classification system that can support the use of standardized terminologies and save nurses time and effort on care documentation.",clinical decision support | electronic health records | natural language processing | nursing documentation | text classification
"INTRODUCTION: Drug safety research asks causal questions but relies on observational data. Confounding bias threatens the reliability of studies using such data. The successful control of confounding requires knowledge of variables called confounders affecting both the exposure and outcome of interest. However, causal knowledge of dynamic biological systems is complex and challenging. Fortunately, computable knowledge mined from the literature may hold clues about confounders. In this paper, we tested the hypothesis that incorporating literature-derived confounders can improve causal inference from observational data. METHODS: We introduce two methods (semantic vector-based and string-based confounder search) that query literature-derived information for confounder candidates to control, using SemMedDB, a database of computable knowledge mined from the biomedical literature. These methods search SemMedDB for confounders by applying semantic constraint search for indications treated by the drug (exposure) and that are also known to cause the adverse event (outcome). We then include the literature-derived confounder candidates in statistical and causal models derived from free-text clinical notes. For evaluation, we use a reference dataset widely used in drug safety containing labeled pairwise relationships between drugs and adverse events and attempt to rediscover these relationships from a corpus of 2.2 M NLP-processed free-text clinical notes. We employ standard adjustment and causal inference procedures to predict and estimate causal effects by informing the models with varying numbers of literature-derived confounders and instantiating the exposure, outcome, and confounder variables in the models with dichotomous EHR-derived data. Finally, we compare the results from applying these procedures with naive measures of association (χ(2) and reporting odds ratio) and with each other. RESULTS AND CONCLUSIONS: We found semantic vector-based search to be superior to string-based search at reducing confounding bias. However, the effect of including more rather than fewer literature-derived confounders was inconclusive. We recommend using targeted learning estimation methods that can address treatment-confounder feedback, where confounders also behave as intermediate variables, and engaging subject-matter experts to adjudicate the handling of problematic covariates.",Causal inference | Confounder selection | Confounding bias | Electronic health records | Pharmacovigilance
"BACKGROUND: Adverse sensitivity (e.g., allergy and intolerance) information is a critical component of any electronic health record system. While several standards exist for structured entry of adverse sensitivity information, many clinicians record this data as free text. OBJECTIVES: This study aimed to 1) identify and compare the existing common adverse sensitivity information models, and 2) to evaluate the coverage of the adverse sensitivity information models for representing allergy information on a subset of inpatient and outpatient adverse sensitivity clinical notes. METHODS: We compared four common adverse sensitivity information models: Health Level 7 Allergy and Intolerance Domain Analysis Model, HL7-DAM; the Fast Healthcare Interoperability Resources, FHIR; the Consolidated Continuity of Care Document, C-CDA; and OpenEHR, and evaluated their coverage on a corpus of inpatient and outpatient notes (n = 120). RESULTS: We found that allergy specialists' notes had the highest frequency of adverse sensitivity attributes per note, whereas emergency department notes had the fewest attributes. Overall, the models had many similarities in the central attributes which covered between 75% and 95% of adverse sensitivity information contained within the notes. However, representations of some attributes (especially the value-sets) were not well aligned between the models, which is likely to present an obstacle for achieving data interoperability. Also, adverse sensitivity exceptions were not well represented among the information models. CONCLUSIONS: Although we found that common adverse sensitivity models cover a significant portion of relevant information in the clinical notes, our results highlight areas needed to be reconciled between the standards for data interoperability.",Allergy | Health Level 7 | electronic health records | health information systems | hypersensitivity
"In an emergency department (ED) sample, we investigated the concordance between identification of suicide-related visits through standardized comprehensive chart review versus a subset of 3 specific chart elements: ICD-9-CM codes, free-text presenting complaints, and free-text physician discharge diagnoses. The method for this study was review of medical records for adults (≥18 years) at 8 EDs across the United States. A total of 3,776 charts were reviewed. A combination of the 3 chart elements (ICD-9-CM, presenting complaints, and discharge diagnoses) provided the most robust data with 85% sensitivity, 96% specificity, 92% PPV, and 92% NPV. These findings highlight the use of key discrete fields in the medical record that can be extracted to facilitate identification of whether an ED visit was suicide-related.",chart review | electronic health records | emergency department | suicide
"OBJECTIVES: Increased adoption of electronic health records has resulted in increased availability of free text clinical data for secondary use. A variety of approaches to obtain actionable information from unstructured free text data exist. These approaches are resource intensive, inherently complex and rely on structured clinical data and dictionary-based approaches. We sought to evaluate the potential to obtain actionable information from free text pathology reports using routinely available tools and approaches that do not depend on dictionary-based approaches. MATERIALS AND METHODS: We obtained pathology reports from a large health information exchange and evaluated the capacity to detect cancer cases from these reports using 3 non-dictionary feature selection approaches, 4 feature subset sizes, and 5 clinical decision models: simple logistic regression, naïve bayes, k-nearest neighbor, random forest, and J48 decision tree. The performance of each decision model was evaluated using sensitivity, specificity, accuracy, positive predictive value, and area under the receiver operating characteristics (ROC) curve. RESULTS: Decision models parameterized using automated, informed, and manual feature selection approaches yielded similar results. Furthermore, non-dictionary classification approaches identified cancer cases present in free text reports with evaluation measures approaching and exceeding 80-90% for most metrics. CONCLUSION: Our methods are feasible and practical approaches for extracting substantial information value from free text medical data, and the results suggest that these methods can perform on par, if not better, than existing dictionary-based approaches. Given that public health agencies are often under-resourced and lack the technical capacity for more complex methodologies, these results represent potentially significant value to the public health field.",Cancer | Data preprocessing | Decision models | Feature selection | Pathology | Public health reporting
"INTRODUCTION: Although electronic health records have been facilitating the management of medical information, there is still room for improvement in daily production of medical report. Possible areas for improvement would be: to improve reports quality (by increasing exhaustivity), to improve patients' understanding (by mean of a graphical display), to save physicians' time (by helping reports writing), and to improve sharing and storage (by enhancing interoperability). We set up the ICIPEMIR project (Improving the completeness, interoperability and patients explanation of medical imaging reports) as an academic solution to optimize medical imaging reports production. Such a project requires two layers: one engineering layer to build the automation process, and a second medical layer to determine domain-specific data models for each type of report. We describe here the medical layer of this project. METHODS: We designed a reproducible methodology to identify -for a given medical imaging exam- mandatory fields, and describe a corresponding simple data model using validated formats. The mandatory fields had to meet legal requirements, domain-specific guidelines, and results of a bibliographic review on clinical studies. An UML representation, a JSON Schema, and a YAML instance dataset were defined. Based on this data model a form was created using Goupile, an open source eCRF script-based editor. In addition, a graphical display was designed and mapped with the data model, as well as a text template to automatically produce a free-text report. Finally, the YAML instance was encoded in a QR-Code to allow offline paper-based transmission of structured data. RESULTS: We tested this methodology in a specific domain: computed tomography for urolithiasis. We successfully extracted 73 fields, and transformed them into a simple data model, with mapping to a simple graphical display, and textual report template. The offline QR-code transmission of a 2,615 characters YAML file was successful with simple smartphone QR-Code scanner. CONCLUSION: Although automated production of medical report requires domain-specific data model and mapping, these can be defined using a reproducible methodology. Hopefully this proof of concept will lead to a computer solution to optimize medical imaging reports, driven by academic research.",Data model | QR-Code | medical imaging report | patient participation
"OBJECTIVES: Multidisciplinary PICU teams must effectively share information while caring for critically ill children. Clinical documentation helps clinicians develop a shared understanding of the patient's diagnosis, which informs decision-making. However, diagnosis-related documentation in the PICU is understudied, thus limiting insights into how pediatric intensivists convey their diagnostic reasoning. Our objective was to describe how pediatric critical care clinicians document patients' diagnoses at PICU admission. DESIGN: Retrospective mixed methods study describing diagnosis documentation in electronic health records. SETTING: Academic tertiary referral PICU. PATIENTS: Children 0-17 years old admitted nonelectively to a single PICU over 1 year. INTERVENTIONS: None. MEASUREMENTS AND MAIN RESULTS: One hundred PICU admission notes for 96 unique patients were reviewed. In 87% of notes, both attending physicians and residents or advanced practice providers documented a primary diagnosis; in 13%, primary diagnoses were documented by residents or advanced practice providers alone. Most diagnoses (72%) were written as narrative free text, 11% were documented as problem lists/billing codes, and 17% used both formats. At least one rationale was documented to justify the primary diagnosis in 91% of notes. Diagnostic uncertainty was present in 52% of notes, most commonly suggested by clinicians' use of words indicating uncertainty (65%) and documentation of differential diagnoses (60%). Clinicians' integration and interpretation of information varied in terms of: 1) organization of diagnosis narratives, 2) use of contextual details to clarify the diagnosis, and 3) expression of diagnostic uncertainty. CONCLUSIONS: In this descriptive study, most PICU admission notes documented a rationale for the primary diagnosis and expressed diagnostic uncertainty. Clinicians varied widely in how they organized diagnostic information, used contextual details to clarify the diagnosis, and expressed uncertainty. Future work is needed to determine how diagnosis narratives affect clinical decision-making, patient care, and outcomes.",
"BACKGROUND AND OBJECTIVE: The prevalence of value-based payment models has led to an increased use of the electronic health record to capture quality measures, necessitating additional documentation requirements for providers. METHODS: This case study uses text mining and natural language processing techniques to identify the timely completion of diabetic eye exams (DEEs) from 26,203 unique clinician notes for reporting as an electronic clinical quality measure (eCQM). Logistic regression and support vector machine (SVM) using unbalanced and balanced datasets, using the synthetic minority over-sampling technique (SMOTE) algorithm, were evaluated on precision, recall, sensitivity, and f1-score for classifying records positive for DEE. We then integrate a high precision DEE model to evaluate free-text clinical narratives from our clinical EHR system. RESULTS: Logistic regression and SVM models had comparable f1-score and specificity metrics with models trained and validated with no oversampling favoring precision over recall. SVM with and without oversampling resulted in the best precision, 0.96, and recall, 0.85, respectively. These two SVM models were applied to the unannotated 31,585 text segments representing 24,823 unique records and 13,714 unique patients. The number of records classified as positive for DEE using the SVM models ranged from 667 to 8,935 (2.7-36% out of 24,823, respectively). Unique patients classified as positive for DEE ranged from 3.5 to 41.8% highlighting the potential utility of these models. DISCUSSION: We believe the impact of oversampling on SVM model performance to be caused by the potential of overfitting of the SVM SMOTE model on the synthesized data and the data synthesis process. However, the specificities of SVM with and without SMOTE were comparable, suggesting both models were confident in their negative predictions. By prioritizing to implement the SVM model with higher precision over sensitivity or recall in the categorization of DEEs, we can provide a highly reliable pool of results that can be documented through automation, reducing the burden of secondary review. Although the focus of this work was on completed DEEs, this method could be applied to completing other necessary documentation by extracting information from natural language in clinician notes. CONCLUSION: By enabling the capture of data for eCQMs from documentation generated by usual clinical practice, this work represents a case study in how such techniques can be leveraged to drive quality without increasing clinician work.",
"BACKGROUND: Electronic health records, increasingly a part of healthcare, provide a wealth of untapped narrative free text data that have the potential to accurately inform clinical outcomes. METHODS: From a validated cohort of patients with Crohn's disease or ulcerative colitis, we identified patients with ≥1 coded or narrative mention of monoclonal antibodies to tumor necrosis factor α. Chart review by ascertained true use of therapy, time of initiation, and cessation of treatment, and also clinical response stratified as nonresponse, partial, or complete response at 1 year. Internal consistency was assessed in an independent validation cohort. RESULTS: A total of 3087 patients had a mention of an antibodies to tumor necrosis factor α. Actual therapy initiation was within 60 days of the first coded mention in 74% of patients. In the derivation cohort, 18% of antibodies to tumor necrosis factor α starts were classified as nonresponse at 1 year, 21% as partial, and 56% as complete response. On multivariate analysis, the number of narrative mentions of diarrhea (odds ratio 1.08; 95% confidence interval, 1.02-1.14) and fatigue (odds ratio 1.16; 95% confidence interval, 1.02-1.32) was independently associated with nonresponse at 1 year (area under the curve 0.82). A likelihood of nonresponse score comprising a weighted sum of both demonstrated a good dose-response relationship across nonresponders (2.18), partial (1.20), and complete (0.50) responders (P < 0.0001) and correlated well with need for surgery or hospitalizations. CONCLUSIONS: Narrative data in an electronic health record offer considerable potential to define temporally evolving disease outcomes such as nonresponse to treatment.",
"Leveraging the Electronic Health Records (EHR) longitudinal data to produce actionable clinical insights has always been a critical issue for recent studies. Non-forecasted extended hospitalizations account for a disproportionate amount of resource use, the mediocre quality of inpatient care, and avoidable fatalities. The capability to predict the Length of Stay (LoS) and mortality in the early stages of the admission provides opportunities to improve care and prevent many preventable losses. Forecasting the in-hospital mortality is important in providing clinicians with enough insights to make decisions and hospitals to allocate resources, hence predicting the LoS and mortality within the first day of admission is a difficult but a paramount endeavor. The biggest challenge is that few data are available by this time, thus the prediction has to bring in the previous admissions history and free text diagnosis that are recorded immediately on admission. We propose a model that uses the multi-modal EHR structured medical codes and key demographic information to classify the LoS in 3 classes; Short Los (LoS⩽10 days), Medium LoS (10<LoS⩽30 days) and Long LoS (LoS>30 days) as well as mortality as a binary classification of a patient's death during current admission. The prediction has to use data available only within 24 h of admission. The key predictors include previous ICD9 diagnosis codes, ICD9 procedures, key demographic data, and free text diagnosis of the current admission recorded right on admission. We propose a Hierarchical Attention Network (HAN-LoS and HAN-Mor) model and train it to a dataset of over 45321 admissions recorded in the de-identified MIMIC-III dataset. For improved prediction, our attention mechanisms can focus on the most influential past admissions and most influential codes in these admissions. For fair performance evaluation, we implemented and compared the HAN model with previous approaches. With dataset balancing techniques HAN-LoS achieved an AUROC of over 0.82 and a Micro-F1 score of 0.24 and HAN-Mor achieved AUC-ROC of 0.87 hence outperforming the existing baselines that use structured medical codes as well as clinical time series for LoS and Mortality forecasting. By predicting mortality and LoS using the same model, we show that with little tuning the proposed model can be used for other clinical predictive tasks like phenotyping, decompensation,re-admission prediction, and survival analysis.",Boosting | Class imbalance | Electronic health record | Hierarchical attention network | Length of stay
"INTRODUCTION: The Clinical Care Classification (CCC) system is one of the standard nursing terminologies recognized by the American Nurses Association, developed to describe nursing care through electronic documentation in different healthcare settings. The translation of the CCC system into languages other than English is useful to promote its widespread use in different countries and to provide the standard nursing data necessary for interoperable health information exchange. The aim of this study was to translate the CCC system from English to Italian and to test its clinical validity. METHODS: A translation with cross-cultural adaptation was performed in four phases: forward-translation, back-translation, review, and dissemination. Subsequently a pilot cross-mapping study between nursing activities in free-text nursing documentation and the CCC interventions was conducted. RESULTS: All elements of the CCC system were translated into Italian. Semantic and conceptual equivalences were achieved. Altogether 77.8% of the nursing activities were mapped into CCC interventions. CONCLUSIONS: The CCC system, and its integration into electronic health records, has the potential to support Italian nurses in describing and providing outcomes and costs of their care in different healthcare settings. Future studies are needed to strengthen the impact of the CCC system on clinical practice.",Clinical Care Classification system | Cross-cultural comparison | Standardized Nursing Terminology | Translation
"BACKGROUND: Free text is helpful for entering information into electronic health records, but reusing it is a challenge. The need for language technology for processing Finnish and Swedish healthcare text is therefore evident; however, Finnish and Swedish are linguistically very dissimilar. In this paper we present a comparison of characteristics in Finnish and Swedish free-text nursing narratives from intensive care. This creates a framework for characterising and comparing clinical text and lays the groundwork for developing clinical language technologies. METHODS: Our material included daily nursing narratives from one intensive care unit in Finland and one in Sweden. Inclusion criteria for patients were an inpatient period of least five days and an age of at least 16 years. We performed a comparative analysis as part of a collaborative effort between Finnish- and Swedish-speaking healthcare and language technology professionals that included both qualitative and quantitative aspects. The qualitative analysis addressed the content and structure of three average-sized health records from each country. In the quantitative analysis 514 Finnish and 379 Swedish health records were studied using various language technology tools. RESULTS: Although the two languages are not closely related, nursing narratives in Finland and Sweden had many properties in common. Both made use of specialised jargon and their content was very similar. However, many of these characteristics were challenging regarding development of language technology to support producing and using clinical documentation. CONCLUSIONS: The way Finnish and Swedish intensive care nursing was documented, was not country or language dependent, but shared a common context, principles and structural features and even similar vocabulary elements. Technology solutions are therefore likely to be applicable to a wider range of natural languages, but they need linguistic tailoring. AVAILABILITY: The Finnish and Swedish data can be found at: http://www.dsv.su.se/hexanord/data/.",
"Clinical codes are used for public reporting purposes, are fundamental to determining public financing for hospitals, and form the basis for reimbursement claims to insurance providers. They are assigned to a patient stay to reflect the diagnosis and performed procedures during that stay. This paper aims to enrich algorithms for automated clinical coding by taking a data-driven approach and by using unsupervised and semi-supervised techniques for the extraction of multi-word expressions that convey a generalisable medical meaning (referred to as concepts). Several methods for extracting concepts from text are compared, two of which are constructed from a large unannotated corpus of clinical free text. A distributional semantic model (i.c. the word2vec skip-gram model) is used to generalize over concepts and retrieve relations between them. These methods are validated on three sets of patient stay data, in the disease areas of urology, cardiology, and gastroenterology. The datasets are in Dutch, which introduces a limitation on available concept definitions from expert-based ontologies (e.g. UMLS). The results show that when expert-based knowledge in ontologies is unavailable, concepts derived from raw clinical texts are a reliable alternative. Both concepts derived from raw clinical texts perform and concepts derived from expert-created dictionaries outperform a bag-of-words approach in clinical code assignment. Adding features based on tokens that appear in a semantically similar context has a positive influence for predicting diagnostic codes. Furthermore, the experiments indicate that a distributional semantics model can find relations between semantically related concepts in texts but also introduces erroneous and redundant relations, which can undermine clinical coding performance.",Clinical coding | Data mining | Distributional semantics | Electronic health records | International classification of diseases | Text mining | Unsupervised learning | Word2vec
"Clinical data are dynamic in nature, often arranged hierarchically and stored as free text and numbers. Effective management of clinical data and the transformation of the data into structured format for data analysis are therefore challenging issues in electronic health records development. Despite the popularity of relational databases, the scalability of the NoSQL database model and the document-centric data structure of XML databases appear to be promising features for effective clinical data management. In this paper, three database approaches--NoSQL, XML-enabled and native XML--are investigated to evaluate their suitability for structured clinical data. The database query performance is reported, together with our experience in the databases development. The results show that NoSQL database is the best choice for query speed, whereas XML databases are advantageous in terms of scalability, flexibility and extensibility, which are essential to cope with the characteristics of clinical data. While NoSQL and XML technologies are relatively new compared to the conventional relational database, both of them demonstrate potential to become a key database technology for clinical data management as the technology further advances.",
"OBJECTIVE: to investigate the effect of potentially inappropriate medications (PIMs) on inpatient falls and to identify whether PIMs as defined by STOPPFall or the designated section K for falls of STOPP v2 have a stronger association with inpatient falls when compared to the general tool STOPP v2. METHODS: a retrospective observational matching study using an electronic health records dataset of patients (≥70 years) admitted to an academic hospital (2015-19), including free text to identify inpatient falls. PIMs were identified using the STOPP v2, section K of STOPP v2 and STOPPFall. We first matched admissions with PIMs to those without PIMs on confounding factors. We then applied multinomial logistic regression analysis and Cox proportional hazards analysis on the matched datasets to identify effects of PIMs on inpatient falls. RESULTS: the dataset included 16,678 hospital admissions, with a mean age of 77.2 years. Inpatient falls occurred during 446 (2.7%) admissions. Adjusted odds ratio (OR) (95% confidence interval (CI)) for the association between PIM exposure and falls were 7.9 (6.1-10.3) for STOPP section K, 2.2 (2.0-2.5) for STOPP and 1.4 (1.3-1.5) for STOPPFall. Adjusted hazard ratio (HR) (95% CI) for the effect on time to first fall were 2.8 (2.3-3.5) for STOPP section K, 1.5 (1.3-1.6) for STOPP and 1.3 (1.2-1.5) for STOPPFall. CONCLUSIONS: we identified an independent association of PIMs on inpatient falls for all applied (de)prescribing tools. The strongest effect was identified for STOPP section K, which is restricted to high-risk medication for falls. Our results suggest that decreasing PIM exposure during hospital stay might benefit fall prevention, but intervention studies are warranted.",accidental falls | hospital | inappropriate prescriptions | older people
"Predicting the risk of mortality for patients with acute myocardial infarction (AMI) using electronic health records (EHRs) data can help identify risky patients who might need more tailored care. In our previous work, we built computational models to predict one-year mortality of patients admitted to an intensive care unit (ICU) with AMI or post myocardial infarction syndrome. Our prior work only used the structured clinical data from MIMIC-III, a publicly available ICU clinical database. In this study, we enhanced our work by adding the word embedding features from free-text discharge summaries. Using a richer set of features resulted in significant improvement in the performance of our deep learning models. The average accuracy of our deep learning models was 92.89% and the average F-measure was 0.928. We further reported the impact of different combinations of features extracted from structured and/or unstructured data on the performance of the deep learning models.",Deep Learning | Electronic Health Records | Machine Learning
"INTRODUCTION AND OBJECTIVES: The accuracy of conclusions from research based on Electronic Healthcare Records (EHRs) is highly dependent on the correct selection of descriptors (codes) by users, but few methods exist for examining quality and drivers of documentation. We aimed to evaluate the feasibility and acceptability of filmed vignette monologues as a resource-light method of assessing and comparing how different EHR users record the same clinical scenario. METHODS: Six short monologues portraying simulated patients presenting allergic conditions to their General Practitioners were filmed head-on then electronically distributed for the study; no researcher was present during data collection. The method was assessed by participant uptake, reported ease of completion by participants, compliance with instructions, the receipt of interpretable data by researchers, and participant perceptions of vignette quality, realism and information content. RESULTS: Twenty-two participants completed the study, reporting only minor difficulties. 132 screenshots were returned electronically, enabling analysis of codes, free text and EHR features. Participants assigned a quality rating of 7.7/10 (range 2-10) to the vignettes and rated the extent to which vignettes reflected real-life at 93% (range 86-100%). Between 1 and 2 hours were required to complete the task. Full compliance with instructions varied between participants, but was largely successful. CONCLUSIONS: Filmed monologues are a reproducible, standardized method, which require relatively few resources, yet allow clear assessment of clinicians' and EHRs systems' impact on documentation. The novel nature of this method necessitates clear instructions, so participants can fully complete the study without face-to-face researcher supervision.",
"INTRODUCTION: National initiatives to develop quality metrics emphasize the need to include patient-centered outcomes. Patient-centered outcomes are complex, require documentation of patient communications, and have not been routinely collected by healthcare providers. The widespread implementation of electronic medical records (EHR) offers opportunities to assess patient-centered outcomes within the routine healthcare delivery system. The objective of this study was to test the feasibility and accuracy of identifying patient centered outcomes within the EHR. METHODS: Data from patients with localized prostate cancer undergoing prostatectomy were used to develop and test algorithms to accurately identify patient-centered outcomes in post-operative EHRs - we used urinary incontinence as the use case. Standard data mining techniques were used to extract and annotate free text and structured data to assess urinary incontinence recorded within the EHRs. RESULTS: A total 5,349 prostate cancer patients were identified in our EHR-system between 1998-2013. Among these EHRs, 30.3% had a text mention of urinary incontinence within 90 days post-operative compared to less than 1.0% with a structured data field for urinary incontinence (i.e. ICD-9 code). Our workflow had good precision and recall for urinary incontinence (positive predictive value: 0.73 and sensitivity: 0.84). DISCUSSION: Our data indicate that important patient-centered outcomes, such as urinary incontinence, are being captured in EHRs as free text and highlight the long-standing importance of accurate clinician documentation. Standard data mining algorithms can accurately and efficiently identify these outcomes in existing EHRs; the complete assessment of these outcomes is essential to move practice into the patient-centered realm of healthcare.",Electronic health records | data mining | health services research | patient-centered care | quality improvement
"Electronic physician documentation is an essential element of a complete electronic medical record (EMR). At Lucile Packard Children's Hospital, a teaching hospital affiliated with Stanford University, we implemented an inpatient electronic documentation system for physicians over a 12-month period. Using an EMR-based free-text editor coupled with automated import of system data elements, we were able to achieve voluntary, widespread adoption of the electronic documentation process. When given the choice between electronic versus dictated report creation, the vast majority of users preferred the electronic method. In addition to increasing the legibility and accessibility of clinical notes, we also decreased the volume of dictated notes and scanning of handwritten notes, which provides the opportunity for cost savings to the institution.",Electronic health records | documentation | information storage and retrieval | physician’s practice patterns | software design | time factors | user-computer interface
"The aim of this study was to determine whether an expert system based on automated processing of electronic health records (EHRs) could provide a more accurate estimate of the annual rate of emergency department (ED) visits for suicide attempts in France, as compared to the current national surveillance system based on manual coding by emergency practitioners. A feasibility study was conducted at Lyon University Hospital, using data for all ED patient visits in 2012. After automatic data extraction and pre-processing, including automatic coding of medical free-text through use of the Unified Medical Language System, seven different machine-learning methods were used to classify the reasons for ED visits into ""suicide attempts"" versus ""other reasons"". The performance of these different methods was compared by using the F-measure. In a test sample of 444 patients admitted to the ED in 2012 (98 suicide attempts, 48 cases of suicidal ideation, and 292 controls with no recorded non-fatal suicidal behaviour), the F-measure for automatic detection of suicide attempts ranged from 70.4% to 95.3%. The random forest and naïve Bayes methods performed best. This study demonstrates that machine-learning methods can improve the quality of epidemiological indicators as compared to current national surveillance of suicide attempts.",attempted suicide | emergency service | machine learning | natural language processing | population surveillance
"BACKGROUND: Clinical trials are the gold standard for generating robust medical evidence, but clinical trial results often raise generalizability concerns, which can be attributed to the lack of population representativeness. The electronic health records (EHRs) data are useful for estimating the population representativeness of clinical trial study population. OBJECTIVES: This research aims to estimate the population representativeness of clinical trials systematically using EHR data during the early design stage. METHODS: We present an end-to-end analytical framework for transforming free-text clinical trial eligibility criteria into executable database queries conformant with the Observational Medical Outcomes Partnership Common Data Model and for systematically quantifying the population representativeness for each clinical trial. RESULTS: We calculated the population representativeness of 782 novel coronavirus disease 2019 (COVID-19) trials and 3,827 type 2 diabetes mellitus (T2DM) trials in the United States respectively using this framework. With the use of overly restrictive eligibility criteria, 85.7% of the COVID-19 trials and 30.1% of T2DM trials had poor population representativeness. CONCLUSION: This research demonstrates the potential of using the EHR data to assess the clinical trials population representativeness, providing data-driven metrics to inform the selection and optimization of eligibility criteria.",
"The year is 2006, and there are just a few hundred medical transcriptionists (MTs) still transcribing reports, serving only those older physicians who haven't changed with the times--and the times have definitely changed. Physicians have finally recognized the power of electronic health records (EHRs) as well as the fact that this power is realized only if they input clinical data directly into the EHR. The vast majority of physicians are using empirically refined templates, pick lists, and other methods of structured, codified input through the evolved progeny of today's Palm PCs, Pocket PCs, and Tablet PCs. Input methods include touch-screen, speech recognition, handwriting recognition, and perhaps other technology not yet invented. There are no longer any delays or expenses resulting from transcription. Plus healthcare organizations enjoy numerous benefits derived from analyzing codified clinical data. But this is only one vision. Another vision of 2006 incorporates an unavoidable reality: many physicians strongly resist directly inputting clinical data. They believe it slows them down, which outweighs the potentials overall healthcare benefits. Additionally, these physicians believe that structured input of patient information limits the freedom of expression afforded by free text. And frankly, these physicians don't put much stock in the value of clinical practive analysis. So transcription continues. In fact, it expands dramatically. Due to regulatory controls and other pressures, more providers dictate more clinical notes than ever. The need for MTs explodes. In 2006, there are half a million MTs required to convert voice dictations into text, more than double today's number.",
"OBJECTIVES: The objective of the present study was to determine the use of systemic corti-costeroids (SCs) in patients with bronchial asthma using big data analysis. METHODS: We performed an observational, retrospective, noninterventional study based on secondary data captured from free text in the electronic health records. This study was per-formed based on data from the regional health service of Castille-La Mancha (SESCAM), Spain. We performed the analysis using big data and artificial intelligence via Savana® Manager version 3.0. RESULTS: During the study period, 103 667 patients were diagnosed with and treated for asthma at different care levels. The search was restricted to patients aged 10 to 90 years (mean age, 43.5 [95%CI, 43.4-43.7] years). Of these, 59.8% were women. SCs were taken for treatment of asthma by 58 745 patients at some point during the study period. These patients were older, with a higher prevalence of hypertension, dyslipidemia, diabetes, ob-esity, depression, and hiatus hernia. SCs are used frequently in the general population with asthma (31.4% in 2015 and 39.6% in 2019). SCs were prescribed mainly in primary care (59%), allergy (13%) and pulmonology (20%). The frequency of prescription of SCs had a direct impact on the main associated adverse effects. CONCLUSION: In clinical practice, SCs are frequently prescribed to patients with asthma, especially in primary care. Use of SCs is associated with a greater number of adverse events. It is necessary to implement measures to reduce prescription of SCs to patients with asthma, especially in primary care.",Artificial intelligence | Asthma | Big data | Systemic corticosteroids
"OBJECTIVE: To use data from electronic health records (EHRs) to describe the demographic, clinical and functional correlates of childhood sexual abuse (CSA) in patients with severe mental illness (SMI), and compare their clinical outcomes (admissions and receipt of antipsychotic medications) to those of patients with no recorded history of CSA. METHODS: We applied a string-matching technique to clinical text records of 7000 patients with SMI (non-organic psychotic disorders or bipolar disorder), identifying 619 (8.8%) patients with a recorded history of CSA. Data were extracted from both free-text and structured fields of patients' EHRs. RESULTS: Comorbid diagnoses of major depressive disorder, post-traumatic stress disorder and personality disorders were more prevalent in patients with CSA. Positive psychotic symptoms, depressed mood, self-harm, substance use and aggression were also more prevalent in this group, as were problems with relationships and living conditions. The odds of inpatient admissions were higher in patients with CSA than in those without (adjusted OR = 1.95, 95% CI: 1.64-2.33), and they were more likely to have spent more than 10 days per year as inpatients (adjusted OR = 1.32, 95% CI: 1.07-1.62). Patients with CSA were more likely to be prescribed antipsychotic medications (adjusted OR = 2.48, 95% CI: 1.69-3.66) and be given over 75% of the maximum recommended daily dose (adjusted OR = 1.72, 95% CI: 1.44-2.04). CONCLUSION: Data-driven approaches are a reliable, promising avenue for research on childhood trauma. Clinicians should be trained and skilled at identifying childhood adversity in patients with SMI, and addressing it as part of the care plan.",childhood trauma | psychotic disorders | sexual abuse
"The objective of the work is to implement and evaluate the automated computation of 9 healthcare quality indicators, by data reuse of electronic health records, in the field of elderly surgical patients. METHODS: Data are extracted from EHR, including administrative data, ICD10 diagnoses, laboratory results, procedures, administered drugs, and free-text letters. The indicators are implemented by a medical data reuse specialist. The conformity rate is automatically computed (3.5 minutes for 15,000 inpatient stays and 9 indicators). A medical expert reviews 45 stays per indicator. The precision is the proportion of non-conform inpatient stays among the cases detected as non-conform by the algorithms. RESULTS: the paper describes the implemented algorithms, the conformity rates and the precisions. Two indicators have a precision of 0%, 3 indicators have a precision of 40 to 60%, and four indicators have a precision from 80 to 100% (for 2 of them, the conformity rate is lower than 2.5%!). This demonstrates that automated quality screening is possible and enables to detect threatening situations. The implementation of the indicators requires special skills in medicine, medical information sciences, and algorithmics. Failures of precision are mainly due to defaults of information quality (missing codes), and could benefit from text analysis.",
"The overall objective of the EU-ADR project is the design, development, and validation of a computerised system that exploits data from electronic health records and biomedical databases for the early detection of adverse drug reactions. Eight different databases, containing health records of more than 30 million European citizens, are involved in the project. Unique queries cannot be performed across different databases because of their heterogeneity: Medical record and Claims databases, four different terminologies for coding diagnoses, and two languages for the information described in free text. The aim of our study was to provide database owners with a common basis for the construction of their queries. Using the UMLS, we provided a list of medical concepts, with their corresponding terms and codes in the four terminologies, which should be considered to retrieve the relevant information for the events of interest from the databases.",
We report on the pilot evaluation of an experimental query-based search functionality that enables phrase-level query rewriting in an unsupervised way. It is intended for supporting search in clinical text. Qualitative evaluation is done by three clinicans using a prototype search tool. They report that they find the tested search functionality to be beneficial for making query-based searching in clinical text more efficient.,Electronic Health Records | Information Storage and Retrieval | Natural Language Processing
"BACKGROUND: After implementation of a system-wide EMR within our university system, e-prescribing is now commonplace. OBJECTIVE: The authors conducted a study to assess whether optimization of computerized provider order entry (CPOE) can reduce errors in these electronically transmitted prescriptions and would require less frequent interventions from pharmacists, in particular the need for them to ""call to clarify"" (CTC) details of particular prescriptions. Secondary analysis based on cost assumptions was preformed to presume cost differences before and after optimization changes. MATERIALS AND METHODS: In order to generate complete, error-free prescriptions, optimization changes were implemented in the form of in line validation messages. These messages were generated if (1) an order did not specify a provider or pharmacy; (2) the DEA requirements were not met; (3) character limits were exceeded in patient sig or demographics or (4) administration instructions had breaks or had both discrete and free text elements. Retrospectively, prescriptions were randomly selected from a nine month period before and after implementing changes. These prescriptions were analyzed by a pharmacist and a nurse to identify types of errors that would require a CTC to a prescribing provider. Errors were compared statistically to determine effectiveness of changes pre and post optimization. RESULTS: A total of 602 prescriptions were analyzed; 301 before changes and 301 after changes. Of these prescriptions, 20.27% had errors before changes and 12.96% had errors after changes. The decline in the error rate was considered statistically significant for p<0.05. The cost savings were estimated at $76 per 100 prescriptions for pharmacist and physician time-cost estimates combined. CONCLUSIONS: Implementing optimization changes to the CPOE resulted in a reduction in error rate requiring pharmacist CTC. This study identifies effective optimization changes for electronic prescribing that can reduce prescribing errors and may result in cost saving.",CPOE | Electronic prescribing | cost | electronic health records | medication errors
"PURPOSE: This study aims to validate major bleeding (MB) cases within a cohort of new users of direct oral anticoagulants (DOACs) in Electronic health records (EHRs) from primary care in Spain (BIFAP), introducing more efficient techniques and automating the process of validation in the pharmacoepidemiologic research with EHR data as much as possible. METHODS: Registered bleedings were identified in a cohort of new users of DOACs in BIFAP using ICPC 2 and ICD 9 codes; we ascertained these bleedings as MB through a validation strategy based on the MB definition from the International Society on Thrombosis and Hemostasis, which used hospitalization and critical localization as proxies. We assessed hospitalization with hospital discharge information (only available for some years and regions) and a free text-based algorithm created to identify hospitalization in EHR's clinical notes. Incidence rates (IR) of MB were evaluated by bleeding type. RESULTS: The study cohort included 104 614 patients, with 274521.5 p-y of follow up. There were 6143 registered bleedings during the study period (519 intracranial bleeding - ICB, 4606 gastrointestinal bleeding - GIB, 1018 extracranial bleeding - ECB), from which 1679 were confirmed as MB (416 ICB, 1086 GIB, and 177 ECB). The free text-based semi-automatic algorithm had moderate recall (0.59), but high specificity (0.99), and precision (0.94). CONCLUSION: The combination of hospitalization and critical localization is a valid approach to validate MB in EHRs with incomplete information. The use of more automatic methods for case validation instead of manual review of clinical notes is favored.",electronic health records | major bleeding | pharmacoepidemiology | validation studies
"OBJECTIVE: Early Intervention (EI) referral is a key connector between health care and early childhood systems serving children with developmental risks. This study aimed to describe the US network of EI referrals by answering the following: ""What information is sent to EI?"", ""Who sends it?"", and ""How is it sent?"" METHOD: This study combined an analysis of national document-based and website-based referral forms with a survey of state Part C Coordinators (PCCs). Data on referral forms were systematically collected from state agency websites. PCCs from 52 jurisdictions were surveyed to assess current EI referral practices. Descriptive statistics were used for responses to multiple-choice items; free-text answers were condensed into key study themes. RESULTS: EI referral forms came as e-documents (81%) or websites (35%), and 72% were in English alone. They emphasized family and referral source contact information and reason for the referral. The survey results indicated that health care (45%) sends the most referrals, followed by families (30%). EI agencies received referrals by phone (38%), electronically (23%), e-mail (17%), and fax (17%), and PCCs valued this diversity of methods. Few states received referral data directly from electronic health records (EHRs); however, PCCs hope to eventually receive referrals through websites, mobile devices, and EHRs. CONCLUSION: EI referral data flow is complex, with opportunities for loss of children to follow-up. This study describes how EI referrals occur and provides examples of how communication and access to information may be improved.",
"PURPOSE: HIV research among transgender and gender nonbinary (TGNB) people is limited by lack of gender identity data collection. We designed an EHR-based algorithm to identify TGNB people among people living with HIV (PLWH) when gender identity was not systematically collected. METHODS: We applied EHR-based search criteria to all PLWH receiving care at a large urban health system between 1997 and 2017, then confirmed gender identity by chart review. We compared patient characteristics by gender identity and screening criteria, then calculated positive predictive values for each criterion. RESULTS: Among 18,086 PLWH, 213 (1.2%) met criteria as potential TGNB patients and 178/213 were confirmed. Positive predictive values were highest for free-text keywords (91.7%) and diagnosis codes (77.4%). Confirmed TGNB patients were younger (median 32.5 vs. 42.5 years, P < .001) and less likely to be Hispanic (37.1% vs. 62.9%, P = .03) than unconfirmed patients. Among confirmed patients, 15% met criteria only for prospective gender identity data collection and were significantly older. CONCLUSION: EHR-based criteria can identify TGNB PLWH, but success may differ by ethnicity and age. Retrospective versus intentional, prospective gender identity data collection may capture different patients. To reduce misclassification in epidemiologic studies, gender identity data collection should address these potential differences and be systematic and prospective.",Algorithms | Electronic health records | HIV | Transgender persons
"BACKGROUND: Eligibility criteria (EC) of clinical trials play a key role in selecting appropriate study candidates and the validity of the outcome of a clinical trial. However, in most cases EC are provided in unstandardised ways such as free text, which raises significant challenges for machine-readability. OBJECTIVES: To establish a list of most frequent medical concepts in clinical trials with semantic annotations. This concept list contributes to standardisation of EC and identifies relevant data items in electronic health records (EHRs) for clinical research. The coverage of the list in two major clinical vocabularies, MeSH and SNOMED-CT, will be assessed. METHODS: Four hundred and twenty-five clinical trials conducted between 2000 and 2011 at a German university hospital were analysed. 6671 EC were manually annotated by a medical coder using Concept Unique Identifiers (CUIs) provided by the Unified Medical Language System. Two physicians performed a semi-automatic CUI code revision. Concept frequency was analysed and clusters of concepts were manually identified.A binomial significance test was applied to quantify coverage differences of the most frequent concepts in MeSH and SNOMED-CT. RESULTS: Based on manual medical coding of 425 clinical trials, 7588 concepts were identified, of which 5236 were distinct. A top 100 list containing 101 most frequent medical concepts was established. The concepts of this list cover 25 % of all concept occurrences in all analysed clinical trials. This list reveals six missing entries in SNOMED-CT, 12 in MeSH. The median of EC frequency per trial has increased throughout the trial years (2000 -2005: 8 EC/trial, 2011: 14 EC/trial). CONCLUSIONS: Relatively few concepts cover one quarter of concept occurrences that represent EC in recent studies. Therefore, these concepts can serve as candidate data elements for integration into EHRs to optimise patient recruitment in clinical research.",CUI | Eligibility criteria | MeSH | ODM | SNOMED-CT | UMLS | clinical trials | data items
"OBJECTIVE: To describe a framework for leveraging big data for research and quality improvement purposes and demonstrate implementation of the framework for design of the Department of Veterans Affairs (VA) Colonoscopy Collaborative. METHODS: We propose that research utilizing large-scale electronic health records (EHRs) can be approached in a 4 step framework: 1) Identify data sources required to answer research question; 2) Determine whether variables are available as structured or free-text data; 3) Utilize a rigorous approach to refine variables and assess data quality; 4) Create the analytic dataset and perform analyses. We describe implementation of the framework as part of the VA Colonoscopy Collaborative, which aims to leverage big data to 1) prospectively measure and report colonoscopy quality and 2) develop and validate a risk prediction model for colorectal cancer (CRC) and high-risk polyps. RESULTS: Examples of implementation of the 4 step framework are provided. To date, we have identified 2,337,171 Veterans who have undergone colonoscopy between 1999 and 2014. Median age was 62 years, and 4.6 percent (n = 106,860) were female. We estimated that 2.6 percent (n = 60,517) had CRC diagnosed at baseline. An additional 1 percent (n = 24,483) had a new ICD-9 code-based diagnosis of CRC on follow up. CONCLUSION: We hope our framework may contribute to the dialogue on best practices to ensure high quality epidemiologic and quality improvement work. As a result of implementation of the framework, the VA Colonoscopy Collaborative holds great promise for 1) quantifying and providing novel understandings of colonoscopy outcomes, and 2) building a robust approach for nationwide VA colonoscopy quality reporting.",big data | electronic health records | epidemiology | quality improvement | veterans
"Clinical laboratory results are stored in electronic health records (EHRs) as structured data coded with local or standard terms. However, laboratory tests that are performed at outside laboratories are often simply labeled ""outside test"" or something similar, with the actual test name in a free-text result or comment field. After being aggregated into clinical data repositories, these ambiguous labels impede the retrieval of specific test results. We present a general multi-step solution that can facilitate the identification, standardization, reconciliation, and transformation of such test results. We applied our approach to data in the NIH Biomedical Translational Research Information System (BTRIS) to identify laboratory tests, map comment values to the LOINC codes that will be incorporated into our Research Entities Dictionary (RED), and develop a reference table that can be used in the EHR data extract-transform-load (ETL) process.",
"OBJECTIVES: To develop an electronic registry of patients with chronic kidney disease (CKD) treated in a nephrology practice in order to provide clinically meaningful measurement and population management to improve rates of blood pressure (BP) control. METHODS: We combined data from multiple electronic sources: the billing system, structured fields in the electronic health record (EHR), and free text physician notes using natural language processing (NLP). We also used point-of-care worksheets to capture clinical rationale. RESULTS: Nephrologist billing accurately identified patients with CKD. Using an algorithm that incorporated multiple BP readings increased the measured rate of control (130/80 mm Hg) from 37.1% to 42.3%. With the addition of NLP to capture BP readings from free text notes, the rate was 52.6%. Data from point-of-care worksheets indicated that in 52% of visits in which patients were identified as not having controlled BP, patients were actually at goal based on BP readings taken at home or on that day in the office. CONCLUSIONS: Building a method for clinically meaningful continuous performance measurement of BP control is possible, but will require data from multiple sources. Electronic measurement systems need to grow to be able to capture and process performance data from patients as well as in real-time from physicians.",chronic disease management | chronic kidney disease | population management | quality improvement | registries
"The overall objective of the eu-ADR project is the design, development, and validation of a computerised system that exploits data from electronic health records and biomedical databases for the early detection of adverse drug reactions. Eight different databases, containing health records of more than 30 million European citizens, are involved in the project. Unique queries cannot be performed across different databases because of their heterogeneity: Medical record and Claims databases, four different terminologies for coding diagnoses, and two languages for the information described in free text. The aim of our study was to provide database owners with a common basis for the construction of their queries. Using the UMLS, we provided a list of medical concepts, with their corresponding terms and codes in the four terminologies, which should be considered to retrieve the relevant information for the events of interest from the databases.",
"BACKGROUND: The development of Electronic Health Records (EHRs) in hospitals offers the ability to reuse data from patient care activities for clinical research. EHR4CR is a European public-private partnership aiming to develop a computerized platform that enables the re-use of data collected from EHRs over its network. However, the reproducibility of queries may depend on attributes of the local data. Our objective was 1/ to describe the different steps that were achieved in order to use the EHR4CR platform and 2/ to identify the specific issues that could impact the final performance of the platform. METHODS: We selected three institutional studies covering various medical domains. The studies included a total of 67 inclusion and exclusion criteria and ran in two University Hospitals. We described the steps required to use the EHR4CR platform for a feasibility study. We also defined metrics to assess each of the steps (including criteria complexity, normalization quality, and data completeness of EHRs). RESULTS: We identified 114 distinct medical concepts from a total of 67 eligibility criteria Among the 114 concepts: 23 (20.2%) corresponded to non-structured data (i.e. for which transformation is needed before analysis), 92 (81%) could be mapped to terminologies used in EHR4CR, and 86 (75%) could be mapped to local terminologies. We identified 51 computable criteria following the normalization process. The normalization was considered by experts to be satisfactory or higher for 64.2% (43/67) of the computable criteria. All of the computable criteria could be expressed using the EHR4CR platform. CONCLUSIONS: We identified a set of issues that could affect the future results of the platform: (a) the normalization of free-text criteria, (b) the translation into computer-friendly criteria and (c) issues related to the execution of the query to clinical data warehouses. We developed and evaluated metrics to better describe the platforms and their result. These metrics could be used for future reports of Clinical Trial Recruitment Support Systems assessment studies, and provide experts and readers with tools to insure the quality of constructed dataset.",Clinical trial | Clinical trial recruitment system | Electronic health records | Patient recruitment
"Background: Computerized decision support systems (CDSS) provide new opportunities for automating antimicrobial stewardship (AMS) interventions and integrating them in routine healthcare. CDSS are recommended as part of AMS programs by international guidelines but few have been implemented so far. In the context of the publicly funded COMPuterized Antibiotic Stewardship Study (COMPASS), we developed and implemented two CDSSs for antimicrobial prescriptions integrated into the in-house electronic health records of two public hospitals in Switzerland. Developing and implementing such systems was a unique opportunity for learning during which we faced several challenges. In this narrative review we describe key lessons learned. Recommendations: (1) During the initial planning and development stage, start by drafting the CDSS as an algorithm and use a standardized format to communicate clearly the desired functionalities of the tool to all stakeholders. (2) Set up a multidisciplinary team bringing together Information Technologies (IT) specialists with development expertise, clinicians familiar with ""real-life"" processes in the wards and if possible, involve collaborators having knowledge in both areas. (3) When designing the CDSS, make the underlying decision-making process transparent for physicians and start simple and make sure to find the right balance between force and persuasion to ensure adoption by end-users. (4) Correctly assess the clinical and economic impact of your tool, therefore try to use standardized terminologies and limit the use of free text for analysis purpose. (5) At the implementation stage, plan usability testing early, develop an appropriate training plan suitable to end users' skills and time-constraints and think ahead of additional challenges related to the study design that may occur (such as a cluster randomized trial). Stay also tuned to react quickly during the intervention phase. (6) Finally, during the assessment stage plan ahead maintenance, adaptation and related financial challenges and stay connected with institutional partners to leverage potential synergies with other informatics projects.",antimicrobial stewardship | cluster randomized controlled trial | computerized decision support system | digital health | implementation | multidisciplinary | usability testing | user training
"The introduction of Computerized Provider Order Entry (CPOE) has been shown to reduce the incidence of medication-related errors in hospitals. Successful implementation of CPOE and electronic health records requires redesigning workflows and analysis of information collected and training of staff to use these new systems. However, well intentioned processes that seem to solve a unique problem can sometimes go in an unanticipated direction for several reasons. This can have unintentional consequences, especially when the built-in safeguards are not engaged. This poster describes one organization's efforts to identify the causes of one such breakdown, and how the obvious solutions were inappropriate.",
"Our work evaluates the contributions of a genetics clinic visit in assessing patients' risk of hereditary cancers and in meeting National Cancer Comprehensive Network (NCCN) criteria for genetic testing. We reviewed the electronic health records (EHR) of 56 women seen for medical care in our healthcare system who were subsequently seen in the Adult Genetics Clinic. We searched for all personal or family cancer history available in either free-text or structured form within the EHR prior to the genetics visit. For each patient, we then compared the aggregate data with the pedigree information obtained at the Genetics Clinic visit for first-, second-, and third-degree relatives. During the genetics clinic visit, the number of relatives with cancer diagnoses doubled from 121 to 235, and for 17 of 56 (30%) of patients, family histories changed one or more NCCN criteria. For 39/56 (70%) of patients, the family history in the EHR was not changed during the genetics clinic visit. Of 56 women referred to the genetics clinic, 45 (80%) met NCCN guidelines for testing, 40 women underwent genetic testing, and 9 of 40 (23%) tested were positive for a Likely Pathogenic or Pathogenic (LP/P) variant. This study of 56 women quantitatively demonstrates the value of a genetics clinic visit by improved identification of key family history components.",family history | genetic counseling | genetics services
"Narrative electronic prescribing instructions (NEPIs) are text that convey information on the administration or co-administration of a drug as directed by a prescriber. For researchers, NEPIs have the potential to advance our understanding of the risks and benefits of medications in populations; however, due to their unstructured nature, they are not often utilized. The goal of this scoping review was to evaluate how NEPIs are currently employed in research, identify opportunities and challenges for their broader application, and provide recommendations on their future use. The scoping review comprised a comprehensive literature review and a survey of key stakeholders. From the literature review, we identified 33 primary articles that described the use of NEPIs. The majority of articles (n = 19) identified issues with the quality of information in NEPIs compared with structured prescribing information; nine articles described the development of novel algorithms that performed well in extracting information from NEPIs, and five described the used of manual or simpler algorithms to extract prescribing information from NEPIs. A survey of 19 stakeholders indicated concerns for the quality of information in NEPIs and called for standardization of NEPIs to reduce data variability/errors. Nevertheless, stakeholders believed NEPIs present an opportunity to identify prescriber's intent for the prescription and to study temporal treatment patterns. In summary, NEPIs hold much promise for advancing the field of pharmacoepidemiology. Researchers should take advantage of addressing important questions that can be uniquely answered with NEPIs, but exercise caution when using this information and carefully consider the quality of the data.",drug prescribing | electronic health records | free text | narrative prescribing instructions | pharmacoepidemiology
"BACKGROUND: Many physicians enter data into the electronic medical record (EMR) as unstructured free text and not as discrete data, making it challenging to use for quality improvement or research initiatives. OBJECTIVES: The objective of this research paper was to develop and implement a structured clinical documentation support (SCDS) toolkit within the EMR to facilitate quality initiatives and practice-based research in a multiple sclerosis (MS) practice. METHODS: We built customized EMR toolkits to capture standardized data at office visits. Content was determined through physician consensus on necessary elements to support best practices in treating patients with demyelinating disorders. We also developed CDS tools and best practice advisories within the toolkits to alert physicians when a quality improvement opportunity exists, including enrollment into our DNA biobanking study at the point of care. RESULTS: We have used the toolkit to evaluate 541 MS patients in our clinic and begun collecting longitudinal data on patients who return for annual visits. We provide a description and example screenshots of our toolkits, and a brief description of our cohort to date. CONCLUSIONS: The EMR can be effectively structured to standardize MS clinic office visits, capture data, and support quality improvement and practice-based research initiatives at the point of care.",Best practices | clinical decision support | clinically isolated syndrome | cohort studies | electronic health records | multiple sclerosis | quality improvement | structured clinical documentation support
"An increase in antibiotic usage is considered to contribute to the emergence of antimicrobial resistance. Although experts are counting on the antimicrobial stewardship programs to reduce antibiotic usage, their effect remains uncertain. In this study, we aimed to evaluate the impact of antibiotic usage and forecast the prevalence of hospital-acquired extended spectrum β-lactamase (ESBL)-producing Escherichia coli (E. coli) using time-series analysis. Antimicrobial culture information of E. coli was obtained using a text processing technique that helped extract free-text electronic health records from standardized data. The antimicrobial use density (AUD) of antibiotics of interest was used to estimate the quarterly antibiotic usage. Transfer function model was applied to forecast relationship between antibiotic usage and ESBL-producing E. coli. Of the 1938 hospital-acquired isolates, 831 isolates (42.9%) were ESBL-producing E. coli. Both the proportion of ESBL-producing E. coli and AUD increased over time. The transfer model predicted that ciprofloxacin AUD is related to the proportion of ESBL-producing E. coli two quarters later. In conclusion, excessive use of antibiotics was shown to affect the prevalence of resistant organisms in the future. Therefore, the control of antibiotics with antimicrobial stewardship programs should be considered to restrict antimicrobial resistance.",
"Temporal information plays a critical role in the understanding of clinical narrative (i.e., free text). We developed a representation for marking up temporal information in a narrative, consisting of five elements: 1) reference point, 2) direction, 3) number, 4) time unit, and 5) pattern. We identified 254 temporal expressions from 50 discharge summaries and represented them using our scheme. The overall inter-rater reliability among raters applying the representation model was 75 percent agreement. The model can contribute to temporal reasoning in computer systems for decision support, data mining, and process and outcomes analyses by providing structured temporal information.",
"Capturing the nuances of clinical observations in an electronic format has been a major challenge in implementing electronic health records. In a formative evaluation study using three different methodologies, we identified that the greatest obstacle to point-of-care data entry for eye care was supporting free text annotation of clinical observations. To overcome this obstacle, we developed an approach that captures an image of a free text entry and associates this image with related data elements in an encounter note. Through simulated patient studies, we observed that this approach successfully supported complex documentation at the point of care by clinicians.",
"To practice medicine in the near future, health care providers in the USA need an information infrastructure they do not yet have. We offer a contribution from social science research to discussions of current medical records practices and how health care activity systems may be transformed by the advent of electronic health records. The goal of the paper is to set forth a framework that connects over-arching questions concerning medical informatics systems development with the practical, cultural and conceptual issues involved in transitions from handwritten and other free text documentation to structured entry of medical records to build patient profiles. The research is broadly framed by an interest in how reciprocal modifications of the design and use of an electronic health record are negotiated in an iterative prototyping project. It is conducted as part of a complex multi-disciplinary research and development effort to create an electronic health record prototype for use in the integrated health care delivery environment of the Southern California Kaiser Permanente Medical Care Program.",
"Towards the goal of automated eligibility determination for clinical trials from electronic health records, we propose a method to formulate Semantic Web based queries using the free-text eligibility criteria on clinicaltrials.gov.",
